[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation."
  },
  {
    "objectID": "bw/bw-uni.html",
    "href": "bw/bw-uni.html",
    "title": "\n1  Univariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking univariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz."
  },
  {
    "objectID": "bw/bw-bi.html",
    "href": "bw/bw-bi.html",
    "title": "\n2  Bivariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking bivariate visualization. The visualization will not perfect the first time but you are expected to improve it throughout the semester especially after covering advanced topics such as effective viz."
  },
  {
    "objectID": "bw/bw-tri.html",
    "href": "bw/bw-tri.html",
    "title": "\n3  Trivariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking trivariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz."
  },
  {
    "objectID": "bw/bw-quad.html",
    "href": "bw/bw-quad.html",
    "title": "\n4  Quadvariate Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking quadvariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz.\n\n\n\n\n\nUse this file to generate a professional looking trivariate visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz."
  },
  {
    "objectID": "bw/bw-spatial.html",
    "href": "bw/bw-spatial.html",
    "title": "\n5  Spatial Viz\n",
    "section": "",
    "text": "Use this file to generate a professional looking spatial visualization. The visualization will not perfect the first time but you are expected to improve on it throughout the semester especially after covering advanced topics such as effective viz."
  },
  {
    "objectID": "bw/bw-exam1.html",
    "href": "bw/bw-exam1.html",
    "title": "\n6  Exam 1\n",
    "section": "",
    "text": "#Load packages\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\n\n\n#Get data\ntuesdata &lt;- tt_load('2020-02-18')\nfc &lt;- tuesdata$food_consumption\n\n\n#Understanding data\nstr(tuesdata) #checks the structure of the data\n\nList of 1\n $ food_consumption: spc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ country      : chr [1:1430] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n  ..$ food_category: chr [1:1430] \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n  ..$ consumption  : num [1:1430] 10.51 38.66 55.48 1.56 4.36 ...\n  ..$ co2_emmission: num [1:1430] 37.2 41.53 1712 54.63 6.96 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   country = col_character(),\n  .. ..   food_category = col_character(),\n  .. ..   consumption = col_double(),\n  .. ..   co2_emmission = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=&lt;externalptr&gt; \n - attr(*, \".tt\")= 'tt' chr \"food_consumption.csv\"\n  ..- attr(*, \".files\")='data.frame':   1 obs. of  3 variables:\n  .. ..$ data_files: chr \"food_consumption.csv\"\n  .. ..$ data_type : chr \"csv\"\n  .. ..$ delim     : chr \",\"\n  ..- attr(*, \".readme\")=List of 2\n  .. ..$ node:&lt;externalptr&gt; \n  .. ..$ doc :&lt;externalptr&gt; \n  .. ..- attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n  ..- attr(*, \".date\")= Date[1:1], format: \"2020-02-18\"\n - attr(*, \"class\")= chr \"tt_data\"\n\nhead(tuesdata)\n\n$food_consumption\n# A tibble: 1,430 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 1,420 more rows\n\n\n\n#Answering the grand question: What does the consumptionof each food category in each country look like? While also taking into account\n\n#| fig-height: 100\n#| fig- width: 50\n\n#| fig-cap: A tiled visualization of the consumption of certain food groups by country. \n#| fig-alt: A tiled visualization of the consumption of certain food groups by country. \n\nggplot(fc, aes(x = food_category, y = country, fill = consumption)) +  \n  geom_tile() +\n  labs (x = \"Categories of Food\", y = \"Country\", fill = \"Rate of Consumption\")+\n  theme_minimal ()+\n  scale_fill_viridis_c()"
  },
  {
    "objectID": "bw/bw-exam2.html",
    "href": "bw/bw-exam2.html",
    "title": "\n7  Exam 2: Food Consumption2\n",
    "section": "",
    "text": "#Load packages\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n\n#Load data\ntuesdata &lt;- tt_load('2020-02-18')\nfc &lt;- tuesdata$food_consumption\n\n\n#Inspect data\nstr(fc)\n\nspc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country      : chr [1:1430] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ food_category: chr [1:1430] \"Pork\" \"Poultry\" \"Beef\" \"Lamb & Goat\" ...\n $ consumption  : num [1:1430] 10.51 38.66 55.48 1.56 4.36 ...\n $ co2_emmission: num [1:1430] 37.2 41.53 1712 54.63 6.96 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country = col_character(),\n  ..   food_category = col_character(),\n  ..   consumption = col_double(),\n  ..   co2_emmission = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nhead(fc, 22)\n\n# A tibble: 22 × 4\n   country   food_category            consumption co2_emmission\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;\n 1 Argentina Pork                           10.5          37.2 \n 2 Argentina Poultry                        38.7          41.5 \n 3 Argentina Beef                           55.5        1712   \n 4 Argentina Lamb & Goat                     1.56         54.6 \n 5 Argentina Fish                            4.36          6.96\n 6 Argentina Eggs                           11.4          10.5 \n 7 Argentina Milk - inc. cheese            195.          278.  \n 8 Argentina Wheat and Wheat Products      103.           19.7 \n 9 Argentina Rice                            8.77         11.2 \n10 Argentina Soybeans                        0             0   \n# ℹ 12 more rows\n\n\n\n#Fix food_category variable\nfcc &lt;- fc |&gt; \n  mutate (food_category = fct_recode (food_category, \n                                      \"Lamb\" = \"Lamb & Goat\",\n                                      \"Dairy\" = \"Milk - inc. cheese\",\n                                      \"Wheat\" = \"Wheat and Wheat Products\",\n                                      \"Nuts\" = \"Nuts inc. Peanut Butter\"))\n\n\n#Most Consuming Countries\nfcc |&gt; \n  group_by(country) |&gt;\n  summarize(total_consumption = sum(consumption)) |&gt;\n  arrange (desc(total_consumption)) |&gt;\n  select(country, total_consumption) |&gt;\n  head(5) \n\n# A tibble: 5 × 2\n  country     total_consumption\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 Finland                  640.\n2 Lithuania                555.\n3 Sweden                   550 \n4 Netherlands              534.\n5 Albania                  533.\n\n\n\n#Visualization\nfcc |&gt; \n  group_by(country) |&gt;\n  summarize(total_consumption = sum(consumption)) |&gt;\n  arrange (desc(total_consumption)) |&gt;\n  select(country, total_consumption) |&gt;\n  filter(country %in% c(\"Finland\", \"Lithuania\", \"Sweden\", \"Netherlands\", \"Albania\")) |&gt;\n  ggplot(aes(x = fct_reorder(country, total_consumption), y = total_consumption)) +  \n  geom_col(fill = \"darkblue\", color = \"pink\")+\n  labs(x = \"Country\", y = \"Total Consumption\")+\n  theme_minimal()"
  },
  {
    "objectID": "bw/bw-SoloProject.html",
    "href": "bw/bw-SoloProject.html",
    "title": "\n8  Solo Project\n",
    "section": "",
    "text": "# A tibble: 6 × 11\n  Incident_ID Date       Location      `Area_Burned (Acres)` Homes_Destroyed\n  &lt;chr&gt;       &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;           &lt;dbl&gt;\n1 INC1000     2020-11-22 Sonoma County                 14048             763\n2 INC1001     2021-09-23 Sonoma County                 33667            1633\n3 INC1002     2022-02-10 Shasta County                 26394             915\n4 INC1003     2021-05-17 Sonoma County                 20004            1220\n5 INC1004     2021-09-22 Sonoma County                 40320             794\n6 INC1005     2023-05-17 Butte County                  48348              60\n# ℹ 6 more variables: Businesses_Destroyed &lt;dbl&gt;, Vehicles_Damaged &lt;dbl&gt;,\n#   Injuries &lt;dbl&gt;, Fatalities &lt;dbl&gt;,\n#   `Estimated_Financial_Loss (Million $)` &lt;dbl&gt;, Cause &lt;chr&gt;"
  },
  {
    "objectID": "ica/quarto-demo.html#intro",
    "href": "ica/quarto-demo.html#intro",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.1 Intro",
    "text": "9.1 Intro\nMacalester College is in the Twin Cities. It has:\n\nfour seasons\nbagpipes\ndelightful students\n\nCheck it out for yourself:"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "href": "ica/quarto-demo.html#exercise-1-deduce-quarto-features",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.2 Exercise 1: Deduce Quarto features",
    "text": "9.2 Exercise 1: Deduce Quarto features\nCheck out the appearance and contents of this document. Thoughts?\nIn the toolbar at the top of this document, Render the .qmd file into a .html file. Where is this file stored? Thoughts about its appearance / contents? Can you edit it?\nToggling between the .qmd and .html files, explain the purpose of the following features in the .qmd file:\n*\n**\n#\n-\n\\\n![](url)"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-2-code",
    "href": "ica/quarto-demo.html#exercise-2-code",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.3 Exercise 2: Code",
    "text": "9.3 Exercise 2: Code\nHow does this appear in the .qmd? The .html? So…?!\nseq(from = 100, to = 1000, by = 50)"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-3-chunks",
    "href": "ica/quarto-demo.html#exercise-3-chunks",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.4 Exercise 3: Chunks",
    "text": "9.4 Exercise 3: Chunks\nQuarto isn’t a mind reader – we must distinguish R code from text. We do so by putting code inside an R chunk:\n\nseq()\n\n[1] 1\n\n\n\nPut the seq() code in the chunk.\nPress the green arrow in the top right of the chunk. What happens in the qmd?\nRender. What appears in the html: R code, output, or both?"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-4-practice",
    "href": "ica/quarto-demo.html#exercise-4-practice",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.5 Exercise 4: Practice",
    "text": "9.5 Exercise 4: Practice\n\nUse R code to create the following sequence: 10 10 10 10\nStore the sequence as four_tens.\nUse an R function (which we haven’t learned!) to add up the numbers in four_tens.\n\n\nrep(10,4)\n\n[1] 10 10 10 10\n\nfour_tens &lt;- rep(10,4)"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "href": "ica/quarto-demo.html#exercise-5-fix-this-code",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.6 Exercise 5: Fix this code",
    "text": "9.6 Exercise 5: Fix this code\nCode is a form of communication, and the code below doesn’t cut it.\nPut the code in a chunk and fix it.\nRep(x = 1, times = 10) seq(from=100,to=1000,length=20) theNumberofStudentsinthisclass&lt;-27\n\nrep(x = 1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\nseq(from = 100, to = 1000, length = 20)\n\n [1]  100.0000  147.3684  194.7368  242.1053  289.4737  336.8421  384.2105\n [8]  431.5789  478.9474  526.3158  573.6842  621.0526  668.4211  715.7895\n[15]  763.1579  810.5263  857.8947  905.2632  952.6316 1000.0000\n\ntotal_students_class&lt;-27"
  },
  {
    "objectID": "ica/quarto-demo.html#exercise-6-comments",
    "href": "ica/quarto-demo.html#exercise-6-comments",
    "title": "\n9  My first Quarto document\n",
    "section": "\n9.7 Exercise 6: Comments",
    "text": "9.7 Exercise 6: Comments\nRun the chunk below. Notice that R ignores anything in a line starting with a pound sign (#). If we took the # away we’d get an error!\n\n# This is a comment\n4 + 5\n\n[1] 9\n\n\nWe’ll utilize this feature to comment our code, i.e. leave short notes about what our code is doing. Below, replace the ??? with an appropriate comment.\n\n# Converting C to F\ntemperature_c &lt;- 10\ntemperature_f &lt;- temperature_c * 9/5 + 32\ntemperature_f\n\n[1] 50"
  },
  {
    "objectID": "ica/ica-uni.html#exercises",
    "href": "ica/ica-uni.html#exercises",
    "title": "\n10  Univariate Viz\n",
    "section": "\n10.1 Exercises",
    "text": "10.1 Exercises\n\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\n10.1.1 Exercise 1: Research Questions\nLet’s dig into the hikes data, starting with the elevation and difficulty ratings of the hikes:\n\nhead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture? We would like to create a layer that demonstrates the changes in scale between each peak.\nWhat about a visualization of the quantitative elevation variable? We would like to create a layer that shows how the elevation changes at each peak (relationship between data points). ### Exercise 2: Load tidyverse {.unnumbered}\n\nIn order to use ggplot tools, we have to first load the tidyverse package in which they live. We’ve installed the package but we need to tell R when we want to use it. Run the chunk below to load the library. You’ll need to do this within any .qmd file that uses ggplot().\n\n# Load the package\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nExercise 3: Bar Chart of Ratings - Part 1\nConsider some specific research questions about the difficulty rating of the hikes:\n\nHow many hikes fall into each category?\nAre the hikes evenly distributed among these categories, or are some more common than others?\n\nAll of these questions can be answered with: (1) a bar chart; of (2) the categorical data recorded in the rating column. First, set up the plotting frame:\n\nggplot(hikes, aes(x = rating))\n\n\n\n\nThink about:\n\nWhat did this do? What do you observe? Blank axis with the three different ratings.\nWhat, in general, is the first argument of the ggplot() function? To look in the data file titled hikes.\nWhat is the purpose of writing x = rating? Tells R to make the x-axis out of the different possible data points in rating.\nWhat do you think aes stands for?!? Aesthetics\n\n10.1.2 Exercise 4: Bar Chart of Ratings - Part 2\nNow let’s add a geometric layer to the frame / canvas, and start customizing the plot’s theme. To this end, try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\nNOTE:\n\nPay attention to the general code properties and structure, not memorization.\nNot all of these are “good” plots. We’re just exploring ggplot.\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\nExercise 5: Bar Chart Follow-up\nPart a\nReflect on the ggplot() code.\n\nWhat’s the purpose of the +? When do we use it? Adding a component to the plot.\nWe added the bars using geom_bar()? Why “geom”? Geometric layer to the frame/canvas\nWhat does labs() stand for? labels\nWhat’s the difference between color and fill? Color is border color and fill is the color within the individual bars.\nPart b\nIn general, bar charts allow us to examine the following properties of a categorical variable:\n\n\nobserved categories: What categories did we observe?\n\nRating (easy, moderate, difficult)\n\n\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others?\n\nSome categories are more common than others.\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\nThe bar chart shows us that moderate hikes were the most common type of hike, with difficult hikes being the least common. There are roughly 45 total hikes.\nPart c\nIs there anything you don’t like about this barplot? For example: check out the x-axis again.\nI wish that the x-axis was in the order of easy to moderate to difficult (I don’t need them to be in order of ascending frequency). I also would like a few more numbers on the y-axis.\nExercise 6: Sad Bar Chart\nLet’s now consider some research questions related to the quantitative elevation variable:\n\nAmong the hikes, what’s the range of elevation and how are the hikes distributed within this range (e.g. evenly, in clumps, “normally”)?\nWhat’s a typical elevation?\nAre there any outliers, i.e. hikes that have unusually high or low elevations?\n\nHere:\n\nConstruct a bar chart of the quantitative elevation variable.\nExplain why this might not be an effective visualization for this and other quantitative variables. (What questions does / doesn’t it help answer?)\n\nThis bar chart is an effective visualization to see whether there are peaks with the same elevation, but it does not show individual peaks, nor does it emphasize individual data points–the line is distracting… could have done better with a point. Hard to see where they clump. There is not an average line to compare the data with, which would demonstrate typical elevation.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_bar(fill = \"purple\")  +\n  labs(x = \"Elevation\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\nExercise 7: A Histogram of Elevation\nQuantitative variables require different viz than categorical variables. Especially when there are many possible outcomes of the quantitative variable. It’s typically insufficient to simply count up the number of times we’ve observed a particular outcome as the bar graph did above. It gives us a sense of ranges and typical outcomes, but not a good sense of how the observations are distributed across this range. We’ll explore two methods for graphing quantitative variables: histograms and density plots.\nHistograms are constructed by (1) dividing up the observed range of the variable into ‘bins’ of equal width; and (2) counting up the number of cases that fall into each bin. Check out the example below:\n\nPart a\nLet’s dig into some details.\n\nHow many hikes have an elevation between 4500 and 4700 feet? ~6 hikes\nHow many total hikes have an elevation of at least 5100 feet? ~2\nPart b\nNow the bigger picture. In general, histograms allow us to examine the following properties of a quantitative variable:\n\n\ntypical outcome: Where’s the center of the data points? What’s typical?\n\nvariability & range: How spread out are the outcomes? What are the max and min outcomes?\n\nshape: How are values distributed along the observed range? Is the distribution symmetric, right-skewed, left-skewed, bi-modal, or uniform (flat)?\n\noutliers: Are there any outliers, i.e. outcomes that are unusually large/small?\n\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Addressing each of the features in the above list, summarize below what you learned from the histogram, in context.\nThe typical elevation of the hikes in the Adirondacks is roughly 4300 but the most common elevation is 4000 ft. The minimum outcome is ~3750 ft and the maximum elevation is 5500 ft.The graph seems to be skewed right. Outliers may exist in hikes above 5100 ft.\nExercise 8: Building Histograms - Part 1\n2-MINUTE CHALLENGE: Thinking of the bar chart code, try to intuit what line you can tack on to the below frame of elevation to add a histogram layer. Don’t forget a +. If it doesn’t come to you within 2 minutes, no problem – all will be revealed in the next exercise.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"pink\", fill = \"purple\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nExercise 9: Building Histograms - Part 2\nLet’s build some histograms. Try each chunk below, one by one. In each chunk, make a comment about how both the code and the corresponding plot both changed.\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n# COMMENT on the change in the code and the corresponding change in the plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\nExercise 10: Histogram Follow-up\n\nWhat function added the histogram layer / geometry?\n\ngeom_histogram\n\nWhat’s the difference between color and fill?\n\ncolor is thr outline, and fill is the color within the bars\n\nWhy does adding color = \"white\" improve the visualization?\n\nBecause we are able to distinguish between the different bars (otherwise it becomes a blob)\n\nWhat did binwidth do?\n\nBinwidth changes the size of the bars (the range of elevation that we are counting the hikes in)\n\nWhy does the histogram become ineffective if the binwidth is too big (e.g. 1000 feet)?\n\nEncapsulates too much of the data set (no patterns can be seen)\n\nWhy does the histogram become ineffective if the binwidth is too small (e.g. 5 feet)?\n\nIt becomes a bar chart again! We cannot see any significant patterns because the bars are too skinny and the array is too limited.\nExercise 11: Density Plots\nDensity plots are essentially smooth versions of the histogram. Instead of sorting observations into discrete bins, the “density” of observations is calculated across the entire range of outcomes. The greater the number of observations, the greater the density! The density is then scaled so that the area under the density curve always equals 1 and the area under any fraction of the curve represents the fraction of cases that lie in that range.\nCheck out a density plot of elevation. Notice that the y-axis (density) has no contextual interpretation – it’s a relative measure. The higher the density, the more common are elevations in that range.\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density()\n\n\n\n\nQuestions\n\n\nINTUITION CHECK: Before tweaking the code and thinking back to geom_bar() and geom_histogram(), how do you anticipate the following code will change the plot?\n\ngeom_density(color = \"blue\")\ngeom_density(fill = \"orange\")\n\n\nTRY IT! Test out those lines in the chunk below. Was your intuition correct?\n\n\nggplot(hikes, aes(x = elevation)) +\ngeom_density(color = \"blue\", fill = \"orange\")\n\n\n\n\n\nExamine the density plot. How does it compare to the histogram? What does it tell you about the typical elevation, variability / range in elevations, and shape of the distribution of elevations within this range?\n\nWe get a more complete breakdown of the density of elevations. I.e. how many hikes at each elevation. More clearly shows the skew of the plot and potential outliers.\nExercise 12: Density Plots vs Histograms\nThe histogram and density plot both allow us to visualize the behavior of a quantitative variable: typical outcome, variability / range, shape, and outliers. What are the pros/cons of each? What do you like/not like about each?\nI like the smooth look of the density plot… it lets us see the skew more clearly, but I think that it could be more clear to talk about frequency of hikes.\nExercise 13: Code = communication\nWe obviously won’t be done until we talk about communication. All code above has a similar general structure (where the details can change):\n\nggplot(___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\nThough not necessary to the code working, it’s common, good practice to indent or tab the lines of code after the first line (counterexample below). Why?\n\n\n# YUCK\nggplot(hikes, aes(x = elevation)) +\ngeom_histogram(color = \"white\", binwidth = 200) +\nlabs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\nThough not necessary to the code working, it’s common, good practice to put a line break after each + (counterexample below). Why?\n\n\n# YUCK \nggplot(hikes, aes(x = elevation)) + geom_histogram(color = \"white\", binwidth = 200) + labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\nExercise 14: Practice\nPart a\nPractice your viz skills to learn about some of the variables in one of the following datasets from the previous class:\n\n# Data on students in this class\nsurvey &lt;- read.csv(\"https://hash-mac.github.io/stat112site-s25/data/survey.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\n\n# Load the package\nlibrary(tidyverse)\n\nggplot(survey, aes(x = hangout)) + \n  geom_bar(color = \"blue\", fill = \"pink\") + \n  labs(x = \"spot\", y = \"frequency\")\n\n\n\n\nPart b\nCheck out the RStudio Data Visualization cheat sheet to learn more features of ggplot.\nWhen done, don’t forgot to click Render Book and check the resulting HTML files. If happy, jump to GitHub Desktop and commit the changes with the message Finish activity 3 and push to GitHub. Wait few seconds, then visit your portfolio website and make sure the changes are there."
  },
  {
    "objectID": "ica/ica-bi.html",
    "href": "ica/ica-bi.html",
    "title": "\n11  Bivariate Viz\n",
    "section": "",
    "text": "Use this file for practice with the bivariate viz in-class activity. Refer to the class website for details.\n\n12 Import data\n\nsurvey &lt;- read.csv(\"https://ajohns24.github.io/data/112/about_us_2024.csv\") \n\n\n13 How many students have now filled out the survey?\n\n14 hmmm… nope!\nhead(survey) #did that work? I think so. nrow(survey)\n\n15 What type of variables do we have?\n\nclass (survey)\n\n[1] \"data.frame\"\n\nmode(survey) \n\n[1] \"list\"\n\ntypeof(survey)\n\n[1] \"list\"\n\nstr(survey)\n\n'data.frame':   28 obs. of  4 variables:\n $ cafe_mac         : chr  \"Cheesecake\" \"Cheese pizza\" \"udon noodles\" \"egg rolls\" ...\n $ minutes_to_campus: int  15 10 4 7 5 35 5 15 7 20 ...\n $ fave_temp        : num  18 24 18 10 18 7 75 24 13 16 ...\n $ hangout          : chr  \"the mountains\" \"a beach\" \"the mountains\" \"a beach\" ...\n\n\n\n16 Attach a package needed to use the ggplot function\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n17 Make a ggplot for hangouts\n\n#Making a ggplot for hangout\nggplot (survey, aes (x=hangout))+\ngeom_bar(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n\n\n\n\n18 Make a ggplot for temperatures\n\nggplot (survey, aes (x=fave_temp))+\ngeom_histogram(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nggplot (survey, aes (x=fave_temp))+\ngeom_density(color = \"blue\", fill = \"dark blue\")+\nlabs(x= \"Location\", y = \"# of Responses\")\n\n\n\n\n\ndata.frame(temp_3pm = c(24, 26, 20, 15, 15, 15), temp_9am = c(14, 18, 15, 13, 11, 11))\n\n  temp_3pm temp_9am\n1       24       14\n2       26       18\n3       20       15\n4       15       13\n5       15       11\n6       15       11\n\n\n\nweather &lt;- data.frame(temp_3pm = c(24, 26, 20, 15, 15, 0, 40, 60, 57, 44, 51, 75),\n                      location = rep(c(\"A\", \"B\"), each = 6))\nweather\n\n   temp_3pm location\n1        24        A\n2        26        A\n3        20        A\n4        15        A\n5        15        A\n6         0        A\n7        40        B\n8        60        B\n9        57        B\n10       44        B\n11       51        B\n12       75        B\n\n\n\nggplot(weather, aes(x = temp_3pm)) +\n      geom_density()\n\n\n\n\n\nweather &lt;- data.frame(rain_today = c(\"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\"),\n                        location = c(rep(\"A\", 7), rep(\"B\", 5)))\n    weather\n\n   rain_today location\n1          no        A\n2          no        A\n3          no        A\n4          no        A\n5         yes        A\n6          no        A\n7         yes        A\n8          no        B\n9         yes        B\n10        yes        B\n11         no        B\n12        yes        B\n\n\n\nggplot(weather, aes(x = location)) +\n      geom_bar()\n\n\n\n\n\n# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\n\n19 Exercise 0\n\nggplot(elections, aes(x = winner_20)) + \n  geom_bar(color = \"blue\", fill = \"pink\") + \n  labs(x = \"Party\", y = \"Vote Count\")\n\n\n\n\n\nggplot(elections, aes(x = repub_pct_20)) + \n  geom_histogram(color = \"blue\", fill = \"pink\") + \n  labs(x = \"Party\", y = \"Vote Count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n#Exercise 1\n\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n\n#For scatter plots, there is a y and x term defined.\n\n\n# Add a layer of points for each county\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n\n\n\n# Change the shape of the points\n# What happens if you change the shape to another number?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 2)\n\n\n\n# Changing the number of the shape term will change the shape of the points (two is triangle, ten is some weird bulls eye thing)\n\n\n# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")\n\n\n\n#color is helpful, fill is not.\n\n\n# Add a layer that represents each county by the state it's in\n# Take note of the geom and the info it needs to run!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")+\n  geom_text(aes(label = state_abbr))\n\n\n\n\n#Exercise 3: Reflect\nStrong positive linear correlation indicating that there is a strong relationship between the way a county votes from one election year to the next.\nThere seems to be a few outliers in Texas… although it is hard to see on this very crowded graph.\n#Exercise 4: Visualizing trend\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n#Exercise 5: Your Turn\nTo examine how the 2020 results are related to some county demographics, construct scatterplots of repub_pct_20 vs median_rent, and repub_pct_20 vs median_age. Summarize the relationship between these two variables and comment on which is the better predictor of repub_pct_20, median_rent or median_age.\n\n# Scatterplot of repub_pct_20 vs median_rent\nggplot(elections, aes(y = repub_pct_20, x = median_rent)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThere is a fair negative linear relationship between median rent and percent of republican vote. This could indicate that with higher rents, there is less of a tendency to vote Republican.\n\n# Scatterplot of repub_pct_20 vs median_age\nggplot(elections, aes(y = repub_pct_20, x = median_age)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis visualization shows a poor positive linear relationship between age and Republican vote, indicating that age may not be a good predictor for political voting.\n#Exercise 6: A Sad Scatterplot\nNext, let’s explore the relationship between a county’s 2020 Republican support repub_pct_20 and the historical political trends in its state. In this case repub_pct_20 is quantitative, but historical is categorical. Explain why a scatterplot might not be an effective visualization for exploring this relationship. (What questions does / doesn’t it help answer?)\n\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\nThis graph shows that there is a wide spread of percentage of republican voting in counties that are historically blue, purple, and red, with a very slight dominance in red counties.\n#Exercise 7: Quantitative vs Categorical – Violins & Boxes\nThough the above scatterplot did group the counties by historical category, it’s nearly impossible to pick out meaningful patterns in 2020 Republican support in each category. Let’s try adding 2 different geom layers to the frame:\n\n# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n\n\n\n# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\n\n\n\nFrom the above plots, I can surmise that there is a higher median of percent Republican votes in red counties, and that that pattern steadily decreases from red to purple to blue. There is a sharp buldge in red counties for ~75% of votes to be Republican.\n#Exercise 8: Quantitative vs Categorical – Intuition Check\nWe can also visualize the relationship between repub_pct_20 and historical using our familiar density plots. In the plot below, notice that we simply created a separate density plot for each historical category. (The plot itself is “bad” but we’ll fix it below.) Try to adjust the code chunk below, which starts with a density plot of repub_pct_20 alone, to re-create this image.\n\nggplot(elections, aes(x = repub_pct_20)) +\n  geom_density()\n\n\n\n\nHm… I have no idea how to adjust the code. Three minutes are up! HMMM IT WAS JUST ADDING FILL? WUT.\n#Exercise 9: Quantitative vs Categorical – Density Plots\n\n# Name two \"bad\" things about this plot\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\nThe overlap is very confusing and I am not sure where the counties come into play…\n\n# What does scale_fill_manual do?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n**scale_fill_manual seems to change the colors of the scale based on categorical values.\n\n# What does alpha = 0.5 do?\n# Play around with different values of alpha, between 0 and 1\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.2) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n“Alpha = some number” seems to adjust transparency. With 0 being translucent and 1 being opaque.\n\n# What does facet_wrap do?!\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ historical)\n\n\n\n\n“facet_wrap” separates each of the categorical values and places them side by side.\n\n# Let's try a similar grouping strategy with a histogram instead of density plot.\n# Why is this terrible?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_histogram(color = \"white\") +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThis is terrible because the colors suuuuck but also because I still don’t understand the county breakdown and it’s hard to see how blue compares with purple, etc except to say that red is the predominant predictor (sometimes… it’s confusing).\n#Exercise 10\nWe’ve now learned 3 (of many) ways to visualize the relationship between a quantitative and categorical variable: side-by-side violins, boxplots, and density plots.\nWhich do you like best? I like boxplots, but maybe that’s just because I have had the most exposure to them.\nWhat is one pro of density plots relative to boxplots? Pros include better side by side comparison, clearer outline of where density is at it’s max.\nWhat is one con of density plots relative to boxplots? Hard to see the spread, ie maximums and minimums are a bit convoluted in the violin plots\n#Exercise 11: Categorical vs Categorical – Intuition Check\nFinally, let’s simply explore who won each county in 2020 (winner_20) and how this breaks down by historical voting trends in the state. That is, let’s explore the relationship between 2 categorical variables! Following the same themes as above, we can utilize grouping features such as fill/color or facets to distinguish between different categories of winner_20 and historical.\n\n# Plot 1: adjust this to recreate the top plot\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()+\n  scale_fill_manual(values = c(\"blue\", \"red\"))\n\n\n\n\n\n# Plot 2: adjust this to recreate the bottom plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar()+\n  facet_wrap(~historical)\n\n\n\n\n#Exercise 12: Categorical vs Categorical Construct the following 4 bar plot visualizations.\n\n# A stacked bar plot\n# How are the \"historical\" and \"winner_20\" variables mapped to the plot, i.e. what roles do they play?\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n# A faceted bar plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical)\n\n\n\n\n\n# A side-by-side bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n# A proportional bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n#Part a Name one pro and one con of using the “proportional bar plot” instead of one of the other three options.\n#Part b What’s your favorite bar plot from part and why?\n#Exercise 13: Practice (now or later) Import some daily weather data from a few locations in Australia:\n\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n\nConstruct plots that address the research questions in each chunk. You might make multiple plots–there are many ways to do things!. However, don’t just throw spaghetti at the wall.\nReflect before doing anything. What types of variables are these? How might you plot just 1 of the variables, and then tweak the plot to incorporate the other?\n\n# How do 3pm temperatures (temp3pm) differ by location?\n# In answering this question we have two variables, one numeric and one categorical. Therefore it is probably best to have either boxplots, violin plots, or a stacked density plot. \n\n#density plot\nggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"pink\", \"purple\",\"green\"))+\n    facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin(fill = \"pink\")\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n# How might we predict the 3pm temperature (temp3pm) by the 9am temperature (temp9am)?\n#Similar to the early exercises, we could make a scatter plot and fit it with a line of best fit. \nggplot(weather, aes(y = temp3pm, x = temp9am)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 27 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# How do the number of rainy days (raintoday) differ by location?\nggplot(weather, aes(x = location, fill = raintoday)) + \n  geom_bar()"
  },
  {
    "objectID": "ica/ica-multi.html#review",
    "href": "ica/ica-multi.html#review",
    "title": "\n12  Mulivariate Viz\n",
    "section": "\n12.1 Review",
    "text": "12.1 Review\nLet’s review some univariate and bivariate plotting concepts using some daily weather data from Australia. This is a subset of the data from the weatherAUS data in the rattle package.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...\n\n\nExample 1\nConstruct a plot that allows us to examine how temp3pm varies.\n\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\nggplot(weather, aes(x = temp3pm)) +\n  geom_density() \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 2\nConstruct 3 plots that address the following research question:\nHow do afternoon temperatures (temp3pm) differ by location?\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) +\n  geom_violin()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nReflection\n\nTemperatures tend to be highest, and most variable, in Uluru. There, they range from ~10 to ~45 with a typical temp around ~30 degrees.\nTemperatures tend to be lowest in Hobart. There, they range from ~5 to ~45 with a typical temp around ~15 degrees.\nWollongong temps are in between and are the least variable from day to day.\n\nSUBTLETIES: Defining fill or color by a variable\nHow we define the fill or color depends upon whether we’re defining it by a named color or by some variable in our dataset. For example:\n\ngeom___(fill = \"blue\")named colors are defined outside the aesthetics and put in quotes\ngeom___(aes(fill = variable)) or ggplot(___, aes(fill = variable))\ncolors/fills defined by a variable are defined inside the aesthetics\nExample 3\nLet’s consider Wollongong alone:\n\n# Don't worry about the syntax (we'll learn it soon)\nwoll &lt;- weather |&gt;\n  filter(location == \"Wollongong\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday))+\n  geom_bar(fill = \"blue\")\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday))+\ngeom_bar(aes(fill = raintomorrow))\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nReflection\nThere’s often not one “best plot”, but a combination of plots that provide a complete picture:\n\nThe stacked and side-by-side bars reflect that on most days, it does not rain.\nThe proportional / filled bars lose that information, but make it easier to compare proportions: it’s more likely to rain tomorrow if it also rains today.\nExample 4\nConstruct a plot that illustrates how 3pm temperatures (temp3pm) vary by date in Wollongong. Represent each day on the plot and use a curve/line to help highlight the trends.\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\n\nggplot(woll, aes(y = temp3pm, x = date)) +\n  geom_point() +\n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\n\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\nNOTE: A line plot isn’t always appropriate! It can be useful in situations like this, when our data are chronological.\nReflection\nThere’s a seasonal / cyclic behavior in temperatures – they’re highest in January (around 23 degrees) and lowest in July (around 16 degrees). There are also some outliers – some abnormally hot and cold days."
  },
  {
    "objectID": "ica/ica-multi.html#new-stuff",
    "href": "ica/ica-multi.html#new-stuff",
    "title": "\n12  Mulivariate Viz\n",
    "section": "\n12.2 New Stuff",
    "text": "12.2 New Stuff\nNext, let’s consider the entire weather data for all 3 locations. The addition of location adds a 3rd variable into our research questions:\n\nHow does the relationship between raintoday and raintomorrow vary by location?\nHow does the behavior of temp3pm over date vary by location?\nAnd so on.\n\nThus far, we’ve focused on the following components of a plot:\n\nsetting up a frame\n\nadding layers / geometric elements\nsplitting the plot into facets for different groups / categories\nchange the theme, e.g. axis labels, color, fill\n\nWe’ll have to think about all of this, along with scales. Scales change the color, fill, size, shape, or other properties according to the levels of a new variable. This is different than just assigning scale by, for example, color = \"blue\".\nWork on the examples below in your groups. Check in with your intuition! We’ll then discuss as a group as relevant.\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()+\n  geom_text(aes(label = location))\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") +\n  facet_wrap(~ location)\n\n\n\n\nThere’s no end to the number and type of visualizations you could make. And it’s important to not just throw spaghetti at the wall until something sticks. FlowingData shows that one dataset can be visualized many ways, and makes good recommendations for data viz workflow, which we modify and build upon here:\n\nIdentify simple research questions.\nWhat do you want to understand about the variables or the relationships among them?\n\nStart with the basics and work incrementally.\n\nIdentify what variables you want to include in your plot and what structure these have (eg: categorical, quantitative, dates)\nStart simply. Build a plot of just 1 of these variables, or the relationship between 2 of these variables.\nSet up a plotting frame and add just one geometric layer at a time.\nStart tweaking: add whatever new variables you want to examine,\n\n\n\nAsk your plot questions.\n\nWhat questions does your plot answer? What questions are left unanswered by your plot?\nWhat new questions does your plot spark / inspire?\nDo you have the viz tools to answer these questions, or might you learn more?\n\n\nFocus.\nReporting a large number of visualizations can overwhelm the audience and obscure your conclusions. Instead, pick out a focused yet comprehensive set of visualizations.\n\n:::"
  },
  {
    "objectID": "ica/ica-multi.html#exercises-required",
    "href": "ica/ica-multi.html#exercises-required",
    "title": "\n12  Mulivariate Viz\n",
    "section": "\n12.3 Exercises (required)",
    "text": "12.3 Exercises (required)\nThe story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\n# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\nExercise 1: SAT scores\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?)\nThere is a broad range of scores from 850 to a little past 1100. Most scores are around 900, with another lesser maximum density at 1050. And the shape is non-normal. \nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\nggplot(education, aes(y = sat, x = expend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\n\nggplot(education, aes(y = sat, x = salary)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you?\nIn both of the above plots we see a very loose decrease SAT scores in spending and salary. While this was initially a surprising result, I realized that the relationship is not strong, making me question whether this is a significant relationship.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nggplot(education, aes(y = sat, x = salary, color = expend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nDescribe the trivariate relationship between sat, salary, and expend.\nIt seems like there is limited relationship between a higher salary and sat scores. However, in schools that have low salaries, we see a sharp decrease in sat scores as expenditures increase. In schools with high salaries, there is a slight upward trend with increasing expenditures leading to increased sat scores.\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\nggplot(education, aes(x = fracCat)) +\n  geom_bar() \n\n\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\n\nggplot(education, aes(y = sat)) +\n  geom_boxplot() +\n  facet_wrap (~fracCat)\n\n\n\n\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?"
  },
  {
    "objectID": "ica/ica-multi.html#exercises-optional",
    "href": "ica/ica-multi.html#exercises-optional",
    "title": "\n12  Mulivariate Viz\n",
    "section": "\n12.4 Exercises (optional)",
    "text": "12.4 Exercises (optional)\nExercise 6: Heat Maps\nAs usual, we’ve only just scratched the surface! There are lots of other data viz techniques for exploring multivariate relationships. Let’s start with a heat map.\nPart a\nRun the chunks below. Check out the code, but don’t worry about every little detail! NOTES:\n\nThis is not part of the ggplot() grammar, making it a bit complicated.\nIf you’re curious about what a line in the plot does, comment it out (#) and check out what happens!\nIn the plot, for each state (row), each variable (column) is scaled to indicate whether the state has a relative high value (yellow), a relatively low value (purple), or something in between (blues/greens).\nYou can also play with the color scheme. Type ?cm.colors in the console to learn about various options.\nWe’ll improve the plot later, so don’t spend too much time trying to learn something from this plot.\n\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\nPart b\nIn the final two plots, the states (rows) are rearranged by similarity with respect to these education metrics. The final plot includes a dendrogram which further indicates clusters of similar states. In short, states that have a shorter path to connection are more similar than others.\nPutting this all together, what insight do you gain about the education trends across U.S. states? Which states are similar? In what ways are they similar? Are there any outliers with respect to 1 or more of the education metrics?\nExercise 7: Star plots\nLike heat maps, star plots indicate the relative scale of each variable for each state. Thus, we can use star maps to identify similar groups of states, and unusual states!\nPart a\nConstruct and check out the star plot below. Note that each state has a “pie”, with each segment corresponding to a different variable. The larger a segment, the larger that variable’s value is in that state. For example:\n\nCheck out Minnesota. How does Minnesota’s education metrics compare to those in other states? What metrics are relatively high? Relatively low?\nWhat states appear to be similar? Do these observations agree with those that you gained from the heat map?\n\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\nPart b\nFinally, let’s plot the state stars by geographic location! What new insight do you gain here?!\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)"
  },
  {
    "objectID": "ica/ica-multi.html#solutions",
    "href": "ica/ica-multi.html#solutions",
    "title": "\n12  Mulivariate Viz\n",
    "section": "\n12.5 Solutions",
    "text": "12.5 Solutions\n\nClick for Solutions\n\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...\n\n\nExample 1\n\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 2\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) + \n  geom_boxplot()\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ location)\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 3\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(fill = \"blue\")\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(aes(fill = raintomorrow))\n\n\n\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\nExample 4\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_point() + \n  geom_smooth(span = 0.5)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 18 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\nWarning: Removed 69 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") + \n  facet_wrap(~ location)\n\n\n\n\nExercise 1: SAT scores\nPart a\n\n# A histogram would work too!\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\nPart b\naverage SAT scores range from roughly 800 to 1100. They appear bi-modal.\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPart b\nThe higher the student expenditures and teacher salaries, the worse the SAT performance.\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\n\nggplot(education, aes(y = sat, x = salary, color = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\nExercise 4: Another Way to Incorporate Scale\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nStates with lower salaries and expenditures tend to have higher SAT scores.\nExercise 5: Finally an Explanation\nPart a\n\nggplot(education, aes(x = fracCat)) + \n  geom_bar()\n\n\n\n\nPart b\nThe more students in a state that take the SAT, the lower the average scores tend to be. This is probably related to self-selection.\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\nPart c\nWhen we control for the fraction of students that take the SAT, SAT scores increase with expenditure.\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPart d\nStudent participation tends to be lower among states with lower expenditures (which are likely also the states with higher ed institutions that haven’t historically required the SAT). Those same states tend to have higher SAT scores because of the self-selection of who participates.\nExercise 6: Heat Maps\nPart a\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\nWarning: package 'gplots' was built under R version 4.3.3\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\nPart b\n\nSimilar values in verbal, math, and sat.\nHigh contrast (an inverse relationship) verbal/math/sat scores and the fraction of students that take the SAT.\nOutliers of Utah and California in ratio (more students per teacher).\nWhile grouped, fraction and salary are not as similar to each other as the sat scores; it is also interesting to notice states that have high ratios have generally low expenditures per student.\nExercise 7: Star Plots\nPart a\nMN is high on the SAT performance related metrics and low on everything else. MN is similar to Iowa, Kansas, Mississippi, Missouri, the Dakotas…\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\n\nPart b\nWhen the states are in geographical ordering, we’d notice more easily that states in similar regions of the U.S. have similar patterns of these variables.\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)"
  },
  {
    "objectID": "ica/ica-spatial.html#review",
    "href": "ica/ica-spatial.html#review",
    "title": "\n13  Spatial Viz\n",
    "section": "\n13.1 Review",
    "text": "13.1 Review\nIn the previous activity, we explored a Simpson’s Paradox–it seemed that - states with higher spending… - tend to have lower average SAT scores.\nBUT this was explained by a confounding (aka omitted and lurking) variable which is the % of students in a state that take the SAT. Hence,\n\nStates with higher spending…\ntend to have a higher % of students of students that take the SAT…\nwhich then “leads to” lower average SAT scores.\n\nThus, when controlling for the % of students that take the SAT, more spending is correlated with higher scores.\nLet’s explore a Simpson’s paradox related to Mac!\nBack in the 2000s, Macalester invested in insulating a few campus-owned houses, with the hopes of leading to energy savings.  Former Mac Prof Danny Kaplan accessed monthly data on energy use and other info for these addresses, before and after renovations:\n\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ purrr::map()    masks maps::map()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import the data and only keep 2 addresses\nenergy &lt;- read.csv(\"https://mac-stat.github.io/data/MacNaturalGas.csv\") |&gt; \n  mutate(date = as.Date(paste0(month, \"/1/\", year), \"%m/%d/%Y\")) |&gt; \n  filter(address != \"c\")\n\n# Check it out\nhead(energy)\n\n  month year  price therms hdd address renovated       date\n1     6 2005  35.21     21   0       a        no 2005-06-01\n2     7 2005  37.37     21   0       a        no 2005-07-01\n3     8 2005  36.93     21   3       a        no 2005-08-01\n4     9 2005  62.36     39  61       a        no 2005-09-01\n5    10 2005 184.15    120 416       a        no 2005-10-01\n6    11 2005 433.35    286 845       a        no 2005-11-01\n\n\nThe part of dataset codebook is below:\n\n\n\n\n\n\nvariable\nmeaning\n\n\n\ntherms\na measure of energy use–the more energy used, the larger the therms\n\n\naddress\na or b\n\n\nrenovated\nwhether the location had been renovated, yes or no\n\n\nmonth\nfrom 1 (January) to 12 (December)\n\n\nhdd\nmonthly heating degree days. A proxy measure of outside temperatures–the higher the hdd, the COLDER it was outside"
  },
  {
    "objectID": "ica/ica-spatial.html#examples",
    "href": "ica/ica-spatial.html#examples",
    "title": "\n13  Spatial Viz\n",
    "section": "\n13.2 Examples",
    "text": "13.2 Examples\n\nConstruct a plot that addresses each research question\nInclude a 1-sentence summary of the plot.\n\nExample 1\nWhat was range in, and typical, energy used each month, as measured by therms? How does this differ by address?\nThere is a wide variety in each plot. In month 3 (March), for example, there is a range from about 200 to 400. However, in month 7 (July) there is a significantly smaller range from 0-50. Typical energy use varies greatly by month. \n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 2\nHow did energy use (therms) change over time (date) at the two addresses?\nThere is a slight increase in energy usage at both addresses over time.\n\nggplot(energy, aes(y = therms, x = date, color = address)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExample 3\nHow did the typical energy use (therms) at the two addresses change before and after they were renovated?\n\nggplot(energy, aes(y = therms, x = renovated, color = renovated))+\n  geom_boxplot()+\n  facet_wrap(~address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nExample 4\nThat seems unfortunate that energy usage went up after renovations. But also fishy.\nTake 5 minutes in your groups to try and explain what’s going on here. Think: What confounding, lurking, or omitted variable related to energy usage are we ignoring here? Try to make some plots to prove your point.\n\nggplot(energy, aes(y = therms, x = hdd, color = renovated))+\n  geom_boxplot()\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nExample 5\nLet’s summarize the punchlines by filling in the ???. It seemed that:\n\nAfter renovation…\nenergy use increased.\n\nBUT this was explained by a confounding or omitted or lurking variable: ???\n\nAfter renovation…\ntherms increased at both addresses\nwhich then leads to higher energy use.\n\nThus, when controlling for hdd, renovations led to decreased energy use.\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "ica/ica-spatial.html#new-stuff",
    "href": "ica/ica-spatial.html#new-stuff",
    "title": "\n13  Spatial Viz\n",
    "section": "\n13.3 New stuff",
    "text": "13.3 New stuff\nTypes of spatial viz:\n\nPoint Maps: plotting locations of individual observations\nexample: bigfoot sightings\nContour Maps: plotting the density or distribution of observations (not the individual observations themselves)\n\nChoropleth Maps: plotting outcomes in different regions\n\nNYT article on effects of redlining\nMinnesota Reformer article on how Mpls / St Paul voted on 2021 ballot measures related to mayoral, policing, and rent policies\n\n\n\nThese spatial maps can be static or dynamic/interactive."
  },
  {
    "objectID": "ica/ica-spatial.html#exercises",
    "href": "ica/ica-spatial.html#exercises",
    "title": "\n13  Spatial Viz\n",
    "section": "\n13.4 Exercises",
    "text": "13.4 Exercises\n\n13.4.1 Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\nPart 1: Interactive points on a map with leaflet\n\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\nfave_places &lt;- read.csv(\"https://ajohns24.github.io/data/112/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\n  latitude longitude\n1       46      -123\n2       33        52\n3       48       -90\n4       36      -112\n5       59        25\n6       39      -106\n\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\n# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\nleaflet(data = fave_places)\n\n\n\n\n\n\n# Now what do we have?\nleaflet(data = fave_places) |&gt; \n  addTiles()\n\n\n\n\n\n\n# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\n# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into? How does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\n# Load package needed to change color\nlibrary(gplots)\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\n# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) |&gt;\n  addProviderTiles(\"USGS\") |&gt;\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) |&gt;\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\n\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\n\nweight = how thick to make the lines, points, pixels\n\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nLet’s focus on only those in Minnesota for now:\n\n# Don't worry about the syntax\nstarbucks_mn &lt;- starbucks |&gt;   \n  filter(Country == \"US\", State.Province == \"MN\")\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addCircles(color = col2hex(\"darkgreen\"))\n\nAssuming \"Longitude\" and \"Latitude\" are longitude and latitude, respectively\n\n\n\n\n\n\nPart 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can not print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\nThe plot doesn’t give me relative context, and the dots are too small for me to really see any overlap.\n\nggplot(starbucks, aes(x = Longitude, y = Latitude)) +\n  geom_point(alpha = 0.2, size = 0.2) \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\n# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\nPart b\nRun the chunks below to build up a new map.\n\n# What does this code produce?\n# What geom are we using for the point map?\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n# Load package needed to change map theme\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.2\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following object is masked from 'package:ggthemes':\n\n    theme_map\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?!\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map.\nThere is a Starbucks in every continent except for Antarctica, with an obvious concentration in the United States and Western Europe.\nExercise 5: Zooming in on some countries\nInstead of world_boundaries &lt;- ne_countries(returnclass = 'sf') we could zoom in on…\n\nthe continent of Africa: ne_countries(continent = 'Africa', returnclass = 'sf')\n\na set of countries: ne_countries(country = c('france', 'united kingdom', 'germany'), returnclass = 'sf')\n\nboundaries within a country: ne_states(country = 'united states of america', returnclass = 'sf')\n\n\nOur goal here will be to map the Starbucks locations in Canada, Mexico, and the US.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only Canada, Mexico, and the US, labeled as “CA”, “MX”, “US” in the starbucks data.\n\n\n# We'll learn this syntax soon! Don't worry about it now.\nstarbucks_cma &lt;- starbucks |&gt; \n  filter(Country %in% c('CA', 'MX', 'US'))\n\n\nA background map of state- and national-level boundaries in Canada, Mexico, and the US. This requires ne_states() in the rnaturalearth package where the countries are labeled ‘canada’, ‘mexico’, ‘united states of america’.\n\n\ncma_boundaries &lt;- ne_states(\n  country = c(\"canada\", \"mexico\", \"united states of america\"),\n  returnclass = \"sf\")\n\nPart b\nMake the map!\n\n# Just the boundaries\nggplot(cma_boundaries) + \n  geom_sf()\n\n\n\n\n\n# Add the points\n# And zoom in\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50)) +\n  theme_map()\n\n\n\n\nExercise 6: A state and county-level map\nLet’s get an even higher resolution map of Starbucks locations within the states of Minnesota, Wisconsin, North Dakota, and South Dakota, with a background map at the county-level.\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only the states of interest.\n\n\nstarbucks_midwest &lt;- starbucks |&gt; \n  filter(State.Province %in% c(\"MN\", \"ND\", \"SD\", \"WI\"))\n\n\nA background map of state- and county-level boundaries in these states. This requires st_as_sf() in the sf package, and map() in the maps package, where the countries are labeled ‘minnesota’, ‘north dakota’, etc.\n\n\n# Load packages\nlibrary(sf)\nlibrary(maps)\n\n# Get the boundaries\nmidwest_boundaries &lt;- st_as_sf(\n  maps::map(\"county\",\n            region = c(\"minnesota\", \"wisconsin\", \"north dakota\", \"south dakota\"), \n            fill = TRUE, plot = FALSE))\n\n# Check it out\nhead(midwest_boundaries)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.81268 ymin: 45.05167 xmax: -93.01397 ymax: 48.53526\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                                     ID                           geom\nminnesota,aitkin       minnesota,aitkin MULTIPOLYGON (((-93.03689 4...\nminnesota,anoka         minnesota,anoka MULTIPOLYGON (((-93.51817 4...\nminnesota,becker       minnesota,becker MULTIPOLYGON (((-95.14537 4...\nminnesota,beltrami   minnesota,beltrami MULTIPOLYGON (((-95.58655 4...\nminnesota,benton       minnesota,benton MULTIPOLYGON (((-93.77027 4...\nminnesota,big stone minnesota,big stone MULTIPOLYGON (((-96.10794 4...\n\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) + \n   geom_sf() + \n   geom_point(\n     data = starbucks_midwest,\n     aes(x = Longitude, y = Latitude),\n     alpha = 0.7,\n     size = 0.2, \n     color = 'darkgreen'\n ) + \n   theme_map()\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot? Instead of points, there are now radiating rings. \n# What changed in our code?! Instead of geom_point we are using geom_density_2d\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nPart 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first. We’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties_Republican &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nExercise 8: State-level choropleth maps\nLet’s map the 2020 Republican support in each state, repub_pct_20.\nPart a\nWe again need two pieces of information.\n\nData on elections in each state, which we already have: elections_by_state.\nA background map of state boundaries in the US. The boundaries we used for point maps don’t work here. (Optional detail: they’re sf objects and we now need a data.frame object.) Instead, we can use the map_data() function from the ggplot2 package:\n\n\n# Get the latitude and longitude coordinates of state boundaries\nstates_map &lt;- map_data(\"state\")\n\n# Check it out\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\n13.4.1.1 Pause\nImportant detail: Note that the region variable in states_map, and the state_name variable in elections_by_state both label states by the full name in lower case letters. This is critical to the background map and our data being able to communicate.\n\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nhead(elections_by_state) \n\n   state_name state_abbr repub_pct_20 repub_20_categories\n1     alabama         AL        62.03               60-64\n2    arkansas         AR        62.40               60-64\n3     arizona         AZ        49.06               45-49\n4  california         CA        34.33               30-34\n5    colorado         CO        41.90               40-44\n6 connecticut         CT        39.21               35-39\n\n\n\n13.4.1.2 Part b\nNow map repub_pct_20 by state.\n\n# Note where the dataset, elections_by_state, is used. It is being used in the first unit of the ggplot().\n# Note where the background map, states_map, is used. Background map is being used in geom_map.\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() \n\n\n\n\n\n# Make it nicer! We are adding a gradient here. \nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradientn(name = \"% Republican\", colors = c(\"blue\", \"purple\", \"red\"), values = scales::rescale(seq(0, 100, by = 5)))\n\n\n\n\nIt’s not easy to get fine control over the color scale for the quantitative repub_pct_20 variable. Instead, let’s plot the discretized version, repub_20_categories:\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map()\n\n\n\n\n\n# Load package needed for refining color palette\nlibrary(RColorBrewer)\n\n# Now fix the colors\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n13.4.1.3 Part c\nWe can add other layers, like points, on top of a choropleth map. Add a Starbucks layer! Do you notice any relationship between Starbucks and elections? Or are we just doing things at this point? ;)\n\n# Get only the starbucks data from the US\nstarbucks_us &lt;- starbucks |&gt; \n  filter(Country == \"US\")\n\n# Map it\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  geom_point(\n    data = starbucks_us,\n    aes(x = Longitude, y = Latitude),\n    size = 0.05,\n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\nDetails (if you’re curious)\n\n\nmap_id is a required aesthetic for geom_map().\n\nIt specifies which variable in our dataset indicates the region (here state_name).\nIt connects this variable (state_name) to the region variable in our mapping background (states_map). These variables must have the same possible outcomes in order to be matched up (alabama, alaska, arizona,…).\n\n\n\nexpand_limits() assures that the map covers the entire area it’s supposed to, by pulling longitudes and latitudes from the states_map.\nPart d\nWe used geom_sf() for point maps. What geom do we use for choropleth maps?\nExercise 9: County-level choropleth maps\nLet’s map the 2020 Republican support in each county.\nPart a\nWe again need two pieces of information.\n\nData on elections in each county, which we already have: elections_by_county.\nA background map of county boundaries in the US, stored in the county_map dataset in the socviz package:\n\n\n# Get the latitude and longitude coordinates of county boundaries\nlibrary(socviz)\ndata(county_map) \n\n# Check it out\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\n\n\n13.4.1.4 Pause\nImportant detail: We officially have a headache. Our county_map refers to each county by a 5-number id. Our elections_by_counties data refers to each county by a county_fips code, which is mostly the same as id, BUT drops any 0’s at the beginning of the code.\n\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\nhead(elections_by_counties)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\nThis just means that we have to wrangle the data so that it can communicate with the background map.\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\n\n13.4.1.5 Part b\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\nNow map Republican support by county. Let’s go straight to the discretized repub_20_categories variable, and a good color scale.\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = repub_20_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n13.4.2 Exercise 10: Play around!\nConstruct county-level maps of median_rent and median_age.\n\nggplot(elections_by_counties, aes(median_age))+\ngeom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n# Map it\n# Note where the dataset, elections_by_state, is used. It is being used in the first unit of the ggplot().\n# Note where the background map, states_map, is used. Background map is being used in geom_map.\n\nelections_by_counties_median_age &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(median_age_categories = \n           cut(median_age, \n               breaks = seq(20, 80, by = 10),\n               include.lowest = TRUE))\n\nggplot(elections_by_counties_median_age, aes(map_id = county_fips, fill = median_age_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"Median Age\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\nExercise 11: Choropleth maps with leaflet\nThough ggplot() is often better for this purpose, we can also make choropleth maps with leaflet(). If you’re curious, check out the leaflet documentation:\nhttps://rstudio.github.io/leaflet/choropleths.html"
  },
  {
    "objectID": "ica/ica-spatial.html#solutions",
    "href": "ica/ica-spatial.html#solutions",
    "title": "\n13  Spatial Viz\n",
    "section": "\n13.5 Solutions",
    "text": "13.5 Solutions\n\nClick for Solutions\nExample 1\nBoth addresses used between 0 and 450 therms per month. There seem to be two types of months – those with lower use around 50 therms and those with higher use around 300/400 therms.\n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 2\nEnergy use is seasonal, with higher usage in winter months. It seems that address a uses slightly more energy.\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_point()\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_line()\n\n\n\n\nExample 3\nAt both addresses, typical energy use increased after renovations.\n\nggplot(energy, aes(y = therms, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n# A density plot isn't very helpful for comparing typical therms in this example!\nggplot(energy, aes(x = therms, fill = renovated)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ address)\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\nExample 4\nlurking variable = outdoor temperature (as reflected by hdd)\n\n# It happened to be colder outside after renovations (higher hdd)\nggplot(energy, aes(y = hdd, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 12 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExample 5\nBUT this was explained by a confounding or omitted or lurking variable: hdd (outdoor temperature)\n\nAfter renovation…\n\nit happened to be colder…\nwhich then leads to higher energy use.\n\nThus, when controlling for outdoor temps, renovations led to decreased energy use.\nExercise 3: Your turn\n\nleaflet(data = starbucks_mn) |&gt; \n  addTiles() |&gt; \n  addMarkers()\n\nAssuming \"Longitude\" and \"Latitude\" are longitude and latitude, respectively\n\n\n\n\n\n\nExercise 3: A simple scatterplot\nIt would be nice to also have some actual reference maps of countries in the background.\n\nggplot(starbucks, aes(y = Latitude, x = Longitude)) + \n  geom_point(size = 0.5)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExercise 6: A state and county-level map\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) +\n  geom_sf() +\n  geom_point(\n    data = starbucks_midwest,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.7,\n    size = 0.2,\n    color = 'darkgreen'\n  ) +\n  theme_map()\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3,\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\nggplot(cma_boundaries) + \n  geom_sf() + \n  geom_density_2d(\n    data = starbucks_cma,\n    aes(x = Longitude, y = Latitude),\n    size = 0.2,\n    color = \"darkgreen\"\n  ) +\n  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  theme_map()\n\n\n\n\nExercises Part 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first.\nWe’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state |&gt; \n  filter(state_abbr != \"DC\") |&gt; \n  select(state_name, state_abbr, repub_pct_20) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties |&gt; \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) |&gt; \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\nExercise 8: State-level choropleth maps\nPart d\ngeom_map()\nExercise 10: Play around!\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_rent)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median rent\", colors = c(\"white\", \"lightgreen\", \"darkgreen\"))\n\n\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_age)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median age\", colors = terrain.colors(10))"
  },
  {
    "objectID": "ica/ica-effective.html#exercises",
    "href": "ica/ica-effective.html#exercises",
    "title": "\n14  Effective Viz\n",
    "section": "\n14.1 Exercises",
    "text": "14.1 Exercises\nExercise 1: Professionalism\nLet’s examine weather in 3 Australian locations.\n\n# Load tidyverse package for plotting and wrangling\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Import the data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))\n\nThe following plot is fine for things like homework or just playing around. But we’ll make it more “professional” looking below.\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nPart a\nReplace A, B, C, and D in the code below to:\n\nAdd a short, but descriptive title. Under 10 words.\nChange the x- and y-axis labels, currently just the names of the variables in the dataset. These should be short and include units.\nChange the legend title to “Location” (just for practice, not because it’s better than “location”).\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temperature at 9 am (C)\", y = \"Temperature at 3 pm (C)\", title = \"Morning vs Afternoon Temperatures\", color = \"Location\")  \n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n14.1.0.1 Part b\nWhen we’re including our plot in an article, paper, book, or other similar outlet, we should (and are expected to) provide a more descriptive figure caption. Typically, this is instead of a title and is more descriptive of what exactly is being plotted.\n\nAdd a figure caption in the top of the chunk.\nInclude your x-axis, y-axis, and legend labels from Part a.\nRender your qmd and check out how the figure caption appears.\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temperature at 9 am (C)\", y = \"Temperature at 3 pm (C)\", color = \"Location\")  \n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nA scatterplot depicting the relationship between morning and afternoon temperatures, varied by three separate locations\n\n\n\n\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n???\n\n\n\n\nExercise 2: Accessibility\nLet’s now make a graphic more accessible.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\nPart a\nLet’s add some alt text that can be picked up by screen readers. This is a great resource on writing alt text for data viz. In short, whereas figure captions are quick descriptions which assume that the viz is accessible, alt text is a longer description which assumes the viz is not accessible. Alt text should concisely articulate:\n\nWhat your visualization is (e.g. a density plot of 3pm temperatures in Hobart, Uluru, and Wollongong, Australia).\nA 1-sentence description of the most important takeaway.\nA link to your data source if it’s not already in the caption.\n\nAdd appropriate alt text at the top of the chunk, in fig-alt. Then render your qmd, and hover over the image in your rendered html file to check out the alt text.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\nPart b\nColor is another important accessibility consideration. Let’s check out the color accessibility of our density plot.\n\nRun the ggplot() code from Part a in your console. The viz will pop up in the Plots tab.\nIn the Plots tab, click “Export” then “Save as image”. Save the image somewhere.\nNavigate to https://www.color-blindness.com/coblis-color-blindness-simulator/\n\nAbove the image of crayons (I think it’s crayons?), click “Choose file” and choose the plot file you just saved.\nClick the various simulator buttons (eg: Red-Weak/Protanomaly) to check out how the colors in this plot might appear to others.\nSummarize what you learn. What impact might our color choices have on one’s ability to interpret the viz?\nPart c\nWe can change our color schemes! There are many color-blind friendly palettes in R. In the future, we’ll set a default, more color-blind friendly color theme at the top of our Rmds. We can also do this individually for any plot that uses color. Run the chunks below to explore various options.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_fill_viridis_d()    \n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n# In the color scale line:\n# Change \"fill\" to \"color\" since we use color in the aes()\n# Change \"d\" (discrete) to \"c\" (continuous) since maxtemp is on a continuous scale\nggplot(weather, aes(y = temp3pm, x = temp9am, color = maxtemp)) + \n  geom_point(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_color_viridis_c()\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\nExercise 3: Ethics\nLet’s scratch the surface of ethics in data viz. Central to this discussion is the consideration of impact.\nPart a\nAt a minimum, our data viz should not mislead. Reconsider the climate change example from above. Why is this plot unethical and what impact might it have on policy, public opinion, etc?\n\nPart b\nAgain, data viz ethical considerations go beyond whether or not a plot is misleading. As described in the warm-up, we need to consider: visibility, privacy, power, emotion & embodiment, pluralism, & context. Depending upon the audience and goals of a data viz, addressing these points might require more nuance. Mainly, the viz tools we’ve learned are a great base or foundation, but aren’t the only approaches to data viz. \nPick one or more of the following examples of data viz to discuss with your group. How do the approaches taken:\n\nemphasize one or more of: visibility, privacy, power, emotion, embodiment, pluralism, and/or context?\nimprove upon what we might be able to convey with a simpler bar chart, scatterplot, etc?\n\n\nExample: W.E.B. Du Bois (1868–1963)\nDu Bois (“Doo Boys”) was a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1. He was also a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. Du Bois noted: “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. NOTE: This work uses language common to that time period and addresses the topic of slavery. Check out:\n\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\nAn article by Allen Hillery (@AlDatavizguy).\n\n\nExample: One person’s experience with long COVID\nNYT article\n\nExample: Decolonizing data viz\nblog post\n\nExample: Visualizing climate change through art\nFutures North with Prof John Kim & Mac students (by Prof Kim, Mac research students)\n\nExample: Personal data collection\nDear Data\n\nPart c\nFor a deeper treatment of similar topics, and more examples, read Data Feminism.\n\nExercise 4: Critique\nPractice critiquing some more complicated data viz listed at Modern Data Science with R, Exercise 2.5.\nThink about the following questions:\n\nWhat story does the data graphic tell? What is the main message that you take away from it?\nCan the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nCritique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Are there things that you would have done differently?\n\n\nExercise 5: Design Details\nThis final exercise is just “food for thought”. It’s more of a discussion than an exercise, and gets into some of the finer design details and data viz theory. Go as deep or not deep as you’d like here.\nIn refining the details of our data viz, Visualize This and Storytelling with Data provide some of their guiding principles. But again, every context is different.\n\nPut yourself in a reader’s shoes. What parts of the data need explanation?\nShine a light on your data. Try to remove any “chart junk” that distracts from the data.\nVary color and style to emphasize the viz elements that are most important to the story you’re telling.\nIt is easier to judge length than it is to judge area or angles.\nBe thoughtful about how your categories are ordered for categorical data.\n\nGetting into even more of the nitty gritty, we need to be mindful of what geometric elements and aesthetics we use. The following elements/aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor. (Color is the most difficult, because it is a 3-dimensional quantity.)\n\nFinally, here are some facts to keep in mind about visual perception from Now You See It.\nPart a: Selectivity\nVisual perception is selective, and our attention is often drawn to contrasts from the norm.\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\nExample: What stands out in this example image? This is originally from C. Ware, Information Visualization: Perception for Design, 2004? Source: S. Few, Now You See It, 2009, p. 33.\n\nPart b: Familiarity\nOur eyes are drawn to familiar patterns. We observe what we know and expect.\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\nExample: Do you notice anything embedded in this rose image from coolbubble.com? Source: S. Few, Now You See It, 2009, p. 34.\n\nPart c: Revisit\nRevisit Part b. Do you notice anything in the shadows? Go to https://mac-stat.github.io/images/112/rose2.png for an image.\n\nWrapping up\nIf you finish early:\n\nWork on homework if not done already\nComplete any activities you haven’t finished yet, eg, spatial viz, the optional but fun exercises in the Multivariate viz and Bivariate viz activities.\nIf you’ve done all that, explore some datasets in TidyTuesday."
  },
  {
    "objectID": "ica/ica-effective.html#solutions",
    "href": "ica/ica-effective.html#solutions",
    "title": "\n14  Effective Viz\n",
    "section": "\n14.2 Solutions",
    "text": "14.2 Solutions\nThe exercises today are discussion based. There are no “solutions”. Happy to chat in office hours about any ideas here!"
  },
  {
    "objectID": "ica/ica-effective.html#footnotes",
    "href": "ica/ica-effective.html#footnotes",
    "title": "\n14  Effective Viz\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎\nB. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15.↩︎"
  },
  {
    "objectID": "ica/ica-dates.html#warm-up",
    "href": "ica/ica-dates.html#warm-up",
    "title": "\n15  Dates\n",
    "section": "\n15.1 Warm-up",
    "text": "15.1 Warm-up\nData Science Process\nBelow is the visual representation of the data science process we saw earlier. Which stage are we in currently?\n\nRecall that wrangling is important. It is much of what we spend our efforts on in Data Science. There are lots of steps, hence R functions, that can go into data wrangling. But we can get far with the following 6 wrangling verbs:\n\n\nverb\naction\n\n\n\narrange\n\narrange the rows according to some column\n\n\n\nfilter\n\nfilter out or obtain a subset of the rows\n\n\n\nselect\n\nselect a subset of columns\n\n\n\nmutate\n\nmutate or create a column\n\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\n\ngroup_by\n\ngroup the rows by a specified column\n\n\n\nExample 1: Single Verb\nLet’s start by working with some TidyTuesday data on penguins. This data includes information about penguins’ flippers (“arms”) and bills (“mouths” or “beaks”). Let’s import this using read_csv(), a function in the tidyverse package. For the most part, this is similar to read.csv(), though read_csv() can be more efficient at importing large datasets.\n\nlibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nConstruct a plot that allows us to examine how the relationship between body mass and bill length varies by species and sex.\n\n#Making a scatter plot with x= body mass, y = bill length, color = species, and facet wrap by sex\nggplot(penguins, aes(x = body_mass_g, y = bill_length_mm, color = sex))+\n  geom_point() +\n  facet_wrap(~species)\n\n\n\n\nUse the 6 wrangling verbs to address each task in the code chunk below. You can tack on |&gt; head() to print out just 6 rows to keep your rendered document manageable. Most of these require just 1 verb.\n\n# Get data on only Adelie penguins that weigh more than 4700g\npenguins |&gt;\n  filter(species == \"Adelie\", body_mass_g &gt; 4700)|&gt;\n  head()\n\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Biscoe           41              20               203        4725\n2 Adelie  Biscoe           43.2            19               197        4775\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Get data on penguin body mass only\n# Show just the first 6 rows\npenguins |&gt;\n  select(body_mass_g)|&gt;\n  head()\n\n# A tibble: 6 × 1\n  body_mass_g\n        &lt;dbl&gt;\n1        3750\n2        3800\n3        3250\n4          NA\n5        3450\n6        3650\n\n# Sort the penguins from smallest to largest body mass\n# Show just the first 6 rows\npenguins |&gt;\n  arrange(body_mass_g)|&gt;\n  head()\n\n# A tibble: 6 × 8\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Chinstrap Dream               46.9          16.6               192        2700\n2 Adelie    Biscoe              36.5          16.6               181        2850\n3 Adelie    Biscoe              36.4          17.1               184        2850\n4 Adelie    Biscoe              34.5          18.1               187        2900\n5 Adelie    Dream               33.1          16.1               178        2900\n6 Adelie    Torgersen           38.6          17                 188        2900\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Calculate the average body mass across all penguins\n# Note: na.rm = TRUE removes the NAs from the calculation\npenguins |&gt;\n  summarize(mean = mean(body_mass_g, na.rm = TRUE))|&gt;\n  head()\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 4202.\n\n# Calculate the average body mass by species\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species    mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Adelie    3701.\n2 Chinstrap 3733.\n3 Gentoo    5076.\n\n# Create a new column that records body mass in kilograms, not grams\n# NOTE: there are 1000 g in 1 kg\n# Show just the first 6 rows\npenguins |&gt; \n  mutate(body_mass_kg = body_mass_g/1000) |&gt; \n  head()\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, body_mass_kg &lt;dbl&gt;\n\n\nHow many penguins of each species do we have? Create a viz that addresses this question.\n\nggplot(penguins, aes(x = species))+\n  geom_bar(color = \"blue\", fill = \"pink\")\n\n\n\n\nCan we use the 6 verbs to calculate exactly how many penguins in each species?\nHINT: n() calculates group size.\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(n()) |&gt;\n  head ()\n\n# A tibble: 3 × 2\n  species   `n()`\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nThe count() verb provides a handy shortcut!\n\n#The count function provides a shortcut to count the total amount of individuals in a column.\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nExample 2: Multiple Verbs\nLet’s practice combining some verbs. For each task:\n\nTranslate the prompt into our 6 verbs. That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\nAsk what you can rearrange and still get the same result.\nRead your final code like a paragraph / a conversation. Would another person be able to follow your logic?\n\n\n# Sort Gentoo penguins from biggest to smallest with respect to their bill length in cm (there are 10 mm in a cm)\npenguins |&gt; \n  filter(species == \"Gentoo\") |&gt; \n  mutate(bill_length_cm = bill_length_mm/10)|&gt;\n  arrange(desc(bill_length_cm)) |&gt;\n  head ()\n\n# A tibble: 6 × 9\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6          17                 230        6050\n2 Gentoo  Biscoe           55.9          17                 228        5600\n3 Gentoo  Biscoe           55.1          16                 230        5850\n4 Gentoo  Biscoe           54.3          15.7               231        5650\n5 Gentoo  Biscoe           53.4          15.8               219        5500\n6 Gentoo  Biscoe           52.5          15.6               221        5450\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, bill_length_cm &lt;dbl&gt;\n\n\n\n# Sort the species from smallest to biggest with respect to their average bill length in cm\npenguins |&gt; \n  mutate(bill_length_cm = bill_length_mm/10) |&gt;\n  group_by(species) |&gt;\n  summarize(mean_bill_length = mean(bill_length_cm, na.rm = TRUE)) |&gt;\n  arrange(desc(mean_bill_length)) |&gt;\n  head ()\n\n# A tibble: 3 × 2\n  species   mean_bill_length\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Chinstrap             4.88\n2 Gentoo                4.75\n3 Adelie                3.88\n\n\nExample 3: Interpret Code\nLet’s practice reading and making sense of somebody else’s code. What do you think this produces?\n\nHow many columns? Rows?\nWhat are the column names?\nWhat’s represented in each row?\n\nOnce you’ve thought about it, put the code inside a chunk and run it!\npenguins |&gt; filter(species == “Chinstrap”) |&gt; group_by(sex) |&gt; summarize(min = min(body_mass_g), max = max(body_mass_g)) |&gt; mutate(range = max - min)\n\npenguins |&gt; \n  filter(species == \"Chinstrap\") |&gt; \n  group_by(sex) |&gt; \n  summarize(min = min(body_mass_g), max = max(body_mass_g)) |&gt; \n  mutate(range = max - min)\n\n# A tibble: 2 × 4\n  sex      min   max range\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 female  2700  4150  1450\n2 male    3250  4800  1550"
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "href": "ica/ica-dates.html#exercises-part-1-same-verbs-new-tricks",
    "title": "\n15  Dates\n",
    "section": "\n15.2 Exercises Part 1: Same Verbs, New Tricks",
    "text": "15.2 Exercises Part 1: Same Verbs, New Tricks\nExercise 1: More Filtering\nRecall the “logical comparison operators” we can use to filter() our data:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(***, ***)\na list of multiple values\n\n\n\nPart a\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\nPart b\nNotice that some of our penguins have missing (NA) data on some values:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nThere are many ways to handle missing data. The right approach depends upon your research goals. A general rule is: Only get rid of observations with missing data if they’re missing data on variables you need for the specific task at hand!\nExample 1\nSuppose our research focus is just on body_mass_g. Two penguins are missing this info:\n\n# NOTE the use of is.na()\npenguins |&gt; \n  summarize(sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  `sum(is.na(body_mass_g))`\n                      &lt;int&gt;\n1                         2\n\n\nLet’s define a new dataset that removes these penguins:\n\n# NOTE the use of is.na()\npenguins_w_body_mass &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g))\n\n# Compare the number of penguins in this vs the original data\nnrow(penguins_w_body_mass)\n\n[1] 342\n\nnrow(penguins)\n\n[1] 344\n\n\nNote that some penguins in penguins_w_body_mass are missing info on sex, but we don’t care since that’s not related to our research question:\n\npenguins_w_body_mass |&gt; \n  summarize(sum(is.na(sex)))\n\n# A tibble: 1 × 1\n  `sum(is.na(sex))`\n              &lt;int&gt;\n1                 9\n\n\nExample 2\nIn the very rare case that we need complete information on every variable for the specific task at hand, we can use na.omit() to get rid of any penguin that’s missing info on any variable:\n\npenguins_complete &lt;- penguins |&gt; \n  na.omit()\n\nHow many penguins did this eliminate?\n\nnrow(penguins_complete)\n\n[1] 333\n\nnrow(penguins)\n\n[1] 344\n\n\nPart c\nExplain why we should only use na.omit() in extreme circumstances.\nOtherwise we are getting rid of very valuable data!\nExercise 2: More Selecting\nBeing able to select() only certain columns can help simplify our data. This is especially important when we’re working with lots of columns (which we haven’t done yet). It can also get tedious to type out every column of interest. Here are some shortcuts:\n\n\n- removes a given variable and keeps all others (e.g. select(-island))\n\nstarts_with(\"___\"), ends_with(\"___\"), or contains(\"___\") selects only the columns that either start with, end with, or simply contain the given string of characters\n\nUse these shortcuts to create the following datasets.\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins |&gt; \n  select(-year, -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; \n  select (species, ends_with(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt;\n  select (species, starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt;\n  select (species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\nExercise 3: Arranging, Counting, & Grouping by Multiple Variables\nWe’ve done examples where we need to filter() by more than one variable, or select() more than one variable. Use your intuition for how we can arrange(), count(), and group_by() more than one variable.\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species, island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\nExercise 4: Dates\nBefore some wrangling practice, let’s explore another important concept: working with or mutating date variables. Dates are a whole special object type or class in R that automatically respect the order of time.\n\n# Get today's date\nas.Date(today())\n\n[1] \"2025-04-04\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\nThe lubridate package inside tidyverse contains functions that can extract various information from dates. Let’s learn about some of the most common functions by applying them to today. For each, make a comment on what the function does\n\n#Finds the year of today\nyear(today)\n\n[1] 2025\n\n\n\n# What do these lines produce / what's their difference?\nmonth(today) # Gives me the numerical value of the month of today\n\n[1] 4\n\nmonth(today, label = TRUE) # Gives me the abbreviation name of the month\n\n[1] Apr\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# What does this number mean?\nweek(today)\n\n[1] 14\n\n# It is the ninth week of the year.\n\n\n# What do these lines produce / what's their difference?\nmday(today) # The day of the month\n\n[1] 4\n\nyday(today)  # This is often called the \"Julian day\", the day of the year\n\n[1] 94\n\n\n\n# What do these lines produce / what's their difference?\nwday(today) # fourth day of the week \n\n[1] 6\n\nwday(today, label = TRUE) #name of the day of the week\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# What do the results of these 2 lines tell us?\ntoday &gt;= ymd(\"2024-02-14\") # Tells us that today is later that or is on 2/14 \n\n[1] TRUE\n\ntoday &lt; ymd(\"2024-02-14\") # Tells us that today is not earlier than 2/14\n\n[1] FALSE"
  },
  {
    "objectID": "ica/ica-dates.html#exercises-part-2-application",
    "href": "ica/ica-dates.html#exercises-part-2-application",
    "title": "\n15  Dates\n",
    "section": "\n15.3 Exercises Part 2: Application",
    "text": "15.3 Exercises Part 2: Application\nThe remaining exercises are similar to some of those on the homework. Hence, the solutions are not provided. Let’s apply these ideas to the daily Birthdays dataset in the mosaic package.\n\nlibrary(mosaic)\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nBirthdays gives the number of births recorded on each day of the year in each state from 1969 to 19881. We can use our wrangling skills to understand some drivers of daily births. Putting these all together can be challenging! Remember the following ways to make tasks more manageable:\n\nTranslate the prompt into our 6 verbs (and count()). That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\n\n\n15.3.1 Exercise 5: Warming up\n\n# How many days of data do we have for each state?\nBirthdays |&gt;\n  count(state, day)\n\n     state day   n\n1       AK   1 240\n2       AK   2 240\n3       AK   3 240\n4       AK   4 240\n5       AK   5 240\n6       AK   6 240\n7       AK   7 240\n8       AK   8 240\n9       AK   9 240\n10      AK  10 240\n11      AK  11 240\n12      AK  12 240\n13      AK  13 240\n14      AK  14 240\n15      AK  15 240\n16      AK  16 240\n17      AK  17 240\n18      AK  18 240\n19      AK  19 240\n20      AK  20 240\n21      AK  21 240\n22      AK  22 240\n23      AK  23 240\n24      AK  24 240\n25      AK  25 240\n26      AK  26 240\n27      AK  27 240\n28      AK  28 240\n29      AK  29 225\n30      AK  30 220\n31      AK  31 141\n32      AL   1 240\n33      AL   2 240\n34      AL   3 240\n35      AL   4 240\n36      AL   5 240\n37      AL   6 240\n38      AL   7 240\n39      AL   8 240\n40      AL   9 240\n41      AL  10 240\n42      AL  11 240\n43      AL  12 240\n44      AL  13 240\n45      AL  14 240\n46      AL  15 240\n47      AL  16 240\n48      AL  17 240\n49      AL  18 240\n50      AL  19 240\n51      AL  20 240\n52      AL  21 240\n53      AL  22 240\n54      AL  23 240\n55      AL  24 240\n56      AL  25 240\n57      AL  26 240\n58      AL  27 240\n59      AL  28 240\n60      AL  29 226\n61      AL  30 221\n62      AL  31 145\n63      AR   1 240\n64      AR   2 240\n65      AR   3 240\n66      AR   4 240\n67      AR   5 240\n68      AR   6 240\n69      AR   7 240\n70      AR   8 240\n71      AR   9 240\n72      AR  10 240\n73      AR  11 240\n74      AR  12 240\n75      AR  13 240\n76      AR  14 240\n77      AR  15 240\n78      AR  16 240\n79      AR  17 240\n80      AR  18 240\n81      AR  19 240\n82      AR  20 240\n83      AR  21 240\n84      AR  22 240\n85      AR  23 240\n86      AR  24 240\n87      AR  25 240\n88      AR  26 240\n89      AR  27 240\n90      AR  28 240\n91      AR  29 226\n92      AR  30 220\n93      AR  31 144\n94      AZ   1 240\n95      AZ   2 240\n96      AZ   3 240\n97      AZ   4 240\n98      AZ   5 240\n99      AZ   6 240\n100     AZ   7 240\n101     AZ   8 240\n102     AZ   9 240\n103     AZ  10 240\n104     AZ  11 240\n105     AZ  12 240\n106     AZ  13 240\n107     AZ  14 240\n108     AZ  15 240\n109     AZ  16 240\n110     AZ  17 240\n111     AZ  18 240\n112     AZ  19 240\n113     AZ  20 240\n114     AZ  21 240\n115     AZ  22 240\n116     AZ  23 240\n117     AZ  24 240\n118     AZ  25 240\n119     AZ  26 240\n120     AZ  27 240\n121     AZ  28 240\n122     AZ  29 226\n123     AZ  30 221\n124     AZ  31 143\n125     CA   1 240\n126     CA   2 240\n127     CA   3 240\n128     CA   4 240\n129     CA   5 240\n130     CA   6 240\n131     CA   7 240\n132     CA   8 240\n133     CA   9 240\n134     CA  10 240\n135     CA  11 240\n136     CA  12 240\n137     CA  13 240\n138     CA  14 240\n139     CA  15 240\n140     CA  16 240\n141     CA  17 240\n142     CA  18 240\n143     CA  19 240\n144     CA  20 240\n145     CA  21 240\n146     CA  22 240\n147     CA  23 240\n148     CA  24 240\n149     CA  25 240\n150     CA  26 240\n151     CA  27 240\n152     CA  28 240\n153     CA  29 228\n154     CA  30 221\n155     CA  31 156\n156     CO   1 240\n157     CO   2 240\n158     CO   3 240\n159     CO   4 240\n160     CO   5 240\n161     CO   6 240\n162     CO   7 240\n163     CO   8 240\n164     CO   9 240\n165     CO  10 240\n166     CO  11 240\n167     CO  12 240\n168     CO  13 240\n169     CO  14 240\n170     CO  15 240\n171     CO  16 240\n172     CO  17 240\n173     CO  18 240\n174     CO  19 240\n175     CO  20 240\n176     CO  21 240\n177     CO  22 240\n178     CO  23 240\n179     CO  24 240\n180     CO  25 240\n181     CO  26 240\n182     CO  27 240\n183     CO  28 240\n184     CO  29 225\n185     CO  30 220\n186     CO  31 140\n187     CT   1 240\n188     CT   2 240\n189     CT   3 240\n190     CT   4 240\n191     CT   5 240\n192     CT   6 240\n193     CT   7 240\n194     CT   8 240\n195     CT   9 240\n196     CT  10 240\n197     CT  11 240\n198     CT  12 240\n199     CT  13 240\n200     CT  14 240\n201     CT  15 240\n202     CT  16 240\n203     CT  17 240\n204     CT  18 240\n205     CT  19 240\n206     CT  20 240\n207     CT  21 240\n208     CT  22 240\n209     CT  23 240\n210     CT  24 240\n211     CT  25 240\n212     CT  26 240\n213     CT  27 240\n214     CT  28 240\n215     CT  29 227\n216     CT  30 221\n217     CT  31 144\n218     DC   1 240\n219     DC   2 240\n220     DC   3 240\n221     DC   4 240\n222     DC   5 240\n223     DC   6 240\n224     DC   7 240\n225     DC   8 240\n226     DC   9 240\n227     DC  10 240\n228     DC  11 240\n229     DC  12 240\n230     DC  13 240\n231     DC  14 240\n232     DC  15 240\n233     DC  16 240\n234     DC  17 240\n235     DC  18 240\n236     DC  19 240\n237     DC  20 240\n238     DC  21 240\n239     DC  22 240\n240     DC  23 240\n241     DC  24 240\n242     DC  25 240\n243     DC  26 240\n244     DC  27 240\n245     DC  28 240\n246     DC  29 227\n247     DC  30 220\n248     DC  31 144\n249     DE   1 240\n250     DE   2 240\n251     DE   3 240\n252     DE   4 240\n253     DE   5 240\n254     DE   6 240\n255     DE   7 240\n256     DE   8 240\n257     DE   9 240\n258     DE  10 240\n259     DE  11 240\n260     DE  12 240\n261     DE  13 240\n262     DE  14 240\n263     DE  15 240\n264     DE  16 240\n265     DE  17 240\n266     DE  18 240\n267     DE  19 240\n268     DE  20 240\n269     DE  21 240\n270     DE  22 240\n271     DE  23 240\n272     DE  24 240\n273     DE  25 240\n274     DE  26 240\n275     DE  27 240\n276     DE  28 240\n277     DE  29 225\n278     DE  30 220\n279     DE  31 142\n280     FL   1 240\n281     FL   2 240\n282     FL   3 240\n283     FL   4 240\n284     FL   5 240\n285     FL   6 240\n286     FL   7 240\n287     FL   8 240\n288     FL   9 240\n289     FL  10 240\n290     FL  11 240\n291     FL  12 240\n292     FL  13 240\n293     FL  14 240\n294     FL  15 240\n295     FL  16 240\n296     FL  17 240\n297     FL  18 240\n298     FL  19 240\n299     FL  20 240\n300     FL  21 240\n301     FL  22 240\n302     FL  23 240\n303     FL  24 240\n304     FL  25 240\n305     FL  26 240\n306     FL  27 240\n307     FL  28 240\n308     FL  29 225\n309     FL  30 220\n310     FL  31 142\n311     GA   1 240\n312     GA   2 240\n313     GA   3 240\n314     GA   4 240\n315     GA   5 240\n316     GA   6 240\n317     GA   7 240\n318     GA   8 240\n319     GA   9 240\n320     GA  10 240\n321     GA  11 240\n322     GA  12 240\n323     GA  13 240\n324     GA  14 240\n325     GA  15 240\n326     GA  16 240\n327     GA  17 240\n328     GA  18 240\n329     GA  19 240\n330     GA  20 240\n331     GA  21 240\n332     GA  22 240\n333     GA  23 240\n334     GA  24 240\n335     GA  25 240\n336     GA  26 240\n337     GA  27 240\n338     GA  28 240\n339     GA  29 227\n340     GA  30 220\n341     GA  31 147\n342     HI   1 240\n343     HI   2 240\n344     HI   3 240\n345     HI   4 240\n346     HI   5 240\n347     HI   6 240\n348     HI   7 240\n349     HI   8 240\n350     HI   9 240\n351     HI  10 240\n352     HI  11 240\n353     HI  12 240\n354     HI  13 240\n355     HI  14 240\n356     HI  15 240\n357     HI  16 240\n358     HI  17 240\n359     HI  18 240\n360     HI  19 240\n361     HI  20 240\n362     HI  21 240\n363     HI  22 240\n364     HI  23 240\n365     HI  24 240\n366     HI  25 240\n367     HI  26 240\n368     HI  27 240\n369     HI  28 240\n370     HI  29 225\n371     HI  30 220\n372     HI  31 141\n373     IA   1 240\n374     IA   2 240\n375     IA   3 240\n376     IA   4 240\n377     IA   5 240\n378     IA   6 240\n379     IA   7 240\n380     IA   8 240\n381     IA   9 240\n382     IA  10 240\n383     IA  11 240\n384     IA  12 240\n385     IA  13 240\n386     IA  14 240\n387     IA  15 240\n388     IA  16 240\n389     IA  17 240\n390     IA  18 240\n391     IA  19 240\n392     IA  20 240\n393     IA  21 240\n394     IA  22 240\n395     IA  23 240\n396     IA  24 240\n397     IA  25 240\n398     IA  26 240\n399     IA  27 240\n400     IA  28 240\n401     IA  29 226\n402     IA  30 220\n403     IA  31 140\n404     ID   1 240\n405     ID   2 240\n406     ID   3 240\n407     ID   4 240\n408     ID   5 240\n409     ID   6 240\n410     ID   7 240\n411     ID   8 240\n412     ID   9 240\n413     ID  10 240\n414     ID  11 240\n415     ID  12 240\n416     ID  13 240\n417     ID  14 240\n418     ID  15 240\n419     ID  16 240\n420     ID  17 240\n421     ID  18 240\n422     ID  19 240\n423     ID  20 240\n424     ID  21 240\n425     ID  22 240\n426     ID  23 240\n427     ID  24 240\n428     ID  25 240\n429     ID  26 240\n430     ID  27 240\n431     ID  28 240\n432     ID  29 225\n433     ID  30 220\n434     ID  31 141\n435     IL   1 240\n436     IL   2 240\n437     IL   3 240\n438     IL   4 240\n439     IL   5 240\n440     IL   6 240\n441     IL   7 240\n442     IL   8 240\n443     IL   9 240\n444     IL  10 240\n445     IL  11 240\n446     IL  12 240\n447     IL  13 240\n448     IL  14 240\n449     IL  15 240\n450     IL  16 240\n451     IL  17 240\n452     IL  18 240\n453     IL  19 240\n454     IL  20 240\n455     IL  21 240\n456     IL  22 240\n457     IL  23 240\n458     IL  24 240\n459     IL  25 240\n460     IL  26 240\n461     IL  27 240\n462     IL  28 240\n463     IL  29 226\n464     IL  30 221\n465     IL  31 147\n466     IN   1 240\n467     IN   2 240\n468     IN   3 240\n469     IN   4 240\n470     IN   5 240\n471     IN   6 240\n472     IN   7 240\n473     IN   8 240\n474     IN   9 240\n475     IN  10 240\n476     IN  11 240\n477     IN  12 240\n478     IN  13 240\n479     IN  14 240\n480     IN  15 240\n481     IN  16 240\n482     IN  17 240\n483     IN  18 240\n484     IN  19 240\n485     IN  20 240\n486     IN  21 240\n487     IN  22 240\n488     IN  23 240\n489     IN  24 240\n490     IN  25 240\n491     IN  26 240\n492     IN  27 240\n493     IN  28 240\n494     IN  29 227\n495     IN  30 221\n496     IN  31 143\n497     KS   1 240\n498     KS   2 240\n499     KS   3 240\n500     KS   4 240\n501     KS   5 240\n502     KS   6 240\n503     KS   7 240\n504     KS   8 240\n505     KS   9 240\n506     KS  10 240\n507     KS  11 240\n508     KS  12 240\n509     KS  13 240\n510     KS  14 240\n511     KS  15 240\n512     KS  16 240\n513     KS  17 240\n514     KS  18 240\n515     KS  19 240\n516     KS  20 240\n517     KS  21 240\n518     KS  22 240\n519     KS  23 240\n520     KS  24 240\n521     KS  25 240\n522     KS  26 240\n523     KS  27 240\n524     KS  28 240\n525     KS  29 226\n526     KS  30 221\n527     KS  31 144\n528     KY   1 240\n529     KY   2 240\n530     KY   3 240\n531     KY   4 240\n532     KY   5 240\n533     KY   6 240\n534     KY   7 240\n535     KY   8 240\n536     KY   9 240\n537     KY  10 240\n538     KY  11 240\n539     KY  12 240\n540     KY  13 240\n541     KY  14 240\n542     KY  15 240\n543     KY  16 240\n544     KY  17 240\n545     KY  18 240\n546     KY  19 240\n547     KY  20 240\n548     KY  21 240\n549     KY  22 240\n550     KY  23 240\n551     KY  24 240\n552     KY  25 240\n553     KY  26 240\n554     KY  27 240\n555     KY  28 240\n556     KY  29 226\n557     KY  30 221\n558     KY  31 146\n559     LA   1 240\n560     LA   2 240\n561     LA   3 240\n562     LA   4 240\n563     LA   5 240\n564     LA   6 240\n565     LA   7 240\n566     LA   8 240\n567     LA   9 240\n568     LA  10 240\n569     LA  11 240\n570     LA  12 240\n571     LA  13 240\n572     LA  14 240\n573     LA  15 240\n574     LA  16 240\n575     LA  17 240\n576     LA  18 240\n577     LA  19 240\n578     LA  20 240\n579     LA  21 240\n580     LA  22 240\n581     LA  23 240\n582     LA  24 240\n583     LA  25 240\n584     LA  26 240\n585     LA  27 240\n586     LA  28 240\n587     LA  29 225\n588     LA  30 220\n589     LA  31 144\n590     MA   1 240\n591     MA   2 240\n592     MA   3 240\n593     MA   4 240\n594     MA   5 240\n595     MA   6 240\n596     MA   7 240\n597     MA   8 240\n598     MA   9 240\n599     MA  10 240\n600     MA  11 240\n601     MA  12 240\n602     MA  13 240\n603     MA  14 240\n604     MA  15 240\n605     MA  16 240\n606     MA  17 240\n607     MA  18 240\n608     MA  19 240\n609     MA  20 240\n610     MA  21 240\n611     MA  22 240\n612     MA  23 240\n613     MA  24 240\n614     MA  25 240\n615     MA  26 240\n616     MA  27 240\n617     MA  28 240\n618     MA  29 226\n619     MA  30 220\n620     MA  31 149\n621     MD   1 240\n622     MD   2 240\n623     MD   3 240\n624     MD   4 240\n625     MD   5 240\n626     MD   6 240\n627     MD   7 240\n628     MD   8 240\n629     MD   9 240\n630     MD  10 240\n631     MD  11 240\n632     MD  12 240\n633     MD  13 240\n634     MD  14 240\n635     MD  15 240\n636     MD  16 240\n637     MD  17 240\n638     MD  18 240\n639     MD  19 240\n640     MD  20 240\n641     MD  21 240\n642     MD  22 240\n643     MD  23 240\n644     MD  24 240\n645     MD  25 240\n646     MD  26 240\n647     MD  27 240\n648     MD  28 240\n649     MD  29 227\n650     MD  30 220\n651     MD  31 144\n652     ME   1 240\n653     ME   2 240\n654     ME   3 240\n655     ME   4 240\n656     ME   5 240\n657     ME   6 240\n658     ME   7 240\n659     ME   8 240\n660     ME   9 240\n661     ME  10 240\n662     ME  11 240\n663     ME  12 240\n664     ME  13 240\n665     ME  14 240\n666     ME  15 240\n667     ME  16 240\n668     ME  17 240\n669     ME  18 240\n670     ME  19 240\n671     ME  20 240\n672     ME  21 240\n673     ME  22 240\n674     ME  23 240\n675     ME  24 240\n676     ME  25 240\n677     ME  26 240\n678     ME  27 240\n679     ME  28 240\n680     ME  29 226\n681     ME  30 221\n682     ME  31 142\n683     MI   1 240\n684     MI   2 240\n685     MI   3 240\n686     MI   4 240\n687     MI   5 240\n688     MI   6 240\n689     MI   7 240\n690     MI   8 240\n691     MI   9 240\n692     MI  10 240\n693     MI  11 240\n694     MI  12 240\n695     MI  13 240\n696     MI  14 240\n697     MI  15 240\n698     MI  16 240\n699     MI  17 240\n700     MI  18 240\n701     MI  19 240\n702     MI  20 240\n703     MI  21 240\n704     MI  22 240\n705     MI  23 240\n706     MI  24 240\n707     MI  25 240\n708     MI  26 240\n709     MI  27 240\n710     MI  28 240\n711     MI  29 229\n712     MI  30 222\n713     MI  31 152\n714     MN   1 240\n715     MN   2 240\n716     MN   3 240\n717     MN   4 240\n718     MN   5 240\n719     MN   6 240\n720     MN   7 240\n721     MN   8 240\n722     MN   9 240\n723     MN  10 240\n724     MN  11 240\n725     MN  12 240\n726     MN  13 240\n727     MN  14 240\n728     MN  15 240\n729     MN  16 240\n730     MN  17 240\n731     MN  18 240\n732     MN  19 240\n733     MN  20 240\n734     MN  21 240\n735     MN  22 240\n736     MN  23 240\n737     MN  24 240\n738     MN  25 240\n739     MN  26 240\n740     MN  27 240\n741     MN  28 240\n742     MN  29 227\n743     MN  30 221\n744     MN  31 147\n745     MO   1 240\n746     MO   2 240\n747     MO   3 240\n748     MO   4 240\n749     MO   5 240\n750     MO   6 240\n751     MO   7 240\n752     MO   8 240\n753     MO   9 240\n754     MO  10 240\n755     MO  11 240\n756     MO  12 240\n757     MO  13 240\n758     MO  14 240\n759     MO  15 240\n760     MO  16 240\n761     MO  17 240\n762     MO  18 240\n763     MO  19 240\n764     MO  20 240\n765     MO  21 240\n766     MO  22 240\n767     MO  23 240\n768     MO  24 240\n769     MO  25 240\n770     MO  26 240\n771     MO  27 240\n772     MO  28 240\n773     MO  29 227\n774     MO  30 220\n775     MO  31 142\n776     MS   1 240\n777     MS   2 240\n778     MS   3 240\n779     MS   4 240\n780     MS   5 240\n781     MS   6 240\n782     MS   7 240\n783     MS   8 240\n784     MS   9 240\n785     MS  10 240\n786     MS  11 240\n787     MS  12 240\n788     MS  13 240\n789     MS  14 240\n790     MS  15 240\n791     MS  16 240\n792     MS  17 240\n793     MS  18 240\n794     MS  19 240\n795     MS  20 240\n796     MS  21 240\n797     MS  22 240\n798     MS  23 240\n799     MS  24 240\n800     MS  25 240\n801     MS  26 240\n802     MS  27 240\n803     MS  28 240\n804     MS  29 225\n805     MS  30 220\n806     MS  31 145\n807     MT   1 240\n808     MT   2 240\n809     MT   3 240\n810     MT   4 240\n811     MT   5 240\n812     MT   6 240\n813     MT   7 240\n814     MT   8 240\n815     MT   9 240\n816     MT  10 240\n817     MT  11 240\n818     MT  12 240\n819     MT  13 240\n820     MT  14 240\n821     MT  15 240\n822     MT  16 240\n823     MT  17 240\n824     MT  18 240\n825     MT  19 240\n826     MT  20 240\n827     MT  21 240\n828     MT  22 240\n829     MT  23 240\n830     MT  24 240\n831     MT  25 240\n832     MT  26 240\n833     MT  27 240\n834     MT  28 240\n835     MT  29 225\n836     MT  30 220\n837     MT  31 140\n838     NC   1 240\n839     NC   2 240\n840     NC   3 240\n841     NC   4 240\n842     NC   5 240\n843     NC   6 240\n844     NC   7 240\n845     NC   8 240\n846     NC   9 240\n847     NC  10 240\n848     NC  11 240\n849     NC  12 240\n850     NC  13 240\n851     NC  14 240\n852     NC  15 240\n853     NC  16 240\n854     NC  17 240\n855     NC  18 240\n856     NC  19 240\n857     NC  20 240\n858     NC  21 240\n859     NC  22 240\n860     NC  23 240\n861     NC  24 240\n862     NC  25 240\n863     NC  26 240\n864     NC  27 240\n865     NC  28 240\n866     NC  29 226\n867     NC  30 220\n868     NC  31 141\n869     ND   1 240\n870     ND   2 240\n871     ND   3 240\n872     ND   4 240\n873     ND   5 240\n874     ND   6 240\n875     ND   7 240\n876     ND   8 240\n877     ND   9 240\n878     ND  10 240\n879     ND  11 240\n880     ND  12 240\n881     ND  13 240\n882     ND  14 240\n883     ND  15 240\n884     ND  16 240\n885     ND  17 240\n886     ND  18 240\n887     ND  19 240\n888     ND  20 240\n889     ND  21 240\n890     ND  22 240\n891     ND  23 240\n892     ND  24 240\n893     ND  25 240\n894     ND  26 240\n895     ND  27 240\n896     ND  28 240\n897     ND  29 225\n898     ND  30 220\n899     ND  31 140\n900     NE   1 240\n901     NE   2 240\n902     NE   3 240\n903     NE   4 240\n904     NE   5 240\n905     NE   6 240\n906     NE   7 240\n907     NE   8 240\n908     NE   9 240\n909     NE  10 240\n910     NE  11 240\n911     NE  12 240\n912     NE  13 240\n913     NE  14 240\n914     NE  15 240\n915     NE  16 240\n916     NE  17 240\n917     NE  18 240\n918     NE  19 240\n919     NE  20 240\n920     NE  21 240\n921     NE  22 240\n922     NE  23 240\n923     NE  24 240\n924     NE  25 240\n925     NE  26 240\n926     NE  27 240\n927     NE  28 240\n928     NE  29 225\n929     NE  30 220\n930     NE  31 140\n931     NH   1 240\n932     NH   2 240\n933     NH   3 240\n934     NH   4 240\n935     NH   5 240\n936     NH   6 240\n937     NH   7 240\n938     NH   8 240\n939     NH   9 240\n940     NH  10 240\n941     NH  11 240\n942     NH  12 240\n943     NH  13 240\n944     NH  14 240\n945     NH  15 240\n946     NH  16 240\n947     NH  17 240\n948     NH  18 240\n949     NH  19 240\n950     NH  20 240\n951     NH  21 240\n952     NH  22 240\n953     NH  23 240\n954     NH  24 240\n955     NH  25 240\n956     NH  26 240\n957     NH  27 240\n958     NH  28 240\n959     NH  29 226\n960     NH  30 221\n961     NH  31 141\n962     NJ   1 240\n963     NJ   2 240\n964     NJ   3 240\n965     NJ   4 240\n966     NJ   5 240\n967     NJ   6 240\n968     NJ   7 240\n969     NJ   8 240\n970     NJ   9 240\n971     NJ  10 240\n972     NJ  11 240\n973     NJ  12 240\n974     NJ  13 240\n975     NJ  14 240\n976     NJ  15 240\n977     NJ  16 240\n978     NJ  17 240\n979     NJ  18 240\n980     NJ  19 240\n981     NJ  20 240\n982     NJ  21 240\n983     NJ  22 240\n984     NJ  23 240\n985     NJ  24 240\n986     NJ  25 240\n987     NJ  26 240\n988     NJ  27 240\n989     NJ  28 240\n990     NJ  29 226\n991     NJ  30 220\n992     NJ  31 155\n993     NM   1 240\n994     NM   2 240\n995     NM   3 240\n996     NM   4 240\n997     NM   5 240\n998     NM   6 240\n999     NM   7 240\n1000    NM   8 240\n1001    NM   9 240\n1002    NM  10 240\n1003    NM  11 240\n1004    NM  12 240\n1005    NM  13 240\n1006    NM  14 240\n1007    NM  15 240\n1008    NM  16 240\n1009    NM  17 240\n1010    NM  18 240\n1011    NM  19 240\n1012    NM  20 240\n1013    NM  21 240\n1014    NM  22 240\n1015    NM  23 240\n1016    NM  24 240\n1017    NM  25 240\n1018    NM  26 240\n1019    NM  27 240\n1020    NM  28 240\n1021    NM  29 226\n1022    NM  30 220\n1023    NM  31 142\n1024    NV   1 240\n1025    NV   2 240\n1026    NV   3 240\n1027    NV   4 240\n1028    NV   5 240\n1029    NV   6 240\n1030    NV   7 240\n1031    NV   8 240\n1032    NV   9 240\n1033    NV  10 240\n1034    NV  11 240\n1035    NV  12 240\n1036    NV  13 240\n1037    NV  14 240\n1038    NV  15 240\n1039    NV  16 240\n1040    NV  17 240\n1041    NV  18 240\n1042    NV  19 240\n1043    NV  20 240\n1044    NV  21 240\n1045    NV  22 240\n1046    NV  23 240\n1047    NV  24 240\n1048    NV  25 240\n1049    NV  26 240\n1050    NV  27 240\n1051    NV  28 240\n1052    NV  29 226\n1053    NV  30 220\n1054    NV  31 141\n1055    NY   1 240\n1056    NY   2 240\n1057    NY   3 240\n1058    NY   4 240\n1059    NY   5 240\n1060    NY   6 240\n1061    NY   7 240\n1062    NY   8 240\n1063    NY   9 240\n1064    NY  10 240\n1065    NY  11 240\n1066    NY  12 240\n1067    NY  13 240\n1068    NY  14 240\n1069    NY  15 240\n1070    NY  16 240\n1071    NY  17 240\n1072    NY  18 240\n1073    NY  19 240\n1074    NY  20 240\n1075    NY  21 240\n1076    NY  22 240\n1077    NY  23 240\n1078    NY  24 240\n1079    NY  25 240\n1080    NY  26 240\n1081    NY  27 240\n1082    NY  28 240\n1083    NY  29 228\n1084    NY  30 222\n1085    NY  31 163\n1086    OH   1 240\n1087    OH   2 240\n1088    OH   3 240\n1089    OH   4 240\n1090    OH   5 240\n1091    OH   6 240\n1092    OH   7 240\n1093    OH   8 240\n1094    OH   9 240\n1095    OH  10 240\n1096    OH  11 240\n1097    OH  12 240\n1098    OH  13 240\n1099    OH  14 240\n1100    OH  15 240\n1101    OH  16 240\n1102    OH  17 240\n1103    OH  18 240\n1104    OH  19 240\n1105    OH  20 240\n1106    OH  21 240\n1107    OH  22 240\n1108    OH  23 240\n1109    OH  24 240\n1110    OH  25 240\n1111    OH  26 240\n1112    OH  27 240\n1113    OH  28 240\n1114    OH  29 228\n1115    OH  30 222\n1116    OH  31 149\n1117    OK   1 240\n1118    OK   2 240\n1119    OK   3 240\n1120    OK   4 240\n1121    OK   5 240\n1122    OK   6 240\n1123    OK   7 240\n1124    OK   8 240\n1125    OK   9 240\n1126    OK  10 240\n1127    OK  11 240\n1128    OK  12 240\n1129    OK  13 240\n1130    OK  14 240\n1131    OK  15 240\n1132    OK  16 240\n1133    OK  17 240\n1134    OK  18 240\n1135    OK  19 240\n1136    OK  20 240\n1137    OK  21 240\n1138    OK  22 240\n1139    OK  23 240\n1140    OK  24 240\n1141    OK  25 240\n1142    OK  26 240\n1143    OK  27 240\n1144    OK  28 240\n1145    OK  29 225\n1146    OK  30 220\n1147    OK  31 141\n1148    OR   1 240\n1149    OR   2 240\n1150    OR   3 240\n1151    OR   4 240\n1152    OR   5 240\n1153    OR   6 240\n1154    OR   7 240\n1155    OR   8 240\n1156    OR   9 240\n1157    OR  10 240\n1158    OR  11 240\n1159    OR  12 240\n1160    OR  13 240\n1161    OR  14 240\n1162    OR  15 240\n1163    OR  16 240\n1164    OR  17 240\n1165    OR  18 240\n1166    OR  19 240\n1167    OR  20 240\n1168    OR  21 240\n1169    OR  22 240\n1170    OR  23 240\n1171    OR  24 240\n1172    OR  25 240\n1173    OR  26 240\n1174    OR  27 240\n1175    OR  28 240\n1176    OR  29 225\n1177    OR  30 220\n1178    OR  31 142\n1179    PA   1 240\n1180    PA   2 240\n1181    PA   3 240\n1182    PA   4 240\n1183    PA   5 240\n1184    PA   6 240\n1185    PA   7 240\n1186    PA   8 240\n1187    PA   9 240\n1188    PA  10 240\n1189    PA  11 240\n1190    PA  12 240\n1191    PA  13 240\n1192    PA  14 240\n1193    PA  15 240\n1194    PA  16 240\n1195    PA  17 240\n1196    PA  18 240\n1197    PA  19 240\n1198    PA  20 240\n1199    PA  21 240\n1200    PA  22 240\n1201    PA  23 240\n1202    PA  24 240\n1203    PA  25 240\n1204    PA  26 240\n1205    PA  27 240\n1206    PA  28 240\n1207    PA  29 229\n1208    PA  30 222\n1209    PA  31 159\n1210    RI   1 240\n1211    RI   2 240\n1212    RI   3 240\n1213    RI   4 240\n1214    RI   5 240\n1215    RI   6 240\n1216    RI   7 240\n1217    RI   8 240\n1218    RI   9 240\n1219    RI  10 240\n1220    RI  11 240\n1221    RI  12 240\n1222    RI  13 240\n1223    RI  14 240\n1224    RI  15 240\n1225    RI  16 240\n1226    RI  17 240\n1227    RI  18 240\n1228    RI  19 240\n1229    RI  20 240\n1230    RI  21 240\n1231    RI  22 240\n1232    RI  23 240\n1233    RI  24 240\n1234    RI  25 240\n1235    RI  26 240\n1236    RI  27 240\n1237    RI  28 240\n1238    RI  29 225\n1239    RI  30 220\n1240    RI  31 140\n1241    SC   1 240\n1242    SC   2 240\n1243    SC   3 240\n1244    SC   4 240\n1245    SC   5 240\n1246    SC   6 240\n1247    SC   7 240\n1248    SC   8 240\n1249    SC   9 240\n1250    SC  10 240\n1251    SC  11 240\n1252    SC  12 240\n1253    SC  13 240\n1254    SC  14 240\n1255    SC  15 240\n1256    SC  16 240\n1257    SC  17 240\n1258    SC  18 240\n1259    SC  19 240\n1260    SC  20 240\n1261    SC  21 240\n1262    SC  22 240\n1263    SC  23 240\n1264    SC  24 240\n1265    SC  25 240\n1266    SC  26 240\n1267    SC  27 240\n1268    SC  28 240\n1269    SC  29 226\n1270    SC  30 222\n1271    SC  31 146\n1272    SD   1 240\n1273    SD   2 240\n1274    SD   3 240\n1275    SD   4 240\n1276    SD   5 240\n1277    SD   6 240\n1278    SD   7 240\n1279    SD   8 240\n1280    SD   9 240\n1281    SD  10 240\n1282    SD  11 240\n1283    SD  12 240\n1284    SD  13 240\n1285    SD  14 240\n1286    SD  15 240\n1287    SD  16 240\n1288    SD  17 240\n1289    SD  18 240\n1290    SD  19 240\n1291    SD  20 240\n1292    SD  21 240\n1293    SD  22 240\n1294    SD  23 240\n1295    SD  24 240\n1296    SD  25 240\n1297    SD  26 240\n1298    SD  27 240\n1299    SD  28 240\n1300    SD  29 225\n1301    SD  30 220\n1302    SD  31 140\n1303    TN   1 240\n1304    TN   2 240\n1305    TN   3 240\n1306    TN   4 240\n1307    TN   5 240\n1308    TN   6 240\n1309    TN   7 240\n1310    TN   8 240\n1311    TN   9 240\n1312    TN  10 240\n1313    TN  11 240\n1314    TN  12 240\n1315    TN  13 240\n1316    TN  14 240\n1317    TN  15 240\n1318    TN  16 240\n1319    TN  17 240\n1320    TN  18 240\n1321    TN  19 240\n1322    TN  20 240\n1323    TN  21 240\n1324    TN  22 240\n1325    TN  23 240\n1326    TN  24 240\n1327    TN  25 240\n1328    TN  26 240\n1329    TN  27 240\n1330    TN  28 240\n1331    TN  29 226\n1332    TN  30 221\n1333    TN  31 141\n1334    TX   1 240\n1335    TX   2 240\n1336    TX   3 240\n1337    TX   4 240\n1338    TX   5 240\n1339    TX   6 240\n1340    TX   7 240\n1341    TX   8 240\n1342    TX   9 240\n1343    TX  10 240\n1344    TX  11 240\n1345    TX  12 240\n1346    TX  13 240\n1347    TX  14 240\n1348    TX  15 240\n1349    TX  16 240\n1350    TX  17 240\n1351    TX  18 240\n1352    TX  19 240\n1353    TX  20 240\n1354    TX  21 240\n1355    TX  22 240\n1356    TX  23 240\n1357    TX  24 240\n1358    TX  25 240\n1359    TX  26 240\n1360    TX  27 240\n1361    TX  28 240\n1362    TX  29 227\n1363    TX  30 223\n1364    TX  31 160\n1365    UT   1 240\n1366    UT   2 240\n1367    UT   3 240\n1368    UT   4 240\n1369    UT   5 240\n1370    UT   6 240\n1371    UT   7 240\n1372    UT   8 240\n1373    UT   9 240\n1374    UT  10 240\n1375    UT  11 240\n1376    UT  12 240\n1377    UT  13 240\n1378    UT  14 240\n1379    UT  15 240\n1380    UT  16 240\n1381    UT  17 240\n1382    UT  18 240\n1383    UT  19 240\n1384    UT  20 240\n1385    UT  21 240\n1386    UT  22 240\n1387    UT  23 240\n1388    UT  24 240\n1389    UT  25 240\n1390    UT  26 240\n1391    UT  27 240\n1392    UT  28 240\n1393    UT  29 226\n1394    UT  30 220\n1395    UT  31 141\n1396    VA   1 240\n1397    VA   2 240\n1398    VA   3 240\n1399    VA   4 240\n1400    VA   5 240\n1401    VA   6 240\n1402    VA   7 240\n1403    VA   8 240\n1404    VA   9 240\n1405    VA  10 240\n1406    VA  11 240\n1407    VA  12 240\n1408    VA  13 240\n1409    VA  14 240\n1410    VA  15 240\n1411    VA  16 240\n1412    VA  17 240\n1413    VA  18 240\n1414    VA  19 240\n1415    VA  20 240\n1416    VA  21 240\n1417    VA  22 240\n1418    VA  23 240\n1419    VA  24 240\n1420    VA  25 240\n1421    VA  26 240\n1422    VA  27 240\n1423    VA  28 240\n1424    VA  29 225\n1425    VA  30 220\n1426    VA  31 145\n1427    VT   1 240\n1428    VT   2 240\n1429    VT   3 240\n1430    VT   4 240\n1431    VT   5 240\n1432    VT   6 240\n1433    VT   7 240\n1434    VT   8 240\n1435    VT   9 240\n1436    VT  10 240\n1437    VT  11 240\n1438    VT  12 240\n1439    VT  13 240\n1440    VT  14 240\n1441    VT  15 240\n1442    VT  16 240\n1443    VT  17 240\n1444    VT  18 240\n1445    VT  19 240\n1446    VT  20 240\n1447    VT  21 240\n1448    VT  22 240\n1449    VT  23 240\n1450    VT  24 240\n1451    VT  25 240\n1452    VT  26 240\n1453    VT  27 240\n1454    VT  28 240\n1455    VT  29 225\n1456    VT  30 220\n1457    VT  31 140\n1458    WA   1 240\n1459    WA   2 240\n1460    WA   3 240\n1461    WA   4 240\n1462    WA   5 240\n1463    WA   6 240\n1464    WA   7 240\n1465    WA   8 240\n1466    WA   9 240\n1467    WA  10 240\n1468    WA  11 240\n1469    WA  12 240\n1470    WA  13 240\n1471    WA  14 240\n1472    WA  15 240\n1473    WA  16 240\n1474    WA  17 240\n1475    WA  18 240\n1476    WA  19 240\n1477    WA  20 240\n1478    WA  21 240\n1479    WA  22 240\n1480    WA  23 240\n1481    WA  24 240\n1482    WA  25 240\n1483    WA  26 240\n1484    WA  27 240\n1485    WA  28 240\n1486    WA  29 225\n1487    WA  30 221\n1488    WA  31 140\n1489    WI   1 240\n1490    WI   2 240\n1491    WI   3 240\n1492    WI   4 240\n1493    WI   5 240\n1494    WI   6 240\n1495    WI   7 240\n1496    WI   8 240\n1497    WI   9 240\n1498    WI  10 240\n1499    WI  11 240\n1500    WI  12 240\n1501    WI  13 240\n1502    WI  14 240\n1503    WI  15 240\n1504    WI  16 240\n1505    WI  17 240\n1506    WI  18 240\n1507    WI  19 240\n1508    WI  20 240\n1509    WI  21 240\n1510    WI  22 240\n1511    WI  23 240\n1512    WI  24 240\n1513    WI  25 240\n1514    WI  26 240\n1515    WI  27 240\n1516    WI  28 240\n1517    WI  29 227\n1518    WI  30 221\n1519    WI  31 143\n1520    WV   1 240\n1521    WV   2 240\n1522    WV   3 240\n1523    WV   4 240\n1524    WV   5 240\n1525    WV   6 240\n1526    WV   7 240\n1527    WV   8 240\n1528    WV   9 240\n1529    WV  10 240\n1530    WV  11 240\n1531    WV  12 240\n1532    WV  13 240\n1533    WV  14 240\n1534    WV  15 240\n1535    WV  16 240\n1536    WV  17 240\n1537    WV  18 240\n1538    WV  19 240\n1539    WV  20 240\n1540    WV  21 240\n1541    WV  22 240\n1542    WV  23 240\n1543    WV  24 240\n1544    WV  25 240\n1545    WV  26 240\n1546    WV  27 240\n1547    WV  28 240\n1548    WV  29 227\n1549    WV  30 221\n1550    WV  31 142\n1551    WY   1 240\n1552    WY   2 240\n1553    WY   3 240\n1554    WY   4 240\n1555    WY   5 240\n1556    WY   6 240\n1557    WY   7 240\n1558    WY   8 240\n1559    WY   9 240\n1560    WY  10 240\n1561    WY  11 240\n1562    WY  12 240\n1563    WY  13 240\n1564    WY  14 240\n1565    WY  15 240\n1566    WY  16 240\n1567    WY  17 240\n1568    WY  18 240\n1569    WY  19 240\n1570    WY  20 240\n1571    WY  21 240\n1572    WY  22 240\n1573    WY  23 240\n1574    WY  24 240\n1575    WY  25 240\n1576    WY  26 240\n1577    WY  27 240\n1578    WY  28 240\n1579    WY  29 225\n1580    WY  30 220\n1581    WY  31 140\n\n# How many total births were there in this time period?\nBirthdays |&gt; \n  summarize(total_births = sum(births))\n\n  total_births\n1     70486538\n\n# How many total births were there per state in this time period, sorted from low to high?\nBirthdays |&gt; \n  group_by (state) |&gt;\n  summarize (total_births_state = sum(births)) |&gt; \n  arrange (total_births_state)\n\n# A tibble: 51 × 2\n   state total_births_state\n   &lt;chr&gt;              &lt;int&gt;\n 1 VT                147886\n 2 WY                154019\n 3 AK                185385\n 4 DE                188705\n 5 SD                235734\n 6 ND                238696\n 7 NV                241470\n 8 MT                253884\n 9 NH                264984\n10 RI                265038\n# ℹ 41 more rows\n\n\nExercise 6: Homework Reprise\nCreate a new dataset named daily_births that includes the total number of births per day (across all states) and the corresponding day of the week, eg, Mon. NOTE: Name the column with total births so that it’s easier to wrangle and plot.\n\ndaily_births &lt;- Birthdays |&gt; \n  group_by(date, wday)|&gt;\n  summarize (total_births = sum (births), .groups = \"drop\")\n\nUsing this data, construct a plot of births over time, indicating the day of week.\n\nggplot(daily_births, aes(x = date, y = total_births, color = wday)) + \n  geom_point()\n\n\n\n\nExercise 7: Wrangle & Plot\nFor each prompt below, you can decide whether you want to: (1) wrangle and store data, then plot; or (2) wrangle data and pipe directly into ggplot. For example:\n\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt; \n  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + \n    geom_point()\n\n\n\n\nPart a\nCalculate the total number of births in each month and year, eg, Jan 1969, Feb 1969, …. Label month by names not numbers, eg, Jan not 1. Then, plot the births by month and comment on what you learn.\n\nBirthdays |&gt;\n  group_by(month, year)|&gt;\n  summarize (total_births = sum(month))\n\n# A tibble: 240 × 3\n# Groups:   month [12]\n   month  year total_births\n   &lt;int&gt; &lt;int&gt;        &lt;int&gt;\n 1     1  1969         1581\n 2     1  1970         1581\n 3     1  1971         1581\n 4     1  1972         1581\n 5     1  1973         1581\n 6     1  1974         1581\n 7     1  1975         1581\n 8     1  1976         1581\n 9     1  1977         1581\n10     1  1978         1581\n# ℹ 230 more rows\n\n\nPart b\nIn 1988, calculate the total number of births per week in each state. Get rid of week “53”, which isn’t a complete week! Then, make a line plot of births by week for each state and comment on what you learn. For example, do you notice any seasonal trends? Are these the same in every state? Any outliers?\nPart c\nRepeat the above for just Minnesota (MN) and Louisiana (LA). MN has one of the coldest climates and LA has one of the warmest. How do their seasonal trends compare? Do you think these trends are similar in other colder and warmer states? Try it!\nExercise 8: More Practice\nPart a\nCreate a dataset with only births in Massachusetts (MA) in 1979 and sort the days from those with the most births to those with the fewest.\nPart b\nMake a table showing the five states with the most births between September 9, 1979 and September 12, 1979, including the 9th and 12th. Arrange the table in descending order of births."
  },
  {
    "objectID": "ica/ica-dates.html#footnotes",
    "href": "ica/ica-dates.html#footnotes",
    "title": "\n15  Dates\n",
    "section": "",
    "text": "The fivethirtyeight package has more recent data.↩︎"
  },
  {
    "objectID": "ica/ica-wrangling.html#motivation",
    "href": "ica/ica-wrangling.html#motivation",
    "title": "\n16  Wrangling\n",
    "section": "\n16.1 Motivation",
    "text": "16.1 Motivation\nRecall the elections data by U.S. county:\n\n# Load tidyverse & data\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nWe’ve used data viz to explore some general patterns in the election outcomes. For example, a map!\n\n# Get a background map\nlibrary(socviz)\ndata(county_map)\n\n# Make a choropleth map\nlibrary(RColorBrewer)  # For the color scale\nlibrary(ggthemes) # For theme_map\nelections |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips)) |&gt; \n  ggplot(aes(map_id = county_fips, fill = cut(repub_pct_20, breaks = seq(0, 100, by = 10)))) +\n    geom_map(map = county_map) +\n    scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n    expand_limits(x = county_map$long, y = county_map$lat)  + \n    theme_map() +\n    theme(legend.position = \"right\") + \n    coord_equal()\n\n\n\n\nConsider some fairly basic follow-up questions, each of which we cannot answer precisely (or sometimes even at all) using our data viz tools:\n\nHow many total people voted for the Democratic and Republican candidates in 2020?\nWhat about in each state?\nIn just the state of Minnesota:\n\nWhich counties had the highest and lowest Democratic vote in 2020?\nHow did the Democratic vote in each county change from 2016 to 2020?"
  },
  {
    "objectID": "ica/ica-wrangling.html#goals",
    "href": "ica/ica-wrangling.html#goals",
    "title": "\n16  Wrangling\n",
    "section": "\n16.2 Goals",
    "text": "16.2 Goals\nWe really cannot do anything with data (viz, modeling, etc) unless we can wrangle the data. The following is a typical quote. I agree with the 90% – data wrangling isn’t something we have to do before we can do data science, it is data science! But let’s rethink the 10% – data wrangling is a fun and empowering puzzle!\n\nThe goals of data wrangling are to explore how to:\n\nGet data into the tidy shape / format we need for analysis. For example, we might want to:\n\nkeep only certain observations\ndefine new variables\nreformat or “clean” existing variables\ncombine various datasets\nprocess “string” or text data\n\n\nNumerically (not just visually) explore and summarize various characteristics of the variables in our dataset."
  },
  {
    "objectID": "ica/ica-wrangling.html#tools",
    "href": "ica/ica-wrangling.html#tools",
    "title": "\n16  Wrangling\n",
    "section": "\n16.3 Tools",
    "text": "16.3 Tools\nWe’ll continue to use packages that are part of the tidyverse which share a common general grammar and structure."
  },
  {
    "objectID": "ica/ica-wrangling.html#warm-up",
    "href": "ica/ica-wrangling.html#warm-up",
    "title": "\n16  Wrangling\n",
    "section": "\n16.4 Warm-Up",
    "text": "16.4 Warm-Up\nThere are lots and lots of steps that can go into data wrangling, thus lots and lots of relevant R functions. BUT just 6 functions can get us very far. People refer to these as the 6 main wrangling verbs or functions:\n\nwhy “verbs”? in the tidyverse grammar, functions serve as action words\n\nthe 6 verbs are all stored in the dplyr package within the tidyverse\n\neach verb acts on a data frame and returns a data frame\n\n\n\nverb\naction\n\n\n\narrange\n\narrange the rows according to some column\n\n\n\nfilter\n\nfilter out or obtain a subset of the rows\n\n\n\nselect\n\nselect a subset of columns\n\n\n\nmutate\n\nmutate or create a column\n\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\n\ngroup_by\n\ngroup the rows by a specified column\n\n\n\n\n\n16.4.1 Example 1\nWhich verb would help us…\n\nkeep only information about state names, county names, and the 2020 and 2016 Democratic support (not the 2012 results, demographics, etc)\nget only the data on Minnesota\ndefine a new variable which calculates the change in Democratic support from 2016 to 2020, using dem_pct_20 and dem_pct_16\nsort the counties from highest to lowest Democratic support\ndetermine the total number of votes cast across all counties\n\n16.4.2 Example 2: Select Columns\nTo get a sense for the code structure, let’s explore a couple verbs together. To start, let’s simplify our dataset to include only some variables of interest. Specifically, select() only the columns relevant to state names, county names, and the 2020 and 2016 Democratic support:\n\n# What's the first argument? The second?\nselect(elections, c(state_name, county_name, dem_pct_20, dem_pct_16))\n\nLet’s re-do this with the pipe function |&gt;:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16)\n\n\n16.4.3 Example 3: Filter Rows\nLet’s filter() out only the rows related to Minnesota (MN):\n\n# Without a pipe\nfilter(elections, state_name == \"Minnesota\")\n\n\n# With a pipe\nelections |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n\n\n\n\n\n== vs =\n\n\n\nWe use a == b to check whether a matches b.\nWe use a = b to define that a is equal to b. We typically use = for this purpose inside a function, and &lt;- for this purpose outside a function.\n\n# Ex: \"=\" defines x\nx = 2\nx\n\n[1] 2\n\n\n\n# Ex: \"==\" checks whether x is/matches 3\nx == 3\n\n[1] FALSE\n\n\n\n\n\n16.4.4 Example 4: Filter and Select\nLet’s combine select() and filter() to create a new dataset with info about the county names, and 2020 and 2016 Democratic support among Minnesota counties.\n\n# Without pipes\nfilter(select(elections, c(state_name, county_name, dem_pct_20, dem_pct_16)), state_name == \"Minnesota\")\n\n\n# With pipes: all verbs in 1 row\nelections |&gt; select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; filter(state_name == \"Minnesota\")\n\n\n# With pipes: each verb in a new row\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n# We can even do this with UN-tidyverse code in \"base\" R\nelections[elections$state_name == \"Minnesota\", c(1, 4, 8, 12)]\n\n\n16.4.5 Example 5: Order of Operations\nSometimes, the order of operations matters, eg, putting on socks then shoes produces a different result than putting on shoes then socks. However, sometimes order doesn’t matter, eg, pouring cereal into a bowl then adding milk produces the same result as pouring milk into a bow then adding cereal (though one order is obviously better than the other ;)) Above (also copied below), we selected some columns and then filtered some rows:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\nWould we get the same result if we reversed select() and filter()? Think first, then try it.\n\n# Try it\nelections |&gt; \n  filter(state_name == \"Minnesota\") |&gt;\n  select(state_name, county_name, dem_pct_20, dem_pct_16)\n\n   state_name              county_name dem_pct_20 dem_pct_16\n1   Minnesota            Aitkin County      35.98      34.12\n2   Minnesota             Anoka County      47.79      41.01\n3   Minnesota            Becker County      33.96      30.47\n4   Minnesota          Beltrami County      47.24      40.76\n5   Minnesota            Benton County      32.70      28.33\n6   Minnesota         Big Stone County      35.41      33.75\n7   Minnesota        Blue Earth County      50.84      43.38\n8   Minnesota             Brown County      32.48      27.54\n9   Minnesota           Carlton County      49.58      46.85\n10  Minnesota            Carver County      46.37      39.03\n11  Minnesota              Cass County      34.68      31.16\n12  Minnesota          Chippewa County      33.67      32.00\n13  Minnesota           Chisago County      34.15      30.92\n14  Minnesota              Clay County      50.74      44.57\n15  Minnesota        Clearwater County      26.76      26.04\n16  Minnesota              Cook County      65.58      56.90\n17  Minnesota        Cottonwood County      30.03      29.45\n18  Minnesota         Crow Wing County      34.17      30.88\n19  Minnesota            Dakota County      55.73      48.22\n20  Minnesota             Dodge County      33.47      29.36\n21  Minnesota           Douglas County      32.56      28.80\n22  Minnesota         Faribault County      31.98      29.27\n23  Minnesota          Fillmore County      37.48      35.28\n24  Minnesota          Freeborn County      40.96      37.92\n25  Minnesota           Goodhue County      41.23      36.99\n26  Minnesota             Grant County      35.58      31.97\n27  Minnesota          Hennepin County      70.46      63.82\n28  Minnesota           Houston County      42.42      39.42\n29  Minnesota           Hubbard County      34.42      30.04\n30  Minnesota            Isanti County      29.45      27.09\n31  Minnesota            Itasca County      40.61      38.12\n32  Minnesota           Jackson County      29.99      27.36\n33  Minnesota           Kanabec County      30.02      28.64\n34  Minnesota         Kandiyohi County      36.12      33.56\n35  Minnesota           Kittson County      38.12      34.83\n36  Minnesota       Koochiching County      38.41      36.53\n37  Minnesota     Lac qui Parle County      35.79      33.92\n38  Minnesota              Lake County      50.64      47.54\n39  Minnesota Lake of the Woods County      27.87      24.80\n40  Minnesota          Le Sueur County      33.73      31.10\n41  Minnesota           Lincoln County      30.08      28.65\n42  Minnesota              Lyon County      35.94      31.54\n43  Minnesota            McLeod County      30.64      26.64\n44  Minnesota          Mahnomen County      48.26      44.84\n45  Minnesota          Marshall County      25.33      25.55\n46  Minnesota            Martin County      30.02      26.11\n47  Minnesota            Meeker County      28.58      26.17\n48  Minnesota        Mille Lacs County      29.98      28.65\n49  Minnesota          Morrison County      22.33      20.74\n50  Minnesota             Mower County      46.00      42.33\n51  Minnesota            Murray County      29.60      27.90\n52  Minnesota          Nicollet County      50.31      44.02\n53  Minnesota            Nobles County      33.65      31.81\n54  Minnesota            Norman County      40.80      39.11\n55  Minnesota           Olmsted County      54.16      45.75\n56  Minnesota        Otter Tail County      32.85      28.93\n57  Minnesota        Pennington County      35.29      32.17\n58  Minnesota              Pine County      33.87      33.36\n59  Minnesota         Pipestone County      26.44      23.58\n60  Minnesota              Polk County      34.88      32.06\n61  Minnesota              Pope County      35.27      33.46\n62  Minnesota            Ramsey County      71.50      65.73\n63  Minnesota          Red Lake County      31.47      28.86\n64  Minnesota           Redwood County      28.43      24.94\n65  Minnesota          Renville County      30.71      27.99\n66  Minnesota              Rice County      48.76      44.81\n67  Minnesota              Rock County      29.69      28.56\n68  Minnesota            Roseau County      25.98      23.90\n69  Minnesota         St. Louis County      56.64      51.92\n70  Minnesota             Scott County      45.52      38.31\n71  Minnesota         Sherburne County      32.48      27.74\n72  Minnesota            Sibley County      28.60      25.29\n73  Minnesota           Stearns County      37.58      32.38\n74  Minnesota            Steele County      37.47      32.77\n75  Minnesota           Stevens County      37.80      39.55\n76  Minnesota             Swift County      34.35      33.80\n77  Minnesota              Todd County      24.79      23.30\n78  Minnesota          Traverse County      35.46      35.23\n79  Minnesota           Wabasha County      35.78      32.86\n80  Minnesota            Wadena County      26.35      24.43\n81  Minnesota            Waseca County      33.65      29.63\n82  Minnesota        Washington County      53.46      46.96\n83  Minnesota          Watonwan County      38.20      36.49\n84  Minnesota            Wilkin County      29.91      27.23\n85  Minnesota            Winona County      49.07      43.97\n86  Minnesota            Wright County      34.49      29.41\n87  Minnesota   Yellow Medicine County      30.54      29.01\n\n\n\n16.4.6 Example 6: Storing Results\nTypically:\n\nWe want to store our data wrangling results.\nIt’s good practice to do so under a new name. We want to preserve, thus don’t want to overwrite, the original data (especially if our code contains errors!!).\n\n\n# Store the results\nmn &lt;- elections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n# Always check it out to confirm it's what you want it to be!\nhead(mn)\n\n  state_name      county_name dem_pct_20 dem_pct_16\n1  Minnesota    Aitkin County      35.98      34.12\n2  Minnesota     Anoka County      47.79      41.01\n3  Minnesota    Becker County      33.96      30.47\n4  Minnesota  Beltrami County      47.24      40.76\n5  Minnesota    Benton County      32.70      28.33\n6  Minnesota Big Stone County      35.41      33.75\n\nnrow(mn)\n\n[1] 87\n\nnrow(elections)\n\n[1] 3109"
  },
  {
    "objectID": "ica/ica-wrangling.html#exercises",
    "href": "ica/ica-wrangling.html#exercises",
    "title": "\n16  Wrangling\n",
    "section": "\n16.5 Exercises",
    "text": "16.5 Exercises\nExercise 1: select Practice\nUse select() to create a simplified dataset that we’ll use throughout the exercises below.\n\nStore this dataset as elections_small.\nOnly keep the following variables: state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16\n\n\n\n# Define elections_small\nelections_small &lt;- elections |&gt;\nselect(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\nExercise 2: filter Demo\nWhereas select() selects certain variables or columns, filter() keeps certain units of observation or rows relative to their outcome on certain variables. To this end, we must:\n\nIdentify the variable(s) that are relevant to the filter.\n\nUse a “logical comparison operator” to define which values of the variable to keep:\n\n\nsymbol\nmeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(???, ???)\na list of multiple values\n\n\n\n\nUse quotes \"\" when specifying outcomes of interest for a categorical variable.\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# What does this do?\nelections_small |&gt;\n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small |&gt; \n  filter (repub_pct_20 &gt; \"93.97\")\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row (observation) than your answer above\nelections_small |&gt; \n  filter (repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\nWe can also filter with respect to 2 rules! Here, think what variables are relevant.\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt;\n filter(state_name == \"Texas\") |&gt;\n filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt;\n filter(state_name == \"Texas\", dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\nExercise 3: arrange Demo\narrange() arranges or sorts the rows in a dataset according to a given column or variable, in ascending or descending order:\narrange(variable), arrange(desc(variable))\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(desc(repub_pct_20)) |&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\nExercise 4: mutate Demo\nmutate() can either transform / mutate an existing variable (column), or define a new variable based on existing ones.\nPart a\n\n# What did this code do? Added a new variable called \"diff_20\" at the end of the list of columns\nelections_small |&gt;\n  mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# What did this code do? Added a new column called \"repub_votes_20\"\nelections_small |&gt;\n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# What did this code do? Made another column based off of two other columns\nelections_small |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nPart b\n\n# You try\n# Define a variable that calculates the change in Dem support in 2020 vs 2016\nelections_small |&gt;\n mutate(change_dems = dem_pct_16 - dem_pct_20) |&gt;\n head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 change_dems\n1          24661      23.96       -3.06\n2          94090      19.57       -2.84\n3          10390      46.66        0.87\n4           8748      21.42        0.72\n5          25384       8.47       -1.10\n6           4701      75.09        0.39\n\n\n\n# You try\n# Define a variable that determines whether the Dem support was higher in 2020 than in 2016 (TRUE/FALSE)\nelections_small |&gt;\n mutate(dems_improved = dem_pct_20 &gt; dem_pct_16) |&gt;\n head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dems_improved\n1          24661      23.96          TRUE\n2          94090      19.57          TRUE\n3          10390      46.66         FALSE\n4           8748      21.42         FALSE\n5          25384       8.47          TRUE\n6           4701      75.09         FALSE\n\n\nExercise 5: Pipe Series\nLet’s now combine these verbs into a pipe series!\nPart a\nBEFORE running the below chunk, what do you think it will produce?\n\nelections_small |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart b\nBEFORE trying, what do you think will happen if you change the order of filter and arrange:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different I predict this one!\n\n\n\n# Now try it. Change the order of filter and arrange below.\nelections_small |&gt;\n  arrange(desc(total_votes_20)) |&gt;\n  filter(state_name == \"Wisconsin\",\n         repub_pct_20 &lt; dem_pct_20) |&gt;\n  head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\nPart c\nSo the order of filter() and arrange() did not matter – rerranging them produces the same results. BUT what is one advantage of filtering before arranging?\nPart d\nBEFORE running the below chunk, what do you think it will produce?\n\nelections_small |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nPart e\nBEFORE trying, what do you think will happen if you change the order of mutate and select:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different\n\n\n# Now try it. Change the order of mutate and select below.\nelections_small |&gt;\n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt;\n  filter(state_name == \"Delaware\") |&gt;\n  select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\nExercise 6: DIY Pipe Series\nWe’ve now learned 4 of the 6 wrangling verbs: select, filter, mutate, arrange. Let’s practice combining these into pipe series. Here are some hot tips:\n\nBefore writing any code, translate the prompt: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time – don’t try writing a whole chunk at once.\n\nPart a\nShow just the counties in Minnesota and their Democratic 2020 vote percentage, from highest to lowest. Your answer should have just 2 columns.\n\n#Step 1: just MN and dem_pct_20\nelections_small |&gt; \n  filter (state_name == \"Minnesota\") |&gt;\n  select (state_name, dem_pct_20)\n\n   state_name dem_pct_20\n1   Minnesota      35.98\n2   Minnesota      47.79\n3   Minnesota      33.96\n4   Minnesota      47.24\n5   Minnesota      32.70\n6   Minnesota      35.41\n7   Minnesota      50.84\n8   Minnesota      32.48\n9   Minnesota      49.58\n10  Minnesota      46.37\n11  Minnesota      34.68\n12  Minnesota      33.67\n13  Minnesota      34.15\n14  Minnesota      50.74\n15  Minnesota      26.76\n16  Minnesota      65.58\n17  Minnesota      30.03\n18  Minnesota      34.17\n19  Minnesota      55.73\n20  Minnesota      33.47\n21  Minnesota      32.56\n22  Minnesota      31.98\n23  Minnesota      37.48\n24  Minnesota      40.96\n25  Minnesota      41.23\n26  Minnesota      35.58\n27  Minnesota      70.46\n28  Minnesota      42.42\n29  Minnesota      34.42\n30  Minnesota      29.45\n31  Minnesota      40.61\n32  Minnesota      29.99\n33  Minnesota      30.02\n34  Minnesota      36.12\n35  Minnesota      38.12\n36  Minnesota      38.41\n37  Minnesota      35.79\n38  Minnesota      50.64\n39  Minnesota      27.87\n40  Minnesota      33.73\n41  Minnesota      30.08\n42  Minnesota      35.94\n43  Minnesota      30.64\n44  Minnesota      48.26\n45  Minnesota      25.33\n46  Minnesota      30.02\n47  Minnesota      28.58\n48  Minnesota      29.98\n49  Minnesota      22.33\n50  Minnesota      46.00\n51  Minnesota      29.60\n52  Minnesota      50.31\n53  Minnesota      33.65\n54  Minnesota      40.80\n55  Minnesota      54.16\n56  Minnesota      32.85\n57  Minnesota      35.29\n58  Minnesota      33.87\n59  Minnesota      26.44\n60  Minnesota      34.88\n61  Minnesota      35.27\n62  Minnesota      71.50\n63  Minnesota      31.47\n64  Minnesota      28.43\n65  Minnesota      30.71\n66  Minnesota      48.76\n67  Minnesota      29.69\n68  Minnesota      25.98\n69  Minnesota      56.64\n70  Minnesota      45.52\n71  Minnesota      32.48\n72  Minnesota      28.60\n73  Minnesota      37.58\n74  Minnesota      37.47\n75  Minnesota      37.80\n76  Minnesota      34.35\n77  Minnesota      24.79\n78  Minnesota      35.46\n79  Minnesota      35.78\n80  Minnesota      26.35\n81  Minnesota      33.65\n82  Minnesota      53.46\n83  Minnesota      38.20\n84  Minnesota      29.91\n85  Minnesota      49.07\n86  Minnesota      34.49\n87  Minnesota      30.54\n\n\n\n#Step 2: \"...from highest to lowest\"\nelections_small |&gt; \n  filter (state_name == \"Minnesota\") |&gt;\n  select (state_name, dem_pct_20) |&gt;\n  arrange (dem_pct_20)\n\n   state_name dem_pct_20\n1   Minnesota      22.33\n2   Minnesota      24.79\n3   Minnesota      25.33\n4   Minnesota      25.98\n5   Minnesota      26.35\n6   Minnesota      26.44\n7   Minnesota      26.76\n8   Minnesota      27.87\n9   Minnesota      28.43\n10  Minnesota      28.58\n11  Minnesota      28.60\n12  Minnesota      29.45\n13  Minnesota      29.60\n14  Minnesota      29.69\n15  Minnesota      29.91\n16  Minnesota      29.98\n17  Minnesota      29.99\n18  Minnesota      30.02\n19  Minnesota      30.02\n20  Minnesota      30.03\n21  Minnesota      30.08\n22  Minnesota      30.54\n23  Minnesota      30.64\n24  Minnesota      30.71\n25  Minnesota      31.47\n26  Minnesota      31.98\n27  Minnesota      32.48\n28  Minnesota      32.48\n29  Minnesota      32.56\n30  Minnesota      32.70\n31  Minnesota      32.85\n32  Minnesota      33.47\n33  Minnesota      33.65\n34  Minnesota      33.65\n35  Minnesota      33.67\n36  Minnesota      33.73\n37  Minnesota      33.87\n38  Minnesota      33.96\n39  Minnesota      34.15\n40  Minnesota      34.17\n41  Minnesota      34.35\n42  Minnesota      34.42\n43  Minnesota      34.49\n44  Minnesota      34.68\n45  Minnesota      34.88\n46  Minnesota      35.27\n47  Minnesota      35.29\n48  Minnesota      35.41\n49  Minnesota      35.46\n50  Minnesota      35.58\n51  Minnesota      35.78\n52  Minnesota      35.79\n53  Minnesota      35.94\n54  Minnesota      35.98\n55  Minnesota      36.12\n56  Minnesota      37.47\n57  Minnesota      37.48\n58  Minnesota      37.58\n59  Minnesota      37.80\n60  Minnesota      38.12\n61  Minnesota      38.20\n62  Minnesota      38.41\n63  Minnesota      40.61\n64  Minnesota      40.80\n65  Minnesota      40.96\n66  Minnesota      41.23\n67  Minnesota      42.42\n68  Minnesota      45.52\n69  Minnesota      46.00\n70  Minnesota      46.37\n71  Minnesota      47.24\n72  Minnesota      47.79\n73  Minnesota      48.26\n74  Minnesota      48.76\n75  Minnesota      49.07\n76  Minnesota      49.58\n77  Minnesota      50.31\n78  Minnesota      50.64\n79  Minnesota      50.74\n80  Minnesota      50.84\n81  Minnesota      53.46\n82  Minnesota      54.16\n83  Minnesota      55.73\n84  Minnesota      56.64\n85  Minnesota      65.58\n86  Minnesota      70.46\n87  Minnesota      71.50\n\n\nPart b\nCreate a new dataset named mn_wi that sorts the counties in Minnesota and Wisconsin from lowest to highest in terms of the change in Democratic vote percentage in 2020 vs 2016. This dataset should include the following variables (and only these variables): state_name, county_name, dem_pct_20, dem_pct_16, and a variable measuring the change in Democratic vote percentage in 2020 vs 2016.\n\n# Define the dataset\n# Only store the results once you're confident that they're correct\nmn_wi &lt;- \nelections_small |&gt;\n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) |&gt;\n  select (state_name, county_name, dem_pct_20, dem_pct_16)|&gt;\n  mutate(change_dems = dem_pct_20 - dem_pct_16) |&gt;\n  arrange(change_dems)\n\n# Check out the first 6 rows to confirm your results\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 change_dems\n1  Minnesota     Stevens County      37.80      39.55       -1.75\n2  Wisconsin      Forest County      34.06      35.12       -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73       -0.86\n4  Wisconsin       Clark County      30.37      31.19       -0.82\n5  Wisconsin       Adams County      36.63      37.40       -0.77\n6  Wisconsin Trempealeau County      40.86      41.57       -0.71\n\n\nPart c\nConstruct and discuss a plot of the county-level change in Democratic vote percent in 2020 vs 2016, and how this differs between Minnesota and Wisconsin.\n\nggplot(mn_wi, aes(x = change_dems, fill = state_name)) + \n  geom_density(alpha = 0.5)\n\n\n\n\nExercise 7: summarize Demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nLet’s talk about the last 2 verbs. summarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics. For each chunk below, indicate what the code does.\n\n# What does this do?\nelections_small |&gt;\n  summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n#Finds the median of the repub_pct_20\n\n\n# What does this do?\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n# Created a new term\n\n\n# What does this do?\nelections_small |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n# Multiple calculations in one\n\nExercise 8: summarize + group_by demo\nFinally, group_by() groups the units of observation or rows of a data frame by a specified set of variables. Alone, this function doesn’t change the appearance of our dataset or seem to do anything at all:\n\nelections_small |&gt;\n  group_by(state_name)\n\n# A tibble: 3,109 × 7\n# Groups:   state_name [50]\n   state_name county_name  total_votes_20 repub_pct_20 dem_pct_20 total_votes_16\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;\n 1 Alabama    Autauga Cou…          27770         71.4      27.0           24661\n 2 Alabama    Baldwin Cou…         109679         76.2      22.4           94090\n 3 Alabama    Barbour Cou…          10518         53.4      45.8           10390\n 4 Alabama    Bibb County            9595         78.4      20.7            8748\n 5 Alabama    Blount Coun…          27588         89.6       9.57          25384\n 6 Alabama    Bullock Cou…           4613         24.8      74.7            4701\n 7 Alabama    Butler Coun…           9488         57.5      41.8            8685\n 8 Alabama    Calhoun Cou…          50983         68.8      29.8           47376\n 9 Alabama    Chambers Co…          15284         57.3      41.6           13778\n10 Alabama    Cherokee Co…          12301         86.0      13.2           10503\n# ℹ 3,099 more rows\n# ℹ 1 more variable: dem_pct_16 &lt;dbl&gt;\n\n\nThough it does change the underlying structure of the dataset:\n\n# Check out the structure before and after group_by\nelections_small |&gt;\n  class()\n\n[1] \"data.frame\"\n\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  class()\n\n[1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWhere it really shines is in partnership with summarize().\n\n# What does this do?\n# (What if we didn't use group_by?)\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\nNotice that group_by() with summarize() produces new data frame or tibble! But the units of observation are now states instead of counties within states.\nExercise 9: DIY\nLet’s practice (some of) our 6 verbs: select, filter, arrange, mutate, summarize, group_by Remember:\n\nBefore writing any code, translate the given prompts: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time.\n\nPart a\nNOTE: Part a is a challenge exercise. If you get really stuck, move on to Part b which is the same overall question, but with hints.\n\n# Sort the *states* from the most to least total votes cast in 2020\nelections_small |&gt;\n  group_by(state_name) |&gt;\n  summarize(total = sum(total_votes_20)) |&gt;\n  arrange(desc(total)) |&gt;\n  head()\n\n# A tibble: 6 × 2\n  state_name      total\n  &lt;chr&gt;           &lt;int&gt;\n1 California   17495906\n2 Texas        11317911\n3 Florida      11067456\n4 New York      8616205\n5 Pennsylvania  6925255\n6 Illinois      6038850\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\n\n\n# What states did the Democratic candidate win in 2020?\n\nPart b\n\n# Sort the states from the most to least total votes cast in 2020\n# HINT: Calculate the total number of votes in each state, then sort\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each state?\n# HINT: First calculate the number of Dem and Repub votes in each *county*\n# Then group and summarize these by state\n\n\n# What states did the Democratic candidate win in 2020?\n# HINT: Start with the results from the previous chunk, and then keep only some rows\n\nExercise 10: Practice on New Data\nRecall the World Cup football/soccer data from TidyTuesday:\n\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nYou can find a codebook here. Use (some of) our 6 verbs (select, filter, arrange, mutate, summarize, group_by) and data viz to address the following prompts.\n\n# In what years did Brazil win the World Cup?\n\n\n# What were the 6 World Cups with the highest attendance?\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\n\nExercise 11: Practice on Your Data\nReturn to the TidyTuesday data you’re using in Homework 3. Use your new wrangling skills to play around. What new insights can you gain?!"
  },
  {
    "objectID": "ica/ica-wrangling.html#solutions",
    "href": "ica/ica-wrangling.html#solutions",
    "title": "\n16  Wrangling\n",
    "section": "\n16.6 Solutions",
    "text": "16.6 Solutions\n\nClick for Solutions\n\n16.6.1 Example 1\n\nselect\nfilter\nmutate\narrange\nsummarize\nExercise 1: select Practice\n\n# Define elections_small\nelections_small &lt;- elections |&gt;\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\nExercise 2: filter Demo\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# Keep counties in Hawaii AND Delaware\nelections_small |&gt; \n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\nelections_small |&gt; \n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row than your answer above\nelections_small |&gt; \n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt;\n filter(state_name == \"Texas\") |&gt;\n filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt;\n filter(state_name == \"Texas\", dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\nExercise 3: arrange Demo\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(desc(repub_pct_20)) |&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\nExercise 4: mutate Demo\n\n# Define diff_20, the difference btwn the Repub and Dem percent in 2020\nelections_small |&gt; \n  mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# Define repub_votes_20, the number (not percent) of Repub votes in 2020\nelections_small |&gt; \n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# Define repub_win_20, whether the Repub won in 2020 (TRUE or FALSE!)\nelections_small |&gt; \n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\nExercise 5: Pipe Series\nPart c\nIt’s more “computationally efficient” to get rid of some rows before arranging.\nPart e\nWe can’t select a variable before we define it!\nExercise 6: DIY Pipe Series\nPart a\nHere’s my translation:\n\njust the counties in Minnesota —&gt; filter\njust the counties in Minnesota and their Democratic 2020 vote percentage —&gt; select\nfrom highest to lowest —&gt; arrange\n\n\n# Remember to try this 1 line at a time\nelections_small |&gt; \n  filter(state_name == \"Minnesota\") |&gt; \n  select(county_name, dem_pct_20) |&gt; \n  arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n\nPart b\nHere’s my translation:\n\ncounties in Minnesota and Wisconsin —&gt; filter\nchange in Democratic vote percentage in 2020 vs 2016 —&gt; mutate (we don’t already have this)\nsorts the counties from highest to lowest —&gt; arrange\ninclude the following variables (and only these variables) —&gt; select\n\n\n# Remember to try this 1 line at a time before storing!\nmn_wi &lt;- elections_small |&gt; \n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt;\n  mutate(dem_change = dem_pct_20 - dem_pct_16) |&gt; \n  arrange(dem_change)\n  \n# Check it out\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 dem_change\n1  Minnesota     Stevens County      37.80      39.55      -1.75\n2  Wisconsin      Forest County      34.06      35.12      -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73      -0.86\n4  Wisconsin       Clark County      30.37      31.19      -0.82\n5  Wisconsin       Adams County      36.63      37.40      -0.77\n6  Wisconsin Trempealeau County      40.86      41.57      -0.71\n\n\nPart c\nThere was a stronger Dem shift from 2016 to 2020 in Minnesota. Further, in most counties across both states, the percent Dem tended to be higher in 2020 than in 2016.\n\nggplot(mn_wi, aes(x = dem_change, fill = state_name)) + \n  geom_density(alpha = 0.5)\n\n\n\nggplot(mn_wi, aes(y = dem_change, x = state_name)) + \n  geom_boxplot()\n\n\n\n\nExercise 7: summarize Demo\n\n# Calculate the median Repub vote percentage in 2020 across all counties\nelections_small |&gt; \n  summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n\n\n# Calculate the median Repub vote percentage in 2020 across all counties\n# AND name it \"median_repub\"\nelections_small |&gt; \n  summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n\n\n# Calculate the median Repub vote percentage in 2020 across all counties\n# AND the total number of votes across all counties\n# AND name the results\nelections_small |&gt; \n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n\nExercise 8: summarize + group_by demo\n\n# Calculate the median 2020 Repub percent and total votes BY STATE\nelections_small |&gt; \n  group_by(state_name) |&gt; \n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20)) \n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\nExercise 9: DIY\nPart a\n\n# Sort the states from the most to least total votes in 2020\nelections_small |&gt; \n  group_by(state_name) |&gt; \n  summarize(total = sum(total_votes_20)) |&gt; \n  arrange(desc(total))\n\n# A tibble: 50 × 2\n   state_name        total\n   &lt;chr&gt;             &lt;int&gt;\n 1 California     17495906\n 2 Texas          11317911\n 3 Florida        11067456\n 4 New York        8616205\n 5 Pennsylvania    6925255\n 6 Illinois        6038850\n 7 Ohio            5922202\n 8 Michigan        5539302\n 9 North Carolina  5524801\n10 Georgia         4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\nelections_small |&gt; \n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100), \n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt; \n  group_by(state_name) |&gt; \n  summarize(dem_total = sum(dem_votes_20),\n            repub_total = sum(repub_votes_20))\n\n# A tibble: 50 × 3\n   state_name           dem_total repub_total\n   &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                 849664     1441155\n 2 Arizona                1672127     1661671\n 3 Arkansas                423919      760641\n 4 California            11109642     6006031\n 5 Colorado               1804393     1364627\n 6 Connecticut            1080677      715315\n 7 Delaware                296274      200601\n 8 District of Columbia    317324       18595\n 9 Florida                5297131     5668600\n10 Georgia                2473661     2461869\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\nelections_small |&gt; \n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100), \n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt; \n  group_by(state_name) |&gt; \n  summarize(dem_total = sum(dem_votes_20),\n            repub_total = sum(repub_votes_20)) |&gt; \n  filter(dem_total &gt; repub_total)\n\n# A tibble: 26 × 3\n   state_name           dem_total repub_total\n   &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n 1 Arizona                1672127     1661671\n 2 California            11109642     6006031\n 3 Colorado               1804393     1364627\n 4 Connecticut            1080677      715315\n 5 Delaware                296274      200601\n 6 District of Columbia    317324       18595\n 7 Georgia                2473661     2461869\n 8 Hawaii                  366121      196865\n 9 Illinois               3471916     2446931\n10 Maine                   430466      359897\n# ℹ 16 more rows\n\n\nExercise 10: Practice on New Data\n\n# In what years did Brazil win the World Cup?\nworld_cup |&gt; \n  filter(winner == \"Brazil\")\n\n  year               host winner         second        third       fourth\n1 1958             Sweden Brazil         Sweden       France West Germany\n2 1962              Chile Brazil Czechoslovakia        Chile   Yugoslavia\n3 1970             Mexico Brazil          Italy West Germany      Uruguay\n4 1994                USA Brazil          Italy       Sweden     Bulgaria\n5 2002 Japan, South Korea Brazil        Germany       Turkey  South Korea\n  goals_scored teams games attendance\n1          126    16    35     868000\n2           89    16    32     776000\n3           95    16    32    1673975\n4          141    24    52    3568567\n5          161    32    64    2724604\n\n\n\n# What were the 6 World Cups with the highest attendance?\nworld_cup |&gt; \n  arrange(desc(attendance)) |&gt; \n  head()\n\n  year               host  winner    second       third      fourth\n1 1994                USA  Brazil     Italy      Sweden    Bulgaria\n2 2014             Brazil Germany Argentina Netherlands      Brazil\n3 2006            Germany   Italy    France     Germany    Portugal\n4 2018             Russia  France   Croatia     Belgium     England\n5 1998             France  France    Brazil     Croatia Netherlands\n6 2002 Japan, South Korea  Brazil   Germany      Turkey South Korea\n  goals_scored teams games attendance\n1          141    24    52    3568567\n2          171    32    64    3441450\n3          147    32    64    3367000\n4          169    32    64    3031768\n5          171    32    64    2859234\n6          161    32    64    2724604\n\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\nggplot(world_cup, aes(x = goals_scored)) + \n  geom_histogram(color = \"white\")\n\n\n\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\nworld_cup |&gt; \n  summarize(min(goals_scored), median(goals_scored), max(goals_scored))\n\n  min(goals_scored) median(goals_scored) max(goals_scored)\n1                70                  126               171\n\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\nggplot(world_cup, aes(x = year, y = goals_scored)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\nper_game_data &lt;- world_cup |&gt; \n  mutate(goals_per_game = goals_scored / games)\n\nggplot(per_game_data, aes(x = year, y = goals_per_game)) + \n  geom_point() + \n  geom_line()"
  },
  {
    "objectID": "ica/ica-reshape.html#review",
    "href": "ica/ica-reshape.html#review",
    "title": "\n17  Reshaping\n",
    "section": "\n17.1 Review",
    "text": "17.1 Review\nEXAMPLE 1: warm-up counts and proportions\nRecall the penguins we worked with last class:\n\nlibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\nTally up the number of male/female penguins by species in 2 ways:\n\n# Using count()\npenguins |&gt;\n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Using group_by() and summarize()\npenguins |&gt;\n  group_by(species, sex) |&gt;\n  summarize (n())\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    `n()`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\nDefine a new column that includes the proportion or relative frequencies of male/female penguins in each species.\n\nWe can’t do this by adjusting our count() code, but can adjust the group_by() and summarize() code since it’s still tracking the group categories in the background.\nDoes the order of species and sex in group_by() matter?\n\n\n#Defining a new column that includes proportion of male/female in each species.\npenguins |&gt;\n  group_by(species, sex) |&gt;\n  summarize (n = n()) |&gt;\n  #mutate(ratio = female / (female + male)) DOES NOT WORK instead try,\n  mutate(proportion = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   species [3]\n  species   sex        n proportion\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    female    73     0.480 \n2 Adelie    male      73     0.480 \n3 Adelie    &lt;NA&gt;       6     0.0395\n4 Chinstrap female    34     0.5   \n5 Chinstrap male      34     0.5   \n6 Gentoo    female    58     0.468 \n7 Gentoo    male      61     0.492 \n8 Gentoo    &lt;NA&gt;       5     0.0403\n\n\nEXAMPLE 2: New data\nWhat will the following code do? Think about it before running.\n\n# I think that this code will create a new data set called penguin_avg, and will group by the species and sex. Then, it will take the average body mass of each species, sorted by sex. NAs will be omitted.\npenguin_avg &lt;- penguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(avg_body_mass = mean(body_mass_g, na.rm = TRUE)) |&gt; \n  na.omit()\n\nEXAMPLE 3: units of observation\nTo get the information on average body masses, we reshaped our original data.\n\nDid the reshaping process change the units of observation?\n\n\n# Units of observation = 6x8\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Units of observation = 6x3\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\n\nDid the reshaping process result in any information loss from the original data? Yes, we lost some measurements such as bill length and depth."
  },
  {
    "objectID": "ica/ica-reshape.html#reshaping-data",
    "href": "ica/ica-reshape.html#reshaping-data",
    "title": "\n17  Reshaping\n",
    "section": "\n17.2 Reshaping Data",
    "text": "17.2 Reshaping Data\nThere are two general types of reshaped data:\n\naggregate data\nFor example, using group_by() with summarize() gains aggregate information about our observations but loses data on individual observations.\nraw data, reshaped\nWe often want to retain all information on individual observations, but need to reshape it in order to perform the task at hand.\n\nEXAMPLE 4: reshape it with your mind\nLet’s calculate the difference in average body mass, male vs female, for each species. Since penguin_avg is small, we could do these calculations by hand. But this doesn’t scale up to bigger datasets.\n\nSketch out (on paper, in your head, anything) how this data would need to be reshaped, without losing any information, in order to calculate the differences in average body mass using our wrangling verbs. Make it as specific as possible, with column labels, entries, correct numbers, etc.\nIdentify the units of observation.\n\n\n# It would lose the sex and average mass column and instead have just one other column titled difference between sexes or something like that. \npenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n# Units of observation would be 2x3\n\nWider vs Longer formats\nMaking our data longer or wider reshapes the data, changing the units of observation while retaining all raw information:\n\nMake the data longer, i.e. combine values from multiple variables into 1 variable. EXAMPLE: 1999 and 2000 represent two years. We want to combine their results into 1 variable without losing any information.\n\n\n\nMake the data wider, i.e. spread out the values across new variables. EXAMPLE: cases and pop represent two categories within type. To compare or combine their count outcomes side-by-side, we can separate them into their own variables.\n\n\nEXAMPLE 5: pivot wider\nBecause it’s a small enough dataset to examine all at once, let’s start with our penguin_avg data:\n\npenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nWith the goal of being able to calculate the difference in average body mass, male vs female, for each species, let’s make the dataset wider. That is, let’s get one row per species with separate columns for the average body mass by sex. Put this code into a chunk and run it:\n\npenguin_avg |&gt; \npivot_wider(names_from = sex, values_from = avg_body_mass)\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\n\n\nnames_from = the variable whose values we want to separate into their own columns, i.e. where we want to get the new column names from\n\n\nvalues_from = which variable to take the new column values from\n\n\nFOLLOW-UP:\n\nWhat are the units of observation?\nDid we lose any information when we widened the data? #we did not lose any information by widening.\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\n\n\npenguin_avg |&gt; \npivot_wider(names_from = sex, values_from = avg_body_mass) |&gt; \n  mutate(dif_body_mass = (male - female))\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male dif_body_mass\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Adelie     3369. 4043.          675.\n2 Chinstrap  3527. 3939.          412.\n3 Gentoo     4680. 5485.          805.\n\n\nEXAMPLE 6: Pivot longer\nLet’s store our wide data:\n\npenguin_avg_wide &lt;- penguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\npenguin_avg_wide\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nSuppose we wanted to change this data back to a longer format. In general, this happens when some variables (here female and male) represent two categories or values of some broader variable (here sex), and we want to combine them into that 1 variable without losing any information. Let’s pivot_longer():\n\n# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\n\n\ncols = the columns (variables) to collect into a single, new variable. We can also specify what variables we don’t want to collect\n\nnames_to = the name of the new variable which will include the names or labels of the collected variables\n\nvalues_to = the name of the new variable which will include the values of the collected variables\n\nFOLLOW-UP:\n\nWhat are the units of observation?\nDid we lose any information when we lengthened the data?\nWhy did we put the variables in quotes “” here but not when we used pivot_wider()?\nEXAMPLE 7: Practice\nLet’s make up some data on the orders of 2 different customers at 3 different restaurants:\n\nfood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\n\n\nThe units of observation in food are customer / restaurant combinations. Wrangle this data so that the units of observation are customers, spreading the restaurants into separate columns.\n\nfood |&gt;\n  pivot_wider(names_from = restaurant, values_from = order)\n\n# A tibble: 2 × 4\n  customer Shish   FrenchMeadow DunnBros\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;   \n1 A        falafel salad        coffee  \n2 B        baklava pastry       tea     \n\n\nConsider 2 more customers:\n\nmore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\n\n\nWrangle this data so that the 3 restaurant columns are combined into 1, hence the units of observation are customer / restaurant combinations.\n\nfood |&gt; \n  pivot_longer(cols = c(-customer), names_to = \"restaurant\", values_to = \"order\")\n\n# A tibble: 12 × 3\n   customer restaurant order       \n   &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;       \n 1 A        restaurant Shish       \n 2 A        order      falafel     \n 3 A        restaurant FrenchMeadow\n 4 A        order      salad       \n 5 A        restaurant DunnBros    \n 6 A        order      coffee      \n 7 B        restaurant Shish       \n 8 B        order      baklava     \n 9 B        restaurant FrenchMeadow\n10 B        order      pastry      \n11 B        restaurant DunnBros    \n12 B        order      tea"
  },
  {
    "objectID": "ica/ica-reshape.html#exercises",
    "href": "ica/ica-reshape.html#exercises",
    "title": "\n17  Reshaping\n",
    "section": "\n17.3 Exercises",
    "text": "17.3 Exercises\nExercise 1: What’s the problem?\nConsider data on a sleep study in which subjects received only 3 hours of sleep per night. Each day, their reaction time to a stimulus (in ms) was recorded.1\n\nsleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86\n\n\nPart a\nWhat are the units of observation in sleep_wide? The units of observation are subjects and their reaction time on different days.\nPart b\nSuppose I ask you to plot each subject’s reaction time (y-axis) vs the number of days of sleep restriction (x-axis). “Sketch” out in words what the first few rows of the data need to look like in order to do this. It might help to think about what you’d need to complete the plotting frame:\nggplot(___, aes(y = ___, x = ___, color = ___))\nWe would have 18 different lines, differentiated by color. On the x-axis, we we would have days of sleep, and on the y axis, we would have reaction time. This means that we have to reshape our data set so that we have 11 rows of data for each subject, and in one column we have the day and in the next column we have the reaction time.\nPart c\nHow can you obtain the dataset you sketched in part b?\npivot_longer()\nExercise 2: Pivot longer\nTo plot reaction time by day for each subject, we need to reshape the data into a long format where each row represents a subject/day combination. Specifically, we want a dataset with 3 columns and a first few rows that look something like this:\n\n\nSubject\nday\nreaction_time\n\n\n\n308\n0\n249.56\n\n\n308\n1\n258.70\n\n\n308\n2\n250.80\n\n\n\nPart a\nUse pivot_longer() to create the long-format dataset above. Show the first 3 lines (head(3)), which should be similar to those above. Follow-up: Thinking forward to plotting reaction time vs day for each subject, what would you like to fix / change about this dataset?\n\n# For cols, try 2 appproaches: using - and starts_with\n#cols minus\nsleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\n#cols included\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = starts_with(\"day\"), names_to = \"day\", values_to = \"reaction_time\")\n\n#Showing the first three lines\nhead(sleep_long, 3)\n\n# A tibble: 3 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 day_0          250.\n2     308 day_1          259.\n3     308 day_2          251.\n\n\nPart b\nRun this chunk:\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\")\n\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 0              250.\n2     308 1              259.\n3     308 2              251.\n4     308 3              321.\n5     308 4              357.\n6     308 5              415.\n\n\nFollow-up:\n\nBesides putting each argument on a different line for readability and storing the results, what changed in the code? Cleaned the code up, and took away the day_ before values in the day column\n\nHow did this impact how the values are recorded in the day column?\nPart c\nUsing sleep_long, construct a line plot of reaction time vs day for each subject. This will look goofy no matter what you do. Why? HINT: look back at head(sleep_long). What class or type of variables are Subject and day? What do we want them to be?\n\nsleep_long |&gt;\n  ggplot(aes (x = day, y = reaction_time, color = Subject)) + \n  geom_line() +\n  labs (\n    x = \"Day\",\n    y = \"Reaction Time\"\n  )\n\n\n\n\nExercise 3: Changing variable classes & plotting\nLet’s finalize sleep_long by mutating the Subject variable to be a factor (categorical) and the day variable to be numeric (quantitative). Take note of the mutate() code! You’ll use this type of code a lot.\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n# Check it out\n# Same data, different class\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n4 308         3          321.\n5 308         4          357.\n6 308         5          415.\n\n\nPart a\nNow make some plots.\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nsleep_long |&gt;\n  ggplot(aes (x = day, y = reaction_time, color = Subject)) + \n  geom_line() +\n  labs (\n    x = \"Day\",\n    y = \"Reaction Time\"\n  )\n\n\n\n\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nsleep_long |&gt;\n  ggplot(aes (x = day, y = reaction_time, color = Subject)) + \n  geom_line() +\n  facet_wrap(~Subject)\n\n\n\n  labs (\n    x = \"Day\",\n    y = \"Reaction Time\"\n  )\n\n$x\n[1] \"Day\"\n\n$y\n[1] \"Reaction Time\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nPart b\nSummarize what you learned from the plots. For example:\n\nWhat’s the general relationship between reaction time and sleep? General relationship seems to be that as days progressed, the subject’s reaction time after only three hours of sleep seem to improve.\n\nIs this the same for everybody? What differs?\n\n17.3.1 Exercise 4: Pivot wider\nMake the data wide again, with each day becoming its own column.\nPart a\nAdjust the code below. What don’t you like about the column labels?\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nPart b\nUsing your intuition, adjust your code from part a to name the reaction time columns “day_0”, “day_1”, etc.\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nExercise 5: Practice with Billboard charts\nLoad data on songs that hit the billboard charts around the year 2000. Included for each song is the artist name, track name, the date it hit the charts (date.enter), and wk-related variables that indicate rankings in each subsequent week on the charts:\n\n# Load data\nlibrary(tidyr)\ndata(\"billboard\")\n\n# Check it out\nhead(billboard)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nIn using this data, you’ll need to determine if and when the data needs to be reshaped for the task at hand.\nPart a\nConstruct and summarize a plot of how a song’s Billboard ranking its 2nd week on the chart (y-axis) is related to its ranking the 1st week on the charts (x-axis). Add a reference line geom_abline(intercept = 0, slope = 1). Songs above this line improved their rankings from the 1st to 2nd week.\nPart b\nUse your wrangling tools to identify which songs are those above the line in Part a, i.e. with rankgings that went up from week 1 to week 2.\nPart c\nDefine a new dataset, nov_1999, which:\n\nonly includes data on songs that entered the Billboard charts on November 6, 1999\nkeeps all variables except track and date.entered. HINT: How can you avoid writing out all the variable names you want to keep?\n\n\n# Define nov_1999\n\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\n\nPart d\nCreate and discuss a visualization of the rankings (y-axis) over time (x-axis) for the 2 songs in nov_1999. There are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\nHints:\n\nShould you first pivot wider or longer?\nOnce you pivot, the week number is turned into a character variable. How can you change it to a number?\nExercise 6: Practice with the Daily Show\nThe data associated with this article is available in the fivethirtyeight package, and is loaded into daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show, a “late-night talk and satirical news” program (per Wikipedia). Check out the dataset and note that when multiple people appeared together, each person receives their own line:\n\nlibrary(fivethirtyeight)\ndata(\"daily_show_guests\")\ndaily &lt;- daily_show_guests\n\nIn analyzing this data, you’ll need to determine if and when the data needs to be reshaped.\nPart a\nIdentify the 15 guests that appeared the most. (This isn’t a very diverse guest list!)\nPart b\nCHALLENGE: Create the following data set containing 19 columns:\n\nThe first column should have the 15 guests with the highest number of total appearances on the show, listed in descending order of number of appearances.\n17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column).\nAnother column should show the total number of appearances for the corresponding guest over the entire duration of the show.\n\nThere are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\nHINTS: There are lots of ways to do this. You don’t necessarily need all of these hints.\n\nFirst obtain the number of times a guest appears each year.\nAdd a new column which includes the total number of times a guest appears across all years.\nPivot (longer or wider?). When you do, use values_fill = 0 to replace NA values with 0.\nArrange, then and keep the top 15.\nPart c\nLet’s recreate the first figure from the article. This groups all guests into 3 broader occupational categories. However, our current data has 18 categories:\n\ndaily |&gt; \n  count(group)\n\n# A tibble: 18 × 2\n   group              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 Academic         103\n 2 Acting           930\n 3 Advocacy          24\n 4 Athletics         52\n 5 Business          25\n 6 Clergy             8\n 7 Comedy           150\n 8 Consultant        18\n 9 Government        40\n10 Media            751\n11 Military          16\n12 Misc              45\n13 Musician         123\n14 Political Aide    36\n15 Politician       308\n16 Science           28\n17 media              5\n18 &lt;NA&gt;              31\n\n\nLet’s define a new dataset that includes a new variable, broad_group, that buckets these 18 categories into the 3 bigger ones used in the article. And get rid of any rows missing information on broad_group. You’ll learn the code soon! For now, just run this chunk:\n\nplot_data &lt;- daily |&gt; \n  mutate(broad_group = case_when(\n    group %in% c(\"Acting\", \"Athletics\", \"Comedy\", \"Musician\") ~ \"Acting, Comedy & Music\",\n    group %in% c(\"Media\", \"media\", \"Science\", \"Academic\", \"Consultant\", \"Clergy\") ~ \"Media\",\n    group %in% c(\"Politician\", \"Political Aide\", \"Government\", \"Military\", \"Business\", \"Advocacy\") ~ \"Government and Politics\",\n    .default = NA\n  )) |&gt; \n  filter(!is.na(broad_group))\n\nNow, using the broad_group variable in plot_data, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Note: You’ll have to wrangle the data first."
  },
  {
    "objectID": "ica/ica-reshape.html#solutions",
    "href": "ica/ica-reshape.html#solutions",
    "title": "\n17  Reshaping\n",
    "section": "\n17.4 Solutions",
    "text": "17.4 Solutions\n\nClick for Solutions\nEXAMPLE 1: warm-up counts and proportions\n\n# Using count()\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Using group_by() and summarize()\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n())\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    `n()`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Relative frequencies\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   species [3]\n  species   sex        n proportion\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    female    73     0.480 \n2 Adelie    male      73     0.480 \n3 Adelie    &lt;NA&gt;       6     0.0395\n4 Chinstrap female    34     0.5   \n5 Chinstrap male      34     0.5   \n6 Gentoo    female    58     0.468 \n7 Gentoo    male      61     0.492 \n8 Gentoo    &lt;NA&gt;       5     0.0403\n\n# Changing the order calculates the proportion of species within each sex\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   sex [3]\n  sex    species       n proportion\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 female Adelie       73      0.442\n2 female Chinstrap    34      0.206\n3 female Gentoo       58      0.352\n4 male   Adelie       73      0.435\n5 male   Chinstrap    34      0.202\n6 male   Gentoo       61      0.363\n7 &lt;NA&gt;   Adelie        6      0.545\n8 &lt;NA&gt;   Gentoo        5      0.455\n\n\nEXAMPLE 3: units of observation\n\n# Units of observation = penguins\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Units of observation = species/sex combos\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nEXAMPLE 5: pivot wider\n\npenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species\nDid we lose any information when we widened the data? no\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\n\n\npenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass) |&gt; \n  mutate(diff = male - female)\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male  diff\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.  675.\n2 Chinstrap  3527. 3939.  412.\n3 Gentoo     4680. 5485.  805.\n\n\nEXAMPLE 6: Pivot longer\n\n# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species/sex combos\nDid we lose any information when we lengthened the data? no\n\n17.4.1 EXAMPLE 7: Practice [-]\n\nfood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\n\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\n\nfood |&gt; \n  pivot_wider(names_from = restaurant, values_from = order)\n\n# A tibble: 2 × 4\n  customer Shish   FrenchMeadow DunnBros\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;   \n1 A        falafel salad        coffee  \n2 B        baklava pastry       tea     \n\n\n\nmore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\n\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\n\nmore_food |&gt; \n  pivot_longer(cols = -customer, names_to = \"restaurant\", values_to = \"order\")\n\n# A tibble: 6 × 3\n  customer restaurant   order   \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   \n1 C        Shish        coffee  \n2 C        FrenchMeadow soup    \n3 C        DunnBros     cookie  \n4 D        Shish        maza    \n5 D        FrenchMeadow sandwich\n6 D        DunnBros     coffee  \n\n\nExercise 1: What’s the problem?\nPart a\nsubjects/people\nPart c\npivot_longer()\nExercise 2: Pivot longer\nPart a\n\n# For cols, try 2 appproaches: using - and starts_with\nsleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\nsleep_wide |&gt;\n  pivot_longer(cols = starts_with(\"day\"), names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\n\nPart b\nAdding names_prefix = \"day_\" removed “day_” from the start of the day entries. did this impact how the values are recorded in the day column?\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") \n\nPart c\nSubject is an integer and day is a character. We want them to be categorical (factor) and numeric, respectively.\n\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\nExercise 3: Changing variable classes & plotting\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\nPart a\nNow make some plots.\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line() + \n  facet_wrap(~ Subject)\n\n\n\n\nPart b\nReaction time increases (worsens) with a lack of sleep. Some subjects seem to be more impacted than others by lack of sleep, and some tend to have faster/slower reaction times in general.\nExercise 4: Pivot wider\nPart a\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nPart b\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\nExercise 5: Practice with Billboard charts\nPart a\nThe higher a song’s week 1 rating, the higher its week 2 rating tends to be. But almost all song’s rankings drop from week 1 to week 2.\n\nggplot(billboard, aes(y = wk2, x = wk1)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\nPart b\n\nbillboard |&gt; \n  filter(wk2 &gt; wk1)\n\n# A tibble: 7 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Carey, Mar… Cryb… 2000-06-24      28    34    48    62    77    90    95    NA\n2 Clark, Ter… A Li… 2000-12-16      75    82    88    96    99    99    NA    NA\n3 Diffie, Joe The … 2000-01-01      98   100   100    90    93    94    NA    NA\n4 Hart, Beth  L.A.… 1999-11-27      99   100    98    99    99    99    98    90\n5 Jay-Z       Hey … 2000-08-12      98   100    98    94    83    83    80    78\n6 Lil' Zane   Call… 2000-07-29      83    89    57    40    34    21    33    46\n7 Pearl Jam   Noth… 2000-05-13      49    70    84    89    93    91    NA    NA\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nPart c\n\n# Define nov_1999\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(-track, -date.entered)\n\n# Or\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(artist, starts_with(\"wk\"))\n\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\nPart c\n\nnov_1999 |&gt; \n  pivot_longer(cols = -artist, names_to = \"week\", names_prefix = \"wk\", values_to = \"ranking\") |&gt; \n  mutate(week = as.numeric(week)) |&gt; \n  ggplot(aes(y = ranking, x = week, color = artist)) + \n    geom_line()\n\n\n\n\nExercise 6: Practice with the Daily Show\nPart a\n\ndaily |&gt; \n  count(raw_guest_list) |&gt; \n  arrange(desc(n)) |&gt; \n  head(15)\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n\nPart b\n\ndaily |&gt; \n  count(year, raw_guest_list) |&gt; \n  group_by(raw_guest_list) |&gt; \n  mutate(total = sum(n)) |&gt;\n  pivot_wider(names_from = year, \n              values_from = n,\n              values_fill = 0) |&gt; \n  arrange(desc(total)) |&gt; \n  head(15)\n\n# A tibble: 15 × 19\n# Groups:   raw_guest_list [15]\n   raw_guest_list  total `1999` `2000` `2001` `2002` `2003` `2004` `2005` `2006`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria     19      0      0      1      0      1      2      2      2\n 2 Denis Leary        17      1      0      1      2      1      0      0      1\n 3 Brian Williams     16      0      0      0      0      1      1      2      1\n 4 Paul Rudd          13      1      0      1      1      1      1      1      0\n 5 Ricky Gervais      13      0      0      0      0      0      0      1      2\n 6 Tom Brokaw         12      0      0      0      1      0      2      1      0\n 7 Richard Lewis      10      1      0      2      2      1      1      0      0\n 8 Will Ferrell       10      0      1      1      0      1      1      1      1\n 9 Bill O'Reilly      10      0      0      1      1      0      1      1      0\n10 Reza Aslan         10      0      0      0      0      0      0      1      2\n11 Sarah Vowell        9      0      0      0      1      0      1      1      1\n12 Adam Sandler        8      1      2      0      1      0      0      0      1\n13 Ben Affleck         8      0      0      0      0      2      0      0      1\n14 Maggie Gyllenh…     8      0      0      0      0      1      0      1      1\n15 Louis C.K.          8      0      0      0      0      0      0      0      1\n# ℹ 9 more variables: `2007` &lt;int&gt;, `2008` &lt;int&gt;, `2009` &lt;int&gt;, `2010` &lt;int&gt;,\n#   `2011` &lt;int&gt;, `2012` &lt;int&gt;, `2013` &lt;int&gt;, `2014` &lt;int&gt;, `2015` &lt;int&gt;\n\n\nPart c\n\nplot_data |&gt;\n  group_by(year, broad_group) |&gt;\n  summarise(n = n()) |&gt;\n  mutate(freq = n / sum(n)) |&gt; \n  ggplot(aes(y = freq, x = year, color = broad_group)) + \n    geom_line()"
  },
  {
    "objectID": "ica/ica-reshape.html#footnotes",
    "href": "ica/ica-reshape.html#footnotes",
    "title": "\n17  Reshaping\n",
    "section": "",
    "text": "Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.↩︎"
  },
  {
    "objectID": "ica/ica-joining.html#review",
    "href": "ica/ica-joining.html#review",
    "title": "\n18  Joining\n",
    "section": "\n18.1 Review",
    "text": "18.1 Review\nWhere are we? Data preparation\n\nThus far, we’ve learned how to:\n\n\narrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\n\nmutate() existing variables and define new variables\n\nsummarize() various aspects of a variable, both overall and by group (group_by())\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())"
  },
  {
    "objectID": "ica/ica-joining.html#motivation",
    "href": "ica/ica-joining.html#motivation",
    "title": "\n18  Joining\n",
    "section": "\n18.2 Motivation",
    "text": "18.2 Motivation\nIn practice, we often have to collect and combine data from various sources in order to address our research questions. Example:\n\nWhat are the best predictors of album sales?\nCombine:\n\nSpotify data on individual songs (eg: popularity, genre, characteristics)\nsales data on individual songs\n\n\nWhat are the best predictors of flight delays?\nCombine:\n\ndata on individual flights including airline, starting airport, and destination airport\ndata on different airlines (eg: ticket prices, reliability, etc)\ndata on different airports (eg: location, reliability, etc)\n\n\n\nExample 1\nConsider the following (made up) data on students and course enrollments:\n\nstudents_1 &lt;- data.frame(\n  student = c(\"A\", \"B\", \"C\"),\n  class = c(\"STAT 101\", \"GEOL 101\", \"ANTH 101\")\n)\n\n# Check it out\nstudents_1\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n3       C ANTH 101\n\n\n\nenrollments_1 &lt;- data.frame(\n  class = c(\"STAT 101\", \"ART 101\", \"GEOL 101\"),\n  enrollment = c(18, 17, 24)\n)\n\n# Check it out\nenrollments_1\n\n     class enrollment\n1 STAT 101         18\n2  ART 101         17\n3 GEOL 101         24\n\n\nOur goal is to combine or join these datasets into one. For reference, here they are side by side:\n\nFirst, consider the following:\n\nWhat variable or key do these datasets have in common? Thus by what information can we match the observations in these datasets?\nRelative to this key, what info does students_1 have that enrollments_1 doesn’t?\nRelative to this key, what info does enrollments_1 have that students_1 doesn’t?\nExample 2\nThere are various ways to join these datasets:\n\nLet’s learn by doing. First, try the left_join() function:\n\nlibrary(tidyverse)\nstudents_1 |&gt; \n  left_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  left_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n\n\nExample 3\nNext, explore how our datasets are joined using inner_join():\n\n\nstudents_1 |&gt; \n  inner_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  inner_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2 GEOL 101         24       B\n\n\nExample 4\nNext, explore how our datasets are joined using full_join():\n\n\nstudents_1 |&gt; \n  full_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n4    &lt;NA&gt;  ART 101         17\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  full_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n4 ANTH 101         NA       C"
  },
  {
    "objectID": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "href": "ica/ica-joining.html#mutating-joins-left-inner-full",
    "title": "\n18  Joining\n",
    "section": "\n18.3 Mutating Joins: left, inner, full\n",
    "text": "18.3 Mutating Joins: left, inner, full\n\nMutating joins add new variables (columns) to the left data table from matching observations in the right table:\nleft_data |&gt; mutating_join(right_data)\nThe most common mutating joins are:\n\nleft_join()\nKeeps all observations from the left, but discards any observations in the right that do not have a match in the left.1\ninner_join()\nKeeps only the observations from the left with a match in the right.\nfull_join()\nKeeps all observations from the left and the right. (This is less common than left_join() and inner_join()).\n\nNOTE: When an observation in the left table has multiple matches in the right table, these mutating joins produce a separate observation in the new table for each match.\nExample 5\nMutating joins combine information, thus increase the number of columns in a dataset (like mutate()). Filtering joins keep only certain observations in one dataset (like filter()), not based on rules related to any variables in the dataset, but on the observations that exist in another dataset. This is useful when we merely care about the membership or non-membership of an observation in the other dataset, not the raw data itself.\nIn our example data, suppose enrollments_1 only included courses being taught in the Theater building:\n\n\nstudents_1 |&gt; \n  semi_join(enrollments_1)\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n\n\n\nWhat did this do? What info would it give us?\nHow does semi_join() differ from inner_join()? Inner join gives us the enrollment, too. Semi_join just gives us student and class.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Kicked out student, but kept enrollment.\n\n\nenrollments_1 |&gt; \n  semi_join(students_1)\n\n     class enrollment\n1 STAT 101         18\n2 GEOL 101         24\n\n\nExample 6\nLet’s try another filtering join for our example data:\n\n\nstudents_1 |&gt; \n  anti_join(enrollments_1)\n\n  student    class\n1       C ANTH 101\n\n\n\nWhat did this do? What info would it give us?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\nenrollments_1 |&gt; \n  anti_join(students_1)\n\n    class enrollment\n1 ART 101         17"
  },
  {
    "objectID": "ica/ica-joining.html#filtering-joins-semi-anti",
    "href": "ica/ica-joining.html#filtering-joins-semi-anti",
    "title": "\n18  Joining\n",
    "section": "\n18.4 Filtering Joins: semi, anti\n",
    "text": "18.4 Filtering Joins: semi, anti\n\nFiltering joins keep specific observations from the left table based on whether they match an observation in the right table.\n\nsemi_join()\nDiscards any observations in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join()\nDiscards any observations in the left table that do have a match in the right table."
  },
  {
    "objectID": "ica/ica-joining.html#summary-of-all-joins",
    "href": "ica/ica-joining.html#summary-of-all-joins",
    "title": "\n18  Joining\n",
    "section": "\n18.5 Summary of All Joins",
    "text": "18.5 Summary of All Joins"
  },
  {
    "objectID": "ica/ica-joining.html#exercises",
    "href": "ica/ica-joining.html#exercises",
    "title": "\n18  Joining\n",
    "section": "\n18.6 Exercises",
    "text": "18.6 Exercises\nExercise 1: Where are my keys?\nPart a\nDefine two new datasets, with different students and courses:\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\nstudents_2 |&gt; \n  left_join(enrollments_2)\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, by = c(\"class\" = \"course\"))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# by = c(\"left data key\" = \"right data key\")\n# The order is mixed up here, thus we get an error:\nstudents_2 |&gt; \n  left_join(enrollments_2, by = c(\"course\" = \"class\"))\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable or key. What are grade.x and grade.y?\n\nstudents_3 |&gt; \n  left_join(enrollments_3, by = c(\"class\" = \"class\"))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt; \n  anti_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt; \n  semi_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. We want any data available on each person\ncontact |&gt; \n  full_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\n# 4. When possible, we want to add contact info to the voting roster\nvoters |&gt;\n  left_join(contact, join_by(id == name))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\n\nsid = student ID\n\ngrade = student’s grade\n\nsessionID = an identifier of the class section\n\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\n\nsessionID = an identifier of the class section\n\ndept = department\n\nlevel = course level (eg: 100)\n\nsem = semester\n\nenroll = enrollment (number of students)\n\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\nExercise 4: Class size\nHow big are the classes?\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\ncourses_combined |&gt;\n  summarize(median(enroll))\n\n# A tibble: 1 × 1\n  `median(enroll)`\n             &lt;int&gt;\n1               18\n\n\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nstudent_class_size &lt;- grades |&gt;\n  left_join(courses_combined) |&gt; \n  group_by(sid) |&gt; \n  summarize(median_class_size = median(enroll))\n\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\n\nggplot(student_class_size, aes(x = median_class_size)) +\n  geom_histogram(color = \"blue\", fill = \"darkblue\")\n\n\n\n\nExercise 5: Narrowing in on classes\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\n\ngrades |&gt; \n  left_join(courses_combined) |&gt;\n  filter (sessionID == \"session1986\")\n\n     sid grade   sessionID enroll\n1 S31401    B+ session1986     10\n2 S32247     B session1986     10\n\n\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\n\ngrades |&gt;\n  semi_join(dept_E)\n\n      sid grade   sessionID\n1  S31245     A session2326\n2  S31470     B session3658\n3  S31470     B session3798\n4  S31470     A session3799\n5  S31938     A session2326\n6  S31968     A session3104\n7  S32022     A session3798\n8  S32046    A- session2326\n9  S32226     A session2326\n10 S32415     B session2835\n11 S32415    B+ session3799\n12 S32484    A- session3658\n\n\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\n\ncourses |&gt; \n  group_by(dept) |&gt; \n  summarise(total_enroll = sum(enroll)) |&gt;\n  arrange(desc(total_enroll))\n\n# A tibble: 40 × 2\n   dept  total_enroll\n   &lt;chr&gt;        &lt;int&gt;\n 1 d             3046\n 2 j             2312\n 3 O             2178\n 4 M             2129\n 5 m             2105\n 6 D             2003\n 7 W             1960\n 8 q             1859\n 9 k             1824\n10 F             1587\n# ℹ 30 more rows\n\n\nPart b\nWhat’s the grade-point average (GPA) for each student?\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt;\n  summarize(gpa = mean(gp, na.rm = TRUE))\n\n# A tibble: 443 × 2\n   sid      gpa\n   &lt;chr&gt;  &lt;dbl&gt;\n 1 S31185  2.41\n 2 S31188  3.02\n 3 S31191  3.21\n 4 S31194  3.36\n 5 S31197  3.35\n 6 S31200  2.2 \n 7 S31203  3.82\n 8 S31206  2.46\n 9 S31209  3.13\n10 S31212  3.67\n# ℹ 433 more rows\n\n\nPart c\nWhat’s the median GPA across all students?\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt;\n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt;\n  summarise(median(gpa))\n\n# A tibble: 1 × 1\n  `median(gpa)`\n          &lt;dbl&gt;\n1          3.47\n\n\nPart d\nWhat fraction of grades are below B+?\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  filter (gp &lt; 3.3) |&gt; \n  summarize (total_below = n()) \n\n  total_below\n1        1539\n\n  grades |&gt;\n  summarize (total_grades = n())\n\n  total_grades\n1         5844\n\n  grades |&gt; \n    summarize(fraction_below = 1539/5844)\n\n  fraction_below\n1       0.263347\n\n\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  left_join(courses) |&gt; \n  group_by(iid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\n# A tibble: 364 × 2\n   iid       gpa\n   &lt;chr&gt;   &lt;dbl&gt;\n 1 inst265  1.3 \n 2 inst444  1.7 \n 3 inst513  1.85\n 4 inst200  2   \n 5 inst507  2.2 \n 6 inst445  2.3 \n 7 inst420  2.6 \n 8 inst262  2.65\n 9 inst176  2.66\n10 inst234  2.7 \n# ℹ 354 more rows\n\n\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins."
  },
  {
    "objectID": "ica/ica-joining.html#footnotes",
    "href": "ica/ica-joining.html#footnotes",
    "title": "\n18  Joining\n",
    "section": "",
    "text": "There is also a right_join() that adds variables in the reverse direction from the left table to the right table, but we do not really need it as we can always switch the roles of the two tables.︎↩︎"
  },
  {
    "objectID": "ica/ica-factors.html#review",
    "href": "ica/ica-factors.html#review",
    "title": "\n19  Factors\n",
    "section": "\n19.1 Review",
    "text": "19.1 Review\nWhere are we? Data preparation\n\nThus far, we’ve learned how to:\n\ndo some wrangling:\n\n\narrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\n\nmutate() existing variables and define new variables\n\nsummarize() various aspects of a variable, both overall and by group (group_by())\n\n\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())\n\njoin() different datasets into one"
  },
  {
    "objectID": "ica/ica-factors.html#factors",
    "href": "ica/ica-factors.html#factors",
    "title": "\n19  Factors\n",
    "section": "\n19.2 Factors",
    "text": "19.2 Factors\nIn the remaining days of our data preparation unit, we’ll focus on working with special types of “categorical” variables: characters and factors. Variables with these structures often require special tools and considerations.\nWe’ll focus on two common considerations:\n\n\nRegular expressions\nWhen working with character strings, we might want to detect, replace, or extract certain patterns. For example, recall our data on courses:\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\n'data.frame':   1718 obs. of  6 variables:\n $ sessionID: chr  \"session1784\" \"session1785\" \"session1791\" \"session1792\" ...\n $ dept     : chr  \"M\" \"k\" \"J\" \"J\" ...\n $ level    : int  100 100 100 300 200 200 200 100 300 100 ...\n $ sem      : chr  \"FA1991\" \"FA1991\" \"FA1993\" \"FA1993\" ...\n $ enroll   : int  22 52 22 20 22 26 25 38 16 43 ...\n $ iid      : chr  \"inst265\" \"inst458\" \"inst223\" \"inst235\" ...\n\n\nFocusing on just the sem character variable, we might want to…\n\nchange FA to fall_ and SP to spring_\n\nkeep only courses taught in fall\nsplit the variable into 2 new variables: semester (FA or SP) and year\n\n\n\n\nConverting characters to factors (and factors to meaningful factors) (today)\nWhen categorical information is stored as a character variable, the categories of interest might not be labeled or ordered in a meaningful way. We can fix that!\n\nExample 1: Default Order\nRecall our data on presidential election outcomes in each U.S. county (except those in Alaska):\n\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\") |&gt; \n  select(state_abbr, historical, county_name, total_votes_20, repub_pct_20, dem_pct_20) |&gt; \n  mutate(dem_support_20 = case_when(\n    (repub_pct_20 - dem_pct_20 &gt;= 5) ~ \"low\",\n    (repub_pct_20 - dem_pct_20 &lt;= -5) ~ \"high\",\n    .default = \"medium\"\n  ))\n\n# Check it out\nhead(elections)  \n\n  state_abbr historical    county_name total_votes_20 repub_pct_20 dem_pct_20\n1         AL        red Autauga County          27770        71.44      27.02\n2         AL        red Baldwin County         109679        76.17      22.41\n3         AL        red Barbour County          10518        53.45      45.79\n4         AL        red    Bibb County           9595        78.43      20.70\n5         AL        red  Blount County          27588        89.57       9.57\n6         AL        red Bullock County           4613        24.84      74.70\n  dem_support_20\n1            low\n2            low\n3            low\n4            low\n5            low\n6           high\n\n\nCheck out the below visual and numerical summaries of dem_support_20:\n\nlow = the Republican won the county by at least 5 percentage points\nmedium = the Republican and Democrat votes were within 5 percentage points\nhigh = the Democrat won the county by at least 5 percentage points\n\n\nggplot(elections, aes(x = dem_support_20)) + \n  geom_bar()\n\n\n\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1           high  458\n2            low 2494\n3         medium  157\n\n\nFollow-up:\nWhat don’t you like about these results? I wish that it went high, medium, low instead of creating a misleading bell shape curve.\nExample 2: Change Order using fct_relevel\n\nThe above categories of dem_support_20 are listed alphabetically, which isn’t particularly meaningful here. This is because dem_support_20 is a character variable and R thinks of character strings as words, not category labels with any meaningful order (other than alphabetical):\n\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: chr  \"low\" \"low\" \"low\" \"low\" ...\n\n\nWe can fix this by using fct_relevel() to both:\n\nStore dem_support_20 as a factor variable, the levels of which are recognized as specific levels or categories, not just words.\nSpecify a meaningful order for the levels of the factor variable.\n\n\n# Notice that the order of the levels is not alphabetical!\nelections &lt;- elections |&gt; \n  mutate(dem_support_20 = fct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\")))\n\n# Notice the new structure of the dem_support_20 variable\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 3 1 1 1 1 ...\n\n\n\n# And plot dem_support_20\nggplot(elections, aes(x = dem_support_20)) +\n  geom_bar()\n\n\n\n\nExample 3: Change Labels using fct_recode\n\nWe now have a factor variable, dem_support_20, with categories that are ordered in a meaningful way:\n\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1            low 2494\n2         medium  157\n3           high  458\n\n\nBut maybe we want to change up the category labels. For demo purposes, let’s create a new factor variable, results_20, that’s the same as dem_support_20 but with different category labels:\n\n# We can redefine any number of the category labels.\n# Here we'll relabel all 3 categories:\nelections &lt;- elections |&gt; \n  mutate(results_20 = fct_recode(dem_support_20, \n                                 \"strong republican\" = \"low\",\n                                 \"close race\" = \"medium\",\n                                 \"strong democrat\" = \"high\"))\n\n# Check it out\n# Note that the new category labels are still in a meaningful,\n# not necessarily alphabetical, order!\nelections |&gt; \n  count(results_20)\n\n         results_20    n\n1 strong republican 2494\n2        close race  157\n3   strong democrat  458\n\n\nExample 4: Re-order Levels using fct_relevel\n\nFinally, let’s explore how the Republican vote varied from county to county within each state:\n\n# Note that we're just piping the data into ggplot instead of writing\n# it as the first argument\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, fill = state_abbr)) + \n    geom_density(alpha = 0.5)\n\n\n\n\nThis is too many density plots to put on top of one another. Let’s spread these out while keeping them in the same frame, hence easier to compare, using a joy plot or ridge plot:\n\nlibrary(ggridges)\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\nOK, but this is alphabetical. Suppose we want to reorder the states according to their typical Republican support. Recall that we did something similar in Example 2, using fct_relevel() to specify a meaningful order for the dem_support_20 categories:\nfct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\"))\nWe could use fct_relevel() to reorder the states here, but what would be the drawbacks?\nslowwwwww\nExample 5: Re-order levels Based on Another Variable using fct_reorder\n\nWhen a meaningful order for the categories of a factor variable can be defined by another variable in our dataset, we can use fct_reorder(). In our joy plot, let’s reorder the states according to their median Republican support:\n\n# Since we might want states to be alphabetical in other parts of our analysis,\n# we'll pipe the data into the ggplot without storing it:\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\")) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n# How did the code change?\n# And the corresponding output?\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\", .desc = TRUE)) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\nSummary\nThe forcats package, part of the tidyverse, includes handy functions for working with categorical variables (for + cats):\n\nHere are just some, few of which we explored above:\n\nfunctions for changing the order of factor levels\n\n\nfct_relevel() = manually reorder levels\n\nfct_reorder() = reorder levels according to values of another variable\n\n\nfct_infreq() = order levels from highest to lowest frequency\n\nfct_rev() = reverse the current order\n\n\nfunctions for changing the labels or values of factor levels\n\n\nfct_recode() = manually change levels\n\nfct_lump() = group together least common levels"
  },
  {
    "objectID": "ica/ica-factors.html#exercises",
    "href": "ica/ica-factors.html#exercises",
    "title": "\n19  Factors\n",
    "section": "\n19.3 Exercises",
    "text": "19.3 Exercises\nThe exercises revisit our grades data:\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nWe’ll explore the number of times each grade was assigned:\n\ngrade_distribution &lt;- grades |&gt; \n  count(grade)\n\nhead(grade_distribution)\n\n  grade    n\n1     A 1506\n2    A- 1381\n3    AU   27\n4     B  804\n5    B+ 1003\n6    B-  330\n\n\nExercise 1: Changing Order\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\ngrade_distribution |&gt; \n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\nExercise 2: Changing Factor Level Labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()"
  },
  {
    "objectID": "ica/ica-dataimport.html#review",
    "href": "ica/ica-dataimport.html#review",
    "title": "\n20  Data Import\n",
    "section": "\n20.1 Review",
    "text": "20.1 Review\nWHERE ARE WE?\nWe’ve thus far focused on data preparation and visualization:\n\nWhat’s coming up?\n\nIn the last few weeks we’ll focus on data storytelling through the completion of a course project.\nThis week we’ll address the other gaps in the workflow: data collection and analysis. We’ll do so in the context of starting a data project…"
  },
  {
    "objectID": "ica/ica-dataimport.html#data-import",
    "href": "ica/ica-dataimport.html#data-import",
    "title": "\n20  Data Import\n",
    "section": "\n20.2 Data Import",
    "text": "20.2 Data Import\nStarting Data Project\nAny data science project consists of two phases:\n\n\nData Collection\nA data project starts with data! Thus far, you’ve either been given data, or used TidyTuesday data. In this unit:\n\nWe WILL explore how to find data, save data, import this data into RStudio, and do some preliminary data cleaning.\nWe will NOT discuss how to collect data from scratch (e.g. via experiment, observational study, or survey).\n\n\n\nData Analysis\nOnce we have data, we need to do some analysis. In this unit…\n\nWe WILL bring together our wrangling & visualization tools to discuss exploratory data analysis (EDA). EDA is the process of getting to know our data, obtaining insights from data, and using these insights to formulate, refine, and explore research questions.\nWe will NOT explore other types of analysis, such as modeling & prediction–if interested, take STAT 155 and 253 to learn more about these topics.\n\n\n\nNOTE: These skills are best learned through practice. We’ll just scratch the surface here.\nFile Formats\nBefore exploring how to find, store, import, check, and clean data, it’s important to recognize that data can be stored in various formats. We’ve been working with .csv files. In the background, these have “comma-separated values” (csv):\n\nBut there are many other common file types. For example, the following are compatible with R:\n\nExcel files: .xls, .xlsx\n\nR “data serialization” files: .rds\n\nfiles with tab-separated values: .tsv\n\n\n\nSTEP 1: Find Data\nCheck the Datasets page for information about how to find a dataset that fits your needs.\nSTEP 2: Save Data Locally\nUnless we’re just doing a quick, one-off data analysis, it’s important to store a local copy of a data file, i.e. save the data file to our own machine.\nMainly, we shouldn’t rely on another person / institution to store a data file online, in the same place, forever!\nWhen saving your data, make sure they are\n\nin a nice format, eg, a csv file format\nwhere you’ll be able to find it again\nideally, within a folder that’s dedicated to the related project / assignment\nalongside the qmd file(s) where you’ll record your analysis of the data\nSTEP 3: Import Data to RStudio\nOnce we have a local copy of our data file, we need to get it into RStudio! This process depends on 2 things: (1) the file type (e.g. .csv); and (2) the file location, i.e. where it’s stored on your computer.\n1. FILE TYPE\nThe file type indicates which function we’ll need to import it. The table below lists some common import functions and when to use them.\n\n\n\n\n\n\nFunction\nData file type\n\n\n\nread_csv()\n.csv - you can save Excel files and Google Sheets as .csv\n\n\nread_delim()\nother delimited formats (tab, space, etc.)\n\n\nread_sheet()\nGoogle Sheet\n\n\nst_read()\nspatial data shapefile\n\n\n\nNOTE: In comparison to read.csv, read_csv is faster when importing large data files and can more easily parse complicated datasets, eg, with dates, times, percentages.\n2. FILE LOCATION\nTo import the data, we apply the appropriate function from above to the file path.\nA file path is an address to where the file is stored on our computer or the web.\nConsider “1600 Grand Ave, St. Paul, MN 55105”. Think about how different parts of the address give increasingly more specific information about the location. “St. Paul, MN 55105” tells us the city and smaller region within the city, “Grand Ave” tells us the street, and “1600” tells us the specific location on the street.\nIn the example below, the file path is absolute where it tells us the location giving more and more specific information as you read it from left to right.\n\n“~”, on an Apple computer, tells you that you are looking in the user’s home directory.\n“Desktop” tells you to go to the Desktop within that home directory.\n“112” tells you that you are looking in the 112 folder on the Desktop.\n“data” tells you to next go in the data folder in the 112 folder.\n“my_data.csv” tells you that you are looking for a file called my_data.csv location within the data folder.\n\n\nlibrary(tidyverse)\nmy_data &lt;- read_csv(\"~/Desktop/112/data/my_data.csv\")\n\nAbsolute file paths should only be used when reading files hosted on the web. Otherwise, relative file paths should be used. Relative file paths as the name suggest is relative to the file where the data file is read.\n\n# assume the code containing this script is under a folder called /src which\n# is at the same level of the /data folder\n\nlibrary(tidyverse)\nmy_data &lt;- read_csv(\"../data/my_data.csv\")\n\nSTEP 4: Check & Clean Data\nOnce the data is loaded, ask yourself a few questions:\nWhat’s the structure of the data?\n\nUse str() to learn about the numbers of variables and observations as well as the classes or types of variables (date, location, string, factor, number, boolean, etc.)\nUse head() to view the top of the data table\nUse View() to view the data in a spreadsheet-like viewer, use this command in the Console but don’t include it in your qmd files since it will prevent your project from rendering.\n\nIs there anything goofy that we need to clean before we can analyze the data?\n\nIs it in a tidy format?\nHow many rows are there? What does the row mean? What is an observation?\nIs there consistent formatting for categorical variables?\nIs there missing data that needs to be addressed?\nSTEP 5: Understand Data\nStart by understanding the data that is available to you. If you have a codebook, you have struck gold! If not (the more common case), you’ll need to do some detective work that often involves talking to people.\nAt this stage, ask yourself:\n\nWhere does my data come from? How was it collected?1\n\nIs there a codebook? If not, how can I learn more about it?\nAre there people I can reach out to who have experience with this data?"
  },
  {
    "objectID": "ica/ica-dataimport.html#exercises",
    "href": "ica/ica-dataimport.html#exercises",
    "title": "\n20  Data Import\n",
    "section": "\n20.3 Exercises",
    "text": "20.3 Exercises\nSuppose our goal is to work with data on movie reviews, and that we’ve already gone through the work to find a dataset. The imdb_5000_messy.csv file is posted on Moodle. Let’s work with it!\nExercise 1: Save Data Locally\nPart a\nOn your laptop:\n\nDownload the “imdb_5000_messy.csv” file from Moodle\nMove it to the data folder in your portfolio repository\nPart b\nHot tip: After saving your data file, it’s important to record appropriate citations and info in either a new qmd (eg: “imdb_5000_messy_README.qmd”) or in the qmd where you’ll analyze the data. These citations should include:\n\nthe data source, i.e. where you found the data\nthe data creator, i.e. who / what group collected the original data\npossibly a data codebook, i.e. descriptions of the data variables\n\nTo this end, check out where we originally got our IMDB data:\nhttps://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata\nAfter visiting that website, take some quick notes here on the data source and creator.\nThe source is kaggle and the creator is The Movie Database (TMDb) · chuan\nExercise 2: Import Data to RStudio\nNow that we have a local copy of our data file, let’s get it into RStudio! Remember that this process depends on 2 things: the file type and location. Since our file type is a csv, we can import it using read_csv(). But we have to supply the file location through a file path. To this end, we can either use an absolute file path or a relative file path.\nPart a\nAn absolute file path describes the location of a file starting from the root or home directory. How we refer to the user root directory depends upon your machine:\n\nOn a Mac: ~\n\nOn Windows: typically C:\\\n\n\nThen the complete file path to the IMDB data file in the data folder, depending on your machine an where you created your portfolio project, can be:\n\nOn a Mac: ~/Desktop/portfolio/data/imdb_5000_messy.csv\n\nOn Windows: C:\\Desktop\\portfolio\\data\\imdb_5000_messy.csv or C:\\\\Desktop\\\\portfolio\\\\data\\\\imdb_5000_messy.csv\n\n\nPutting this together, use read_csv() with the appropriate absolute file path to import your data into RStudio. Save this as imdb_messy.\n\nlibrary(tidyverse)\nimdb_messy &lt;- read_csv(\"~/Desktop/portfolio-F-Z-LoPresti/data/imdb_5000_messy.csv\")\n\nPart b\nAbsolute file paths can get really long, depending upon our number of sub-folders, and they should not be used when sharing code with other and instead relative file paths should be used. A relative file path describes the location of a file from the current “working directory”, i.e. where RStudio would currently look for on your computer. Check what your working directory is inside this qmd:\n\n# This should be the folder where you stored this qmd!\ngetwd()\n\n[1] \"/Users/fzlo/Desktop/portfolio-F-Z-LoPresti/ica\"\n\n\nNext, check what the working directory is for the console by typing getwd() in the console. This is probably different, meaning that the relative file paths that will work in your qmd won’t work in the console! You can either exclusively work inside your qmd, or change the working directory in your console, by navigating to the following in the upper toolbar: Session &gt; Set Working Directory &gt; To Source File location.\nPart c\nAs a good practice, we created a data folder and saved our data file (imdb_5000_messy.csv) into.\nSince our .qmd analysis and .csv data live in the same project, we don’t have to write out absolute file paths that go all the way to the root directory. We can use relative file paths that start from where our code file exists to where the data file exist:\n\nOn a Mac: ../data/imdb_5000_messy.csv\n\nOn Windows: ..\\data\\imdb_5000_messy.csv or ..\\\\data\\\\imdb_5000_messy.csv\n\n\nNOTE: .. means go up one level in the file hierarchy, ie, go to the parent folder/directory.\nPutting this together, use read_csv() with the appropriate relative file path to import your data into RStudio. Save this as imdb_temp (temp for “temporary”). Convince yourself that this worked, i.e. you get the same dataset as imdb_messy.\n\nimdb_temp &lt;- read_csv(\"../data/imdb_5000_messy.csv\")\n\nAbsolute file paths should be used when referring to files hosed on the web, eg, https://mac-stat.github.io/data/kiva_partners2.csv. In all other instances, relative file paths should be used.\nExercise 3: Check Data\nAfter importing new data into RStudio, you MUST do some quick checks of the data. Here are two first steps that are especially useful.\nPart a\nOpen imdb_messy in the spreadsheet-like viewer by typing View(imdb_messy) in the console. Sort this “spreadsheet” by different variables by clicking on the arrows next to the variable names. Do you notice anything unexpected?\nPart b\nDo a quick summary() of each variable in the dataset. One way to do this is below:\n\nimdb_messy |&gt;\n  mutate(across(where(is.character), as.factor)) |&gt;  # convert characters to factors in order to summarize\n  summary()\n\n      ...1                  color               director_name \n Min.   :   1   B&W            :  10   Steven Spielberg:  26  \n 1st Qu.:1262   Black and White: 199   Woody Allen     :  22  \n Median :2522   color          :  30   Clint Eastwood  :  20  \n Mean   :2522   Color          :4755   Martin Scorsese :  20  \n 3rd Qu.:3782   COLOR          :  30   Ridley Scott    :  17  \n Max.   :5043   NA's           :  19   (Other)         :4834  \n                                       NA's            : 104  \n num_critic_for_reviews    duration     director_facebook_likes\n Min.   :  1.0          Min.   :  7.0   Min.   :    0.0        \n 1st Qu.: 50.0          1st Qu.: 93.0   1st Qu.:    7.0        \n Median :110.0          Median :103.0   Median :   49.0        \n Mean   :140.2          Mean   :107.2   Mean   :  686.5        \n 3rd Qu.:195.0          3rd Qu.:118.0   3rd Qu.:  194.5        \n Max.   :813.0          Max.   :511.0   Max.   :23000.0        \n NA's   :50             NA's   :15      NA's   :104            \n actor_3_facebook_likes          actor_2_name  actor_1_facebook_likes\n Min.   :    0.0        Morgan Freeman :  20   Min.   :     0        \n 1st Qu.:  133.0        Charlize Theron:  15   1st Qu.:   614        \n Median :  371.5        Brad Pitt      :  14   Median :   988        \n Mean   :  645.0        James Franco   :  11   Mean   :  6560        \n 3rd Qu.:  636.0        Meryl Streep   :  11   3rd Qu.: 11000        \n Max.   :23000.0        (Other)        :4959   Max.   :640000        \n NA's   :23             NA's           :  13   NA's   :7             \n     gross                            genres             actor_1_name \n Min.   :      162   Drama               : 236   Robert De Niro:  49  \n 1st Qu.:  5340988   Comedy              : 209   Johnny Depp   :  41  \n Median : 25517500   Comedy|Drama        : 191   Nicolas Cage  :  33  \n Mean   : 48468408   Comedy|Drama|Romance: 187   J.K. Simmons  :  31  \n 3rd Qu.: 62309438   Comedy|Romance      : 158   Bruce Willis  :  30  \n Max.   :760505847   Drama|Romance       : 152   (Other)       :4852  \n NA's   :884         (Other)             :3910   NA's          :   7  \n                    movie_title   num_voted_users   cast_total_facebook_likes\n Ben-Hur                  :   3   Min.   :      5   Min.   :     0           \n Halloween                :   3   1st Qu.:   8594   1st Qu.:  1411           \n Home                     :   3   Median :  34359   Median :  3090           \n King Kong                :   3   Mean   :  83668   Mean   :  9699           \n Pan                      :   3   3rd Qu.:  96309   3rd Qu.: 13756           \n The Fast and the Furious :   3   Max.   :1689764   Max.   :656730           \n (Other)                  :5025                                              \n         actor_3_name  facenumber_in_poster\n Ben Mendelsohn:   8   Min.   : 0.000      \n John Heard    :   8   1st Qu.: 0.000      \n Steve Coogan  :   8   Median : 1.000      \n Anne Hathaway :   7   Mean   : 1.371      \n Jon Gries     :   7   3rd Qu.: 2.000      \n (Other)       :4982   Max.   :43.000      \n NA's          :  23   NA's   :13          \n                                                                           plot_keywords \n based on novel                                                                   :   4  \n 1940s|child hero|fantasy world|orphan|reference to peter pan                     :   3  \n alien friendship|alien invasion|australia|flying car|mother daughter relationship:   3  \n animal name in title|ape abducts a woman|gorilla|island|king kong                :   3  \n assistant|experiment|frankenstein|medical student|scientist                      :   3  \n (Other)                                                                          :4874  \n NA's                                                                             : 153  \n                                             movie_imdb_link\n http://www.imdb.com/title/tt0077651/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0232500/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt0360717/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt1976009/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2224026/?ref_=fn_tt_tt_1:   3  \n http://www.imdb.com/title/tt2638144/?ref_=fn_tt_tt_1:   3  \n (Other)                                             :5025  \n num_user_for_reviews     language       country       content_rating\n Min.   :   1.0       English :4704   USA    :3807   R        :2118  \n 1st Qu.:  65.0       French  :  73   UK     : 448   PG-13    :1461  \n Median : 156.0       Spanish :  40   France : 154   PG       : 701  \n Mean   : 272.8       Hindi   :  28   Canada : 126   Not Rated: 116  \n 3rd Qu.: 326.0       Mandarin:  26   Germany:  97   G        : 112  \n Max.   :5060.0       (Other) : 160   (Other): 406   (Other)  : 232  \n NA's   :21           NA's    :  12   NA's   :   5   NA's     : 303  \n     budget            title_year   actor_2_facebook_likes   imdb_score   \n Min.   :2.180e+02   Min.   :1916   Min.   :     0         Min.   :1.600  \n 1st Qu.:6.000e+06   1st Qu.:1999   1st Qu.:   281         1st Qu.:5.800  \n Median :2.000e+07   Median :2005   Median :   595         Median :6.600  \n Mean   :3.975e+07   Mean   :2002   Mean   :  1652         Mean   :6.442  \n 3rd Qu.:4.500e+07   3rd Qu.:2011   3rd Qu.:   918         3rd Qu.:7.200  \n Max.   :1.222e+10   Max.   :2016   Max.   :137000         Max.   :9.500  \n NA's   :492         NA's   :108    NA's   :13                            \n  aspect_ratio   movie_facebook_likes\n Min.   : 1.18   Min.   :     0      \n 1st Qu.: 1.85   1st Qu.:     0      \n Median : 2.35   Median :   166      \n Mean   : 2.22   Mean   :  7526      \n 3rd Qu.: 2.35   3rd Qu.:  3000      \n Max.   :16.00   Max.   :349000      \n NA's   :329                         \n\n\nFollow-up:\n\nWhat type of info is provided on quantitative variables?\nWhat type of info is provided on categorical variables?\nWhat stands out to you in these summaries? Is there anything you’d need to clean before using this data?\nExercise 4: Clean Data: Factor Variables 1\nIf you didn’t already in Exercise 3, check out the color variable in the imdb_messy dataset.\n\nWhat’s goofy about this / what do we need to fix?\nMore specifically, what different categories does the color variable take, and how many movies fall into each of these categories?\n\n\nimdb_messy |&gt; \ncount(color)\n\n# A tibble: 6 × 2\n  color               n\n  &lt;chr&gt;           &lt;int&gt;\n1 B&W                10\n2 Black and White   199\n3 COLOR              30\n4 Color            4755\n5 color              30\n6 &lt;NA&gt;               19\n\n\nExercise 5: Clean Data: Factor Variables 2\nWhen working with categorical variables like color, the categories must be “clean”, i.e. consistent and in the correct format. Let’s make that happen.\nPart a\nWe could open the .csv file in, say, Excel or Google sheets, clean up the color variable, save a clean copy, and then reimport that into RStudio. BUT that would be the wrong thing to do. Why is it important to use R code, which we then save inside this qmd, to clean our data?\nPart b\nLet’s use R code to change the color variable so that it appropriately combines the various categories into only 2: Color and Black_White. We’ve learned a couple sets of string-related tools that could be handy here. First, starting with the imdb_messy data, change the color variable using one of the functions we learned in the Factors lesson.\nfct_relevel(), fct_recode(), fct_reorder()\nStore your results in imdb_temp (don’t overwrite imdb_messy). To check your work, print out a count() table of the color variable in imdb_temp.\n\nimdb_temp &lt;- imdb_messy |&gt;\n  mutate(color = fct_recode(color,\n                            \"Color\" = \"COLOR\",\n                            \"Color\" = \"color\",\n                            \"Black_White\" = \"B&W\",\n                            \"Black_White\" = \"Black and White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;fct&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nPart c\nRepeat Part b using one of our string functions from the String lesson:\nstr_replace(), str_replace_all(), str_to_lower(), str_sub(), str_length(), str_detect()\n\nimdb_temp &lt;- imdb_messy |&gt;\n  mutate(color = str_replace(color,\"COLOR\", \"Color\"), \n         color = str_replace(color, \"color\", \"Color\"), \n         color = str_replace(color, \"B&W\", \"Black_White\"), \n         color = str_replace(color, \"Black and White\", \"Black_White\"))\n\nimdb_temp |&gt; \n  count(color)\n\n# A tibble: 3 × 2\n  color           n\n  &lt;chr&gt;       &lt;int&gt;\n1 Black_White   209\n2 Color        4815\n3 &lt;NA&gt;           19\n\n\nExercise 6: Clean Data: Missing Data 1\nThroughout these exercises, you’ve probably noticed that there’s a bunch of missing data. This is encoded as NA (not available) in R. There are a few questions to address about missing data:\n\n\nHow many values are missing data? What’s the volume of the missingness?\n\nWhy are some values missing?\n\nWhat should we do about the missing values?\n\nLet’s consider the first 2 questions in this exercise.\nPart a\nAs a first step, let’s simply understand the volume of NAs. Specifically:\n\n# Count the total number of rows in imdb_messy\nnrow(imdb_messy)\n\n[1] 5043\n\n# Then count the number of NAs in each column\ncolSums(is.na(imdb_messy))\n\n                     ...1                     color             director_name \n                        0                        19                       104 \n   num_critic_for_reviews                  duration   director_facebook_likes \n                       50                        15                       104 \n   actor_3_facebook_likes              actor_2_name    actor_1_facebook_likes \n                       23                        13                         7 \n                    gross                    genres              actor_1_name \n                      884                         0                         7 \n              movie_title           num_voted_users cast_total_facebook_likes \n                        0                         0                         0 \n             actor_3_name      facenumber_in_poster             plot_keywords \n                       23                        13                       153 \n          movie_imdb_link      num_user_for_reviews                  language \n                        0                        21                        12 \n                  country            content_rating                    budget \n                        5                       303                       492 \n               title_year    actor_2_facebook_likes                imdb_score \n                      108                        13                         0 \n             aspect_ratio      movie_facebook_likes \n                      329                         0 \n\n# Then count the number of NAs in a specific column\nimdb_messy |&gt; \n  summarize(sum(is.na(actor_1_facebook_likes)))\n\n# A tibble: 1 × 1\n  `sum(is.na(actor_1_facebook_likes))`\n                                 &lt;int&gt;\n1                                    7\n\n\nPart b\nAs a second step, let’s think about why some values are missing. Study the individual observations with NAs carefully. Why do you think they are missing? Are certain films more likely to have more NAs than others?\nPart c\nConsider a more specific example. Obtain a dataset of movies that are missing data on actor_1_facebook_likes. Then explain why you think there are NAs. HINT: is.na(___)\n\nimdb_messy |&gt; \n  filter(is.na(actor_1_facebook_likes))\n\n# A tibble: 7 × 29\n   ...1 color director_name     num_critic_for_reviews duration\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                              &lt;dbl&gt;    &lt;dbl&gt;\n1  4503 Color Léa Pool                              23       97\n2  4520 Color Harry Gantz                           12      105\n3  4721 Color U. Roberto Romano                      3       80\n4  4838 Color Pan Nalin                             15      102\n5  4946 Color Amal Al-Agroobi                       NA       62\n6  4947 Color Andrew Berends                        12       90\n7  4991 Color Jem Cohen                             12      111\n# ℹ 24 more variables: director_facebook_likes &lt;dbl&gt;,\n#   actor_3_facebook_likes &lt;dbl&gt;, actor_2_name &lt;chr&gt;,\n#   actor_1_facebook_likes &lt;dbl&gt;, gross &lt;dbl&gt;, genres &lt;chr&gt;,\n#   actor_1_name &lt;chr&gt;, movie_title &lt;chr&gt;, num_voted_users &lt;dbl&gt;,\n#   cast_total_facebook_likes &lt;dbl&gt;, actor_3_name &lt;chr&gt;,\n#   facenumber_in_poster &lt;dbl&gt;, plot_keywords &lt;chr&gt;, movie_imdb_link &lt;chr&gt;,\n#   num_user_for_reviews &lt;dbl&gt;, language &lt;chr&gt;, country &lt;chr&gt;, …\n\n\nExercise 7: Clean Data: Missing Data 2\nNext, let’s think about what to do about the missing values. There is no perfect or universal approach here. Rather, we must think carefully about…\n\nWhy the values are missing?\nWhat we want to do with our data?\nWhat is the impact of removing or replacing missing data on our work / conclusions?\n\nPart a\nCalculate the average duration of a film. THINK: How can we deal with the NA’s?\n\nimdb_messy |&gt; \n  summarize(mean(duration, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(duration, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                           107.\n\n#the difference is that this is an option for the function rather than !is.na is a function\n\nFollow-up:\nHow are the NAs dealt with here? Did we have to create and save a new dataset in order to do this analysis?\nPart b\nTry out the drop_na() function:\n\nimdb_temp &lt;- drop_na(imdb_messy)\n\nFollow-up questions:\n\nWhat did drop_na() do? How many data points are left?\nIn what situations might this function be a good idea?\nIn what situations might this function be a bad idea?\nPart c\ndrop_na() removes data points that have any NA values, even if we don’t care about the variable(s) for which data is missing. This can result in losing a lot of data points that do have data on the variables we actually care about! For example, suppose we only want to explore the relationship between film duration and whether it’s in color. Check out a plot:\n\nggplot(imdb_messy, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\nFollow-up:\n\nCreate a new dataset with only and all movies that have complete info on duration and color. HINT: You could use !is.na(___) or drop_na() (differently than above)\nUse this new dataset to create a new and improved plot.\nHow many movies remain in your new dataset? Hence why this is better than using the dataset from part b?\n\n\nimdb_temp &lt;- imdb_messy|&gt;\n  select(duration, color) |&gt;\n  drop_na()\n  \nggplot(imdb_temp, aes(x = duration, fill = color)) +\n  geom_density()\n\n\n\n\nPart d\nIn some cases, missing data is more non-data than unknown data. For example, the films with NAs for actor_1_facebook_likes actually have 0 Facebook likes–they don’t even have actors! In these cases, we can replace the NAs with a 0. Use the replace_na() function to create a new dataset (imdb_temp) that replaces the NAs in actor_1_facebook_likes with 0. You’ll have to check out the help file for this function.\n\nimdb_temp &lt;- imdb_messy|&gt;\n  mutate(actor_1_facebook_likes = \n           replace_na(actor_1_facebook_likes, 0))\n\nExercise 8: New Data + Projects\nLet’s practice the above ideas while also planting some seeds for the course project. Each group will pick and analyze their own dataset. The people you’re sitting with today aren’t necessarily your project groups! BUT do some brainstorming together:\n\nShare with each other: What are some personal hobbies or passions or things you’ve been thinking about or things you’d like to learn more about? Don’t think too hard about this! Just share what’s at the top of mind today.\nEach individual: Find a dataset online that’s related to one of the topics you shared in the above prompt.\nDiscuss what data you found with your group!\nLoad the data into RStudio, perform some basic checks, and perform some preliminary cleaning, as necessary."
  },
  {
    "objectID": "ica/ica-dataimport.html#footnotes",
    "href": "ica/ica-dataimport.html#footnotes",
    "title": "\n20  Data Import\n",
    "section": "",
    "text": "Particularly important questions about how it was collected include who: Is it a sample of a larger data set? If so, how the sampling was done? Randomly? All cases during a specific time frame? All data for a selected set of users?, when: Is this current data? Historical? What events may have had an impact?, what: What variables were measured? How was it measured? Self-reported through a questionnaire? Directly?, why: Who funded the data collection? What was the purposes? To whose benefit was the data collected? Answers to such questions strongly impact the conclusions you will be able to draw from the data.↩︎"
  }
]